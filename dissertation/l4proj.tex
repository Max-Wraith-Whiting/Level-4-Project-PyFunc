% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.

\documentclass{l4proj}

    
%
% put any additional packages here
%

\begin{document}

%==============================================================================
%% METADATA
\title{Pythonic Syntax, Functional Core}
\author{Maxwell A. Wraith-Whiting}
\date{January 8th, 2024}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    % Every abstract follows a similar pattern. Motivate; set aims; describe work; explain results.
    % \vskip 0.5em
    % ``XYZ is bad. This project investigated ABC to determine if it was better. 
    % ABC used XXX and YYY to implement ZZZ. This is particularly interesting as XXX and YYY have
    % never been used together. It was found that  
    % ABC was 20\% better than XYZ, though it caused rabies in half of subjects.''


    % (Introduction) - The goal of the project is to produce a proof-of-concept functional programming language that utilises a Hindley-Milner type system.
    %  This is for the explicit purpose of utilising Python's ease of use whilst leveraging functional programming's correctness and type-safety.
    % (Methods) - In this project I made the prototype language and ran it in contrast to Python 3.* over several test benchmarks.
    % (Results) - The new language proved to be [GOOD / BAD]
    % (Discussion) - The future relevance of this type of language is significant as the the computer science world turns its attentions to the functional paradigm.


    % The allowance of state in modern high-level programming is a mistake.
    % Many of the most popular programming languages of today are riddled with state, for example: Python.
    % This project aims to explore and mitigate the issue of state whilst rendering runtime-errors an unlikely possiblity instead of an inevitability.
    % The [functional language] was created purely in OCaml, implementing the full language pipeline.
    % FPL performed in a [similar/disimilar] manner to Python in the selected test-suites, though as a prototype, it could not compare for more advanced features.

    In modern software development there is a search for languages that offer the expressiveness, readability, and robustness.
    Python is one of the most popular first languages among new learners standing as a highly readable, simple, versatile introduction to programming.
    However, despite its popularity Python suffers from its imperative nature and poses challenges for developers embracing the increasingly popular functional paradigm.
    This paper introduces a novel functional programming language inspired by Python’s syntax but with additional catering to functional programming.
    The language aims to bridge the gap between the accessibility of Python and the robustness of languages like Haskell and Ocaml.

\end{abstract}

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
%\def\consentname {My Name} % your full name
%\def\consentdate {20 March 2018} % the date you agree
%
\educationalconsent


%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
% Do not alter the bibliography style.
%
% The first Chapter should then be on page 1. You are allowed 40 pages for a 40 credit project and 30 pages for a 
% 20 credit report. This includes everything numbered in Arabic numerals (excluding front matter) up
% to but excluding the appendices and bibliography.
%
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
%
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however. 
%
%==================================================================================================================================
\chapter{Introduction}



% reset page numbering. Don't remove this!
\pagenumbering{arabic} 

% STARTSTART
\begin{center}
    -------------------
\end{center}


\textbf{This is drivel. What is the idea? Why should anyone care? Does it have any value?}

[\emph{The C-like syntax of imperative languages stands as the predominant and extensively utilized programming syntax, prized for its readability and capacity to succinctly express sequences of instructions.
Conversely, functional programming languages represent a departure from this "conventional" syntax, tracing their lineage back to predecessors like Lisp and ML rather than their widely adopted descendant.
The functional paradigm, though lauded for its robustness and expressiveness, often presents a formidable barrier to entry for novice programmers, lacking the intuitive accessibility characteristic of more familiar programming styles.
In contrast, Python emerges as an approachability in the programming realm, frequently heralded as an ideal introductory language owing to its user-friendly syntax, clear structure, and potent features.
It is against this backdrop that the present project idea arises—a simple inquiry into the feasibility of bridging the gap between the safety and potency inherent in functional programming and the familiar terrain of Python-like syntax.
Can the principles of the functional paradigm be rendered more accessible to programmers accustomed to Python's simplicity, and conversely, do Python-like languages stand to gain from incorporating these features?}]

This project aims to address the inherent limitations of Python for functional programming while leveraging its renowned readability and simplicity.
Python, although popular, lacks native support for functional constructs and a robust type system, making it challenging for developers to embrace the functional paradigm fully.
In response, this paper proposes a novel functional programming language (PyFunc) inspired by Python's syntax but enhanced with features tailored for functional programming, such as being expression-based, including first-class functions, and having a Hindley-Milner type system.
By melding Python's approachability with the advantages of a robust type system, such as enhanced code reliability and ease of maintenance, the language seeks to lower the barrier to entry for newcomers to functional style programming while providing a prototype of a powerful tool for building large-scale, maintainable applications.
The project's core contributions lie in the development and implementation of features like a friendly Python-like syntax, a Hindley-Milner type system, and simple functional constructs.
Through research, language design, and an implementation in Ocaml, the project aims to deliver a language that offers the best of both worlds, thereby facilitating smoother transitions to functional programming paradigms and improving the overall quality and maintainability of software systems.

This project does not aim to introduce groundbreaking ideas or sophisticated algorithms. 
Instead, it's about combining existing an exisiting functional programming base with Python's user-friendly and efficient nature.

For example, here is a simple python program:
\begin{lstlisting}[language=Python, caption=A simple python program to find the factorial of a given number (n).]
    def factorial(n):
        if n == 1:
            return 1
        else:
            return n * factorial(n - 1)
    
    print(factorial(5)) # 120
\end{lstlisting}

Now whilst it is very easy to read and follow the logic of the function, Python can provide no guarantees on the type accuracy of this relatively simple code.
However, in a Hindley-Milner type system, such code could easily have its type infered.
There is no major reason why this syntax, or at least a close expression-based equivalent, cannot have a relatively sophisticated type system.
The inclusion of a type system onto this fundamental structure would prove only to be beneficial

The proposed additions to the familiar language would only marginally modify the syntax:

\begin{lstlisting}[language=Python, caption=A proposed PyFunc program to find the factorial of a given numbe (n).]
    def factorial(n):
        if n == 1:
            1
        else:
            n * factorial(n - 1)
\end{lstlisting}


% Python is known for being easy to learn and quick to write code in, making it a solid starting point for read-able syntax.
% On the other hand, functional programming offers powerful tools but can be hard to grasp for some due to its terse syntax.
% By blending functional programming principles into Python, I hope to make these concepts more accessible to a wider audience.
% However, whether this integration will effectively simplify functional programming remains an open question.

\textbf{Contributions}

The core contributions of this project lie in the development and implementation of the language’s features:

\begin{itemize}
    \item Friendly Python-like Syntax.
    \item Hindley-Milner Type System.
    \item Support for simple functional programming.
\end{itemize}

% \begin{center}
%     -------------------
% \end{center}
% ENDEND


% \textbf{Why should the reader care about what are you doing and what are you actually doing?}

% The C-like syntax of imperative languages is by far the most read and used programming syntax.
% It lends itself to simplicity and variably terse series of instructions.

% Functional programming languages are however completely distinct from this syntax style.
% They often follow predecessors languages (Lisp, ML, etc.), more closely than their ubiquitous descendant.

% The functional paradigm is one not generally approached by novice programmers.
% It is not a place that allows easy learning for those inexperienced with programming in general.

% Python is often cited [] as being a particularly easy first programming language, due to its friendly syntax, simple style, and powerful constructs.
% The idea behind this project is a simple one.
% Can the inherent safety and power of the functional paradigm be made more accessible to programmers more familiar with Python-like syntax and do Python-like languages benefit from these features.

% WHY!
% The functional paradigm itself suffers for lack of novice programmers and its academic ouvre.
% Functional languages have much to offer beyond their "catch-phrase" features.
% There exists no truly C-like functional language that does not approach academic malaise.

% I have a dream: That the safety and power, nay the \emph{joy} of programming in a functional language can be made accessible even to those only just learning to think like a programmer.
% That the complexity of certain structures in the languages do not breed the sort of terror that things like the word \emph{monad} can evoke in a Java programmer.

% \section{Guidance}

% \textbf{Motivate} first, then state the general problem clearly. 

% % \section{Writing guidance}
% % \subsection{Who is the reader?}

% This is the key question for any writing. Your reader:

% \begin{itemize}
%     \item
%     is a trained computer scientist: \emph{don't explain basics}.
%     \item
%     has limited time: \emph{keep on topic}.
%     \item
%     has no idea why anyone would want to do this: \emph{motivate clearly}
%     \item
%     might not know \emph{anything} about your project in particular:
%     \emph{explain your project}.
%     \item
%     but might know precise details and check them: \emph{be precise and
%     strive for accuracy.}
%     \item
%     doesn't know or care about you: \emph{personal discussions are
%     irrelevant}.
% \end{itemize}

% Remember, you will be marked by your supervisor and one or more members
% of staff. You might also have your project read by a prize-awarding
% committee or possibly a future employer. Bear that in mind.

% % \subsection{References and style guides}
% There are many style guides on good English writing. You don't need to
% read these, but they will improve how you write.

% \begin{itemize}
%     \item
%     \emph{How to write a great research paper} \cite{Pey17} (\textbf{recommended}, even though you aren't writing a research paper)
%     \item
%     \emph{How to Write with Style} \cite{Von80}. Short and easy to read. Available online.
%     \item
%     \emph{Style: The Basics of Clarity and Grace} \cite{Wil09} A very popular modern English style guide.
%     \item
%     \emph{Politics and the English Language} \cite{Orw68}  A famous essay on effective, clear writing in English.
%     \item
%     \emph{The Elements of Style} \cite{StrWhi07} Outdated, and American, but a classic.
%     \item
%     \emph{The Sense of Style} \cite{Pin15} Excellent, though quite in-depth.
% \end{itemize}

% \subsubsection{Citation styles}

% \begin{itemize}
% \item If you are referring to a reference as a noun, then cite it as: ``\citet{Orw68} discusses the role of language in political thought.''
% \item If you are referring implicitly to references, use: ``There are many good books on writing \citep{Orw68, Wil09, Pin15}.''
% \end{itemize}

% There is a complete guide on good citation practice by Peter Coxhead available here: \url{http://www.cs.bham.ac.uk/~pxc/refs/index.html}. 
% If you are unsure about how to cite online sources, please see this guide: \url{https://student.unsw.edu.au/how-do-i-cite-electronic-sources}.

% % \subsection{Plagiarism warning}

% \begin{highlight_title}{WARNING}
    
%     If you include material from other sources without full and correct attribution, you are commiting plagiarism. The penalties for plagiarism are severe.
%     Quote any included text and cite it correctly. Cite all images, figures, etc. clearly in the caption of the figure.
% \end{highlight_title}


%==================================================================================================================================
\chapter{Background}
% \textbf{What did other people do, and how is it relevant to what you want to do?}

The realm of programming languages boasts a rich diversity, offering numerous options known for their straightforward and easily comprehensible syntax.
Notable examples include Ruby and Python, celebrated for their simplicity and readability.
Additionally, a significant number of languages integrate elements of the Hindley-Milner type system, particularly within functional programming paradigms.
This system, renowned for its robust type inference capabilities, serves as the backbone for languages that prioritize expressive and concise code.
Academic research has made substantial contributions to the field of type inference systems, exploring various approaches and techniques to enhance type checking and inference processes.
Furthermore, scholarly endeavors have delved into language implementations, such as MiniML, to explore the intricacies of language design and implementation.
This section aims to offer a cohesive overview of these distinct areas, providing the foundation from which this project derives inspiration and insight.

% This sections intends to give a brief overview of the background ideas this project relies on, namely:
% \begin{itemize}
%     \item Python's Syntax
%     \item Functional Programming
%     \item Hindley-Milner Type Systems
% \end{itemize}

\section{Strengths of Pythonic Syntax}

% Python is popular.
% Python's continuing popularity is well-established, evident from its consistent top ranking in the TIOBE index for February 2024. [\cite{TIOBE}]
% So the language's popularity is not a question, but rather: \emph{Why} is Python so popular? 

% There are many reasons to dislike the language, as it has many 'flaws' that other languages do not.
% For example, Machine Learning and Data Science are fields that from an purely intuitive perspective would value speed over everything.

% Performance is something Python fundamentally lacks due to its internal structure, being a dynamically typed interpreted language. (Both traits adding significant runtime processing requirements.)
% It would be a lie to say Python cannot be relatively fast, but performance is not its \emph{raison d'être
% }.

% Python's popularity is well-established, evidenced by its consistent top ranking in the \cite{TIOBE} index for February 2024 and PYPL Popularity Index (March 2024). So, for what reason is Python so popular and why?

% [THIS IS PURILE FLUFF]

% While Python has garnered widespread adoption, it is not without its critics, especially regarding performance concerns.
% Python's dynamic typing and interpreted nature inherently limit its performance capabilities, resulting in significant runtime processing requirements.
% However, Python's popularity in fields like machine learning and data science seems paradoxical given its sluggishness.

% The answer is somewhat counter-intuitive: \emph{Because Python is fast.}


% The answer lies in its surprising speed: Python's strength lies not in pure CPU execution time but in its efficiency in coding, making changes, and quickly producing functional systems.
% Python's simplicity makes it accessible to beginners while remaining powerful enough for complex projects.
% Despite its reputation for slowness, Python's ability to construct programs rapidly often outweighs the CPU time it consumes.
% Readability is another significant factor in Python's appeal; its syntax, influenced by the C-family, provides a familiar structure for programmers and aids comprehension.
% Python's success as a beginner language stems from its ability to teach fundamental programming syntax.
% In essence, Python's strengths lie in its syntax and semantics rather than its performance, emphasizing its value in producing easily comprehensible code.


Python's popularity is undeniable, evident from its consistent top rankings on indices like the TIOBE Index (February 2024) and the PYPL Index (March 2024).
But what makes Python so widely favored? One of its key strengths lies in its unparalleled agility, enabling rapid iteration and fostering a development environment conducive to experimentation and quick prototyping.
Moreover, Python's simplicity and readability contribute significantly to its appeal, making it accessible to both novice and experienced programmers alike.
The language's syntax, often resembling plain English, further enhances its beginner-friendly nature, minimizing the learning curve for newcomers.
However, despite its myriad advantages, Python is not without its critics.
Some argue that its performance is subpar, although it's important to note that performance isn't the primary focus of the language.
Additionally, Python's type system, characterized by duck typing, may be considered less sophisticated, leading to potential type errors at runtime.
Nevertheless, the essence of Python's allure lies not in its implementation, but rather in the inherent strengths of its syntax and semantics.

To highlight the benefits of the syntax, the following examples will contrast the same program in both Python and Ocaml.


% Speed of execution is not the only bottleneck in producing systems.
% The speed of writing code, the speed of making changes, the speed of just attempting to get a working system is far more important than pure CPU execution time.
% Python performs extremely well in these areas.
% A user's time is more valuable than CPU time.
% It's really easy to get started with.
% Python is simple enough for anyone to get started in, but still powerful enough to hanlde large complex projects.

% Python is considered slow, but often it is neglected to mention "In what context? Compared to what?"

% It has reached the point of a stereotype that Python is slow.
% Oftentimes, the ability to construct a program fast, to iterate quickly, is far more valuable than the CPU time it incurs.

% The readablilty also play a significant part in the value of the language.
% It is often stated that code will be read far more than it will be written.
% The statement itself makes it intuitive that code should be inherently aimed at readability.

% Python is a descendant of the C-family of syntaxes, with a few unique twists of its own, such as white-space indentation.
% This ancestry gives programmers unfamiliar with the language a grounding, or more accurately an expectation, of how the code will behave based on its syntax.
% Obviously, the exact semantics differ, but certain universals remain.

% It is to Python's benefit that there is an effective understanding of the syntax of the language.
% It is also likely why Python is so successful as a beginner language, as it informs new programmers of one of the most prevalent syntax's the collection of all of programming langauges.

% So to short: Python's strengths lie in its syntax and semantics, not necessarily its implementation.
% The value that this project aims to extract from it is not in its underlying structure but its front-facing interface as a means to producing easily comprehensible code.

\section{Benefits of the Functional Paradigm}

Functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions.
It avoids mutable state, often by enforcing data immutability.
[Way to state the obvious...]

Some of the key concepts include:

\begin{itemize}
    \item \textbf{Pure Functions:} Functions that, given the same input, always return the same output.
    This requires the restriction of access to mutable state inside functions that could interfere in the functions execution.
    (I.e., Functions do not depend on or modify variables outside of their scope.)
    \item \textbf{Immutable Data:} Data cannot be changed in-situ after creation.
    Instead of modification of data, functional programming encourages the creation of new variables with the desired changes.
    \item \textbf{Higher Order Functions:} Functions that can take functions as arguments or return them as results.
    A simple example is \emph{Map} which maps a function onto a give structure.
    \item \textbf{Referential Transparency:} The property that an expression can be replaced with its value without changing the program's behaviour. 
\end{itemize}

All of these concepts can be considered spectrums rather than hard rules, as many popular functional languages bend these concepts for usabilities sake.\footnote{Ocaml for instance is not a \emph{purely} functional programming language. It technically is a functional, imperative, and object-oritened programming language.}
Though these languages do make behaviour outside of the norm specified by these rules explicit and force the programmer to be deliberate with their forays outside of the paradigm.

Functional programming offers several advantages and addresses certain challenges inherent in other paradigms, such as imperative and object-oritened programming. 

\begin{itemize}
    \item \textbf{Clarity and Maintainability:} Functional programming emphasises (and sometimes enforces) pure functions and immutable data.
    By avoiding side-effects and mutable state, functional programs tend to be easier to reason about and maintain.
    \item \textbf{Conciseness and Expressiveness:} Functional programming languages often provide high-level abstractions and expressive syntax for common programming patterns, such as map, filter, and reduce operations.
    This can lead to shorter, more concise code that is easier to write and understand.
    \item \textbf{Robustness and Predictability:} Pure functions and immutable data make functional programs more robust and predictable.
    Since pure functions have no side effects, they can be unit-tested and debugged easily.
    Additionally, immutable data structures can help prevent common bugs related to unintended modifications of data.
    \item \textbf{Formal Verification:} Functional programming languages and techniques are often amenable to formal verification techniques, which can provide guarantees about the correctness of a program.
    This is a valuable trait in safety-critical and high-assurance applications.
\end{itemize}

These numerous powerful and useful features work across many contexts, so why are functional programming languages not as commonly used as other languages like Python, Java, or JavaScript?

There is no definitive answer, but the general sentiment is that functional programming is \emph{hard.}

Most FP languages do not allow careless side-effects; most make mutability (if even possible) explicit; most virtually require the use of higher order functions. 
These features are not, in my opinion, what makes it difficult to learn functional programming. 

\emph{It is the fact that you need to know what these concepts are, before you start.}

In most popular languages, the barrier to entry is not conceptual, but syntactical.
Python, as our standard example, requires only rudimentary knowledge of the syntax to hack together something that works out of the box.

Whilst getting a functional language to print something to the standard output is often like pulling teeth by comparison.
\section{Type Systems and Hindley-Milner}

% \section{Guidance}
% \begin{itemize}    
%     \item
%       Don't give a laundry list of references.
%     \item
%       Tie everything you say to your problem.
%     \item
%       Present an argument.
%     \item Think critically; weigh up the contribution of the background and put it in context.    
%     \item
%       \textbf{Don't write a tutorial}; provide background and cite
%       references for further information.
% \end{itemize}

% What are type systems?

Type systems are a fundamental concept in programming languages.
They define a set of rules and constraints for categorising and manipulating data based on the type of data it is.
A type refers to a classification that specifies what kind of data a variable can hold and what operations can be performed on it.

% What is a Hindley-Milner Type System?

The Hindley-Milner (HM) Type System is a type system used in statically typed functional langauges to automatically deduce the types of expressions without explicit type annotations.
It also supports polymorphic functions 
$\tilde{}$ It was independantly developed by Rogder Hindley in () and later extended by Robin Milner in ().
The most well known implementation of the system is in the programming language ML (Meta Language), though it exists in many langauges, such as Haskell, OCaml, and Clean.

\subsection{Typing Systems and Notation}

In type theory, a typing rule (also called an inference rule) describes how a type system assigns types.
In a general sense, a type system is a set of typing rules which when applied to a program indicate whether the program is well-typed. (I.e., A valid program.)

The notation of these typing rules is somewhat unusual but relatively straight forward.
A given rule specifies the structure of syntax to the appropriate type.
The most basic part of the notation is a type relation:

\[3 : Int\]
\[e : \tau\]

Literal values have a definitive type, $3$ is \emph{literally} an Int value, as \texttt{true} is a boolean.
The more general rule for $e$ being of type $\tau$ is not particularly useful in this context but can easily be read as "$e$ is of type $\tau$", where $e$ refers to some expression and $\tau$ to some type.

Now, in proof theory there already exists a notation for inference rules called \textbf{natural deduction}, which typing rules use extensively.
The general structure of a rule is as a collection of propositions on the top, and the conclusions below.

\begin{equation} \label{eq:inference-syntax-example}
\frac{Proposition\: A \;\; Proposition\: B \;\; ... \;\; Proposition\: N}{Conclusion\: A \;\; Conclusion\: B \;\; ... \;\; Conclusion\: N}
\end{equation}

Here is a simple example of a non-typing inference rule:
\begin{equation} \label{eq:inference-example}
\frac{A : true \;\;\; B : true}{(A \wedge B) \; true}
\end{equation}

This rule can be read as: 'If A is \texttt{true} and B is \texttt{true} we can conclude A \emph{and} B is \texttt{true}'.

For a typing example, the following type rules specify the typing relations for a simple integer addition:

\begin{equation} \label{eq:add-type-rule}
\frac{}{5 : int} \;\;\; \frac{}{7 : int} \;\;\; \frac{e_1 : int \;\;\; e_2 : int}{e_1 + e_2 : int}
\end{equation}

The first two rules have no propositions and therefore can be universally concluded, which makes them \textbf{axioms}.
The third rule is an inference rule, with two integer expressions given as propositions.

This brief outline of the natural deduction typing notation should suffice for the extent of this project. 

% The final piece of notation to understand, which is of key importance to programming languages, is binding.
% The type of a variable depends on where is is bound, which requires context-sensitive typing rules.
% These rules are given by a typing judgements (sometimes refered to as assertions), usually written in the form:

% \begin{equation} \label{eq:context-example}
%     \Gamma \vdash e : \tau
% \end{equation}

% which can be read as 'expression $e$ has type $\tau$ under the typing context $\Gamma$'. On occasion it is necessary for there to be cases where these typing contexts are shown to contain specific typed variables.

% \begin{equation} \label{eq:context-introduction}
%     \frac{e:\tau \in \Gamma}{\Gamma \vdash x:\tau}
% \end{equation}

% \ref{eq:context-introduction} shows the relatively simple inference that if expression $e$ of type $\tau$ is a member of context $\Gamma$ then it can be infered that context $\Gamma$ contains expression $e$ of type $\tau$.



% ================================================================================

\section{A Brief Review of Lambda Calculus} \label{sec:lambda-review}

The IR of PyFunc is an extended lambda calculus, and therefore a brief review of the lambda calculus seems appropriate before extending it.

The lambda calculus is a formal system designed by Alonzo Church as a foundational way of expressing computation.
The system is based on function abstraction and application, using variable binding and subsitution.

The lambda calculus is very minimal by intention:

\begin{equation} \label{eq:lambda-example}
    (\lambda x . x) y
\end{equation}

The simple lambda expression in \ref{eq:lambda-example} contains all three elements the lambda calculus includes:

\begin{table}[!h]
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}|l|l|l|l|@{}}
        \hline
        \textbf{Name}       &   \textbf{Syntax}     &   \textbf{Example}    &   \textbf{Explanation} \\
        \hline
        Variable & <name> & $x$ & a variable named "x" \\
        Abstraction & $\lambda$<parameter>$.$<body> & $\lambda x . x$ & a function with parameter "x" and body "x" \\
        Application & <function><variable or function> & $(\lambda x . x) y$    & calling the function "$\lambda x . x$" with arugment "y"\\
        \hline
    \end{tabular}}
\end{table}


\begin{enumerate}
    \item A variable is a character of string representing a parameter. 
    \item A lambda abstraction, $(\lambda x . y)$, is a function defintion, taking as input the bound variable $x$ (between the $\lambda$ and the dot) and returning the body $y$.
    \item An application of a function, $(X Y)$, is simply applying function $X$ to an argument $Y$. Both $X$ and $Y$ are lambda terms.
\end{enumerate}

Now with these three structures, there are two further rules for actually running the computations:
\begin{itemize}
    \item $(\lambda x . x) \rightarrow (\lambda y . y)$: $\alpha$-conversion, renaming bound variables in the expression. The rule is that so long as bindings are renamed consistently, they are equivalent. $\alpha$-converison is used to avoid name collisions.
    \item $((\lambda x . M) N) \rightarrow (M [x := N])$: $\beta$-reduction, replacing bound variables with the argument expression in the body of the abstraction. In this example $x$ is bound to the value $N$.
\end{itemize}


%==================================================================================================================================
\chapter{Analysis/Requirements}
% What is the problem that you want to solve, and how did you arrive at it?

% [NOTE]: What is the problem
% The dominant programming paradigm is imperative, no matter the uptick of interest in functional programming.
% The current state of functional programming does not lend itself to easy entry for new programmers who will be the future of the paradigm.
% The functional paradigm has many pitfalls for new developers:
% \begin{itemize}
%     \item Abstract concepts
%     \item Mathematical foundations
%     \item Lack of familiarity
%     \item Limited resourses and support
%     \item Complex error messages
%     \item Perceived lack of practical applications
%     \item steep learning curve
% \end{itemize}

% \section{Aims and Justification}

The aim of the project is to make functional \emph{prototype} language, where the foundational aspects of the language are set up.
This section should emphasise the actual scope for the project and create the boundaries that determine whether a feature is inside or outside the scope of the project.
There are several interesting and valuable aspects to Python, Functional programming, and type-systems that this entire project could focus on.
Yet, with feasibility in mind, the extent to which these topics can be covered has to be rigourously delineated.

\section{Minimal Viable System}

The Minimal Viable System (MVS) for the prototype language, adhering to the outlined specifications, will prioritise essential features to demonstrate core functionality while deferring more advanced or less critical features.
To do this, certain constraints must be put in place:

\begin{itemize}
    \item First, the frontend does not need to replicate each and every piece of Python syntax.
    Python has many interesting syntactic ideas which would add excessive focus onto the structure of the grammar and parsing it. 
    \item The type system needs to be fully functional over the entire language, otherwise it cannot be called a valid type system.
    \item The interpreter does not need to do anything more than run the code.
    Whilst many powerful optimisations could be implemented, it would go beyond the proof-of-concept prototype that this is.
\end{itemize}

\subsection{Frontend Syntax}

To attempt to include all the features of a mature programing language would be far greater than the expected scope of this project.
Therefore, to rein in expected features of the project, limitations will be imposed on the extent of the frontend syntax.
These limitations should, in effect, propagate through the requirements of the whole language, whilst retaining the fundamental concepts that make the language worth building.

First, it must be considered what a programming language cannot do without.
There are a few things no useful language does not have, which include: arithmetic, control structures, functions.
Arithmetic is mandatory and not a major constraint as the base implementation of operators will be necessary, but attention should be paid to making sure the implementation is extensible.

There are many possible control structures that could be used, but for the sake of simplicity, only the if-expressions will be guaranteed to be included. 

Of the things that the language cannot live without, functions are at the top of the list.
It would be ironic if a functional style language was bereft of functions.
However, whilst functions must be included, not all of Python's varied uses of them are necessary.

\textbf{Closures}
For example, Python includes the ability to create "closures" which can recall values in their defined scope even if they are not present anymore with the \texttt{nonlocal} keyword.

\begin{lstlisting}[language=Python, caption={An example of a closure in Python. See the use of the 'nonlocal' keyword to pull the surrounding defintion of 'current\_count' into the scope of 'inner\_function'.}, label={lst:py-closure}]
# Create a counter.
def make_counter():
    current_count = 0
    
    def inner_function():
        nonlocal current_count # Pull the variable into scope.
        current_count += 1
        return current_count

    return inner_function

counter = make_counter()
print(counter()) # 1
print(counter()) # 2
print(counter()) # 3
\end{lstlisting}

Whilst it would be possible to include such a feature, the priority of such a task would be of a much lower priority than others.

\textbf{Higher Order Functions}
A language may want to include higher-order functions to support more expressive and flexible programming paradigms. 
Higher-order functions allow functions to take other functions as arguments or return functions as results.
This enables powerful programming techniques such as function composition, partial application, and passing functions as arguments to other functions, which can lead to more concise, modular, and reusable code.

\textbf{Inner Functions}
The necessity of including inner functions into PyFunc is not apparent.
Whilst they are of great utility for encapsulation of logic, the potential increase in complexity of bindings makes them an lesser priority than other frontend features like Higher Order Functions.

\subsection{Type-System}
The Hindley-Milner Type System offers several advantages in programming:

\begin{itemize}
    \item \textbf{Type Inference:} Hindley-Milner allows for type inference, which means that you don't have to explicitly annotate the types of every variable or function in your code.
    The type checker can often automatically infer the most general types, reducing the burden on the programmer and making the code more concise.
    \item \textbf{Type Safety:} Programs written in languages with Hindley-Milner type systems tend to have fewer type-related bugs because the compiler or interpreter can catch many type errors at compile time.
    This leads to more robust and reliable code.
    \item \textbf{Polymorphism:} Hindley-Milner supports parametric polymorphism, also known as generics in some languages.
    This allows you to write functions and data structures that can operate on a wide range of types without sacrificing type safety.
    This idea fundamentally aligns with the ideas behind a human-readable syntax, minimising 'boiler-plate' code whilst remaining expressive.
    \item \textbf{Expressiveness:} Despite being statically typed, Hindley-Milner type systems are quite expressive and flexible.
    They can handle complex type constraints and provide powerful abstractions for structuring programs.
    \item \textbf{Efficiency:} The type inference algorithm used in Hindley-Milner systems is efficent, making compilation or interpretation fast compared to more complex type systems.
\end{itemize}

As for alternatives:

\begin{itemize}
    \item \textbf{Dynamic Typing:} In languages with dynamic typing, such as Python itself, there is no static typechecking at compile time.
    Instead, types are checked at runtime, which can lead to type-related errors occuring during program execution rather than compile time. 
    While dynamic typing offers flexibility and rapid development, it can also make code harder to understand and maintain, and it may result in runtime type errors that could have been caught at compile time.
    This would be an underutilisation of the functional paradigm and would leave the language with issues that Python already suffers from, like upredictable type errors at runtime.
    \item \textbf{Dependent Typing:} Dependent type systems, found in languages like Idris and Agda, allow types to depend on values.
    This provides very strong guarantees about program correctness but can be complex to work with and may require explicit type annotations from the programmer.
    Adding type annotations dominates the syntax of the language, which defeats the point of aiming for a 'Pythonic' syntax. \footnote{Wells, Joe B. "Typability and type checking in System F are equivalent and undecidable." Annals of Pure and Applied Logic 98.1-3 (1999): 111-156.}
    \item \textbf{Gradual Typing:} Graudal typing combines elements from static and dynamic typing, allowing programmers to choose when to use type annotations and when to rely on dynamic typing.
    This can be useful when transitioning existing codebases to a statically typed language or when working in areas where static typing is impractical or overly restricitive.
\end{itemize}

% In cases where no type system is used at all, programmers have complete freedom but lose the benefits of type checking, such as early error detection and improved code documentation.
% This approach can work well for small projects with dedicated teams but can become problematic as codebases grow larger and more complex.

% \subsection{Language Features}

\section{Proposed Minimal Requirements}

After the previous analysis it was determined that the MVS should include the following:

MUST HAVE:
\begin{itemize}
    \item \textbf{Functional REPL and shell tool:} Without this there is not much of a tool.
    \item \textbf{Full Language Pipeline:} A fully incorporated pipeline for all of the disparate features of the language is a must have.
    \item \textbf{Basic Primitives:} The fundamental primitives that must be included in the language are: Integers, Strings, Floats, Booleans, and a Unit\footnote{Much like in OCaml, the Unit value or () is the equivalent of a null type in many ways, but disimilar in that expressions treat evaluating to () as being voided.} value.
    \item \textbf{Basic Arithmetic:} The inclusion of basic arithmetic is an obvious inclusion but one worth stating as the type-system requires appropriate operations for types.
    (I.e., Integer addition and float addition internally are different operations, that appear the same in the frontend.)
    To be explicit the arithmetic operations to include are: +, -, $\times$, ÷, ==, >=, <=, >, <, !=.
    \item \textbf{Basic Control Structures:} Basic controls structures are a must-have feature. They must include: If-else expressions, match-expressions.
    \item \textbf{Functions:} Simple non-recursive functions and recursive functions must be included. Without recursive functions the language cannot support looping.
    \item \textbf{Lists and Appropriate Operations:} List are the major data structure in Python and for any resemblanace to be accurate the prototype language must implement them in atleast a basic form.
    \item \textbf{Hindley-Milner Type System:} The type system is a fundamental structure that the benefits of this language hinge upon, and therefore it is an essential feature.
\end{itemize}

SHOULD HAVE:
\begin{itemize}
    \item \textbf{Standard Functions:} Python includes an array of keywords to do common tasks, the prototype language should do something similar.
    (E.g., include keywords such as \verb|len| or \verb|any| to simplify code.)
    \item \textbf{List Comprehensions:} List comprehensions are an incredibly useful feature of Python that fit nicely into the functional ethos whilst being relatively easy to read and parse.
    Such a useful construct would be a boon to the prototype languages though not critical.
    \item \textbf{String Manipulation:} Whilst any true language implementation should include some measure of string manipulation, for this prototype it falls outside of truly critical featuers, whilst being an important addition to potentially add.
    \item \textbf{Basic IO:} A program that cannot take input or produce an output is fundamentally useless, and therefore some amount of IO (no matter how minimal) should be included to this prototype.
\end{itemize}

COULD HAVE:
\begin{itemize}
    \item \textbf{Inner Functions:} Inner functions are a useful tool for encapsulating relevant functions within other function scopes.
    However, managing the potentially complex scoping involved might require semantic analysis and more work than they are worth in a prototype language.
    \item \textbf{Closures:} Being an unusually hidden piece of syntax, closures are not as commonly used in Python though it supports them.
    Closures are undeniably very powerful, but much like inner functions, the scoping rules involved introduces additional complexity into the implementation of function scopes. 
    \item \textbf{Sets and Dictionaries:} Whilst a commonly used feature, sets and dictionaries are not significant enough to be a primary implementation concern.
\end{itemize}

WILL NOT HAVE:
\begin{itemize}
    \item \textbf{Objects:} Implementing objects into the prototype language would be closer to what users would expect of Python. However, the functional paradigm is the focus and it would defeat the point of the project to focus on Objects.
    \item \textbf{Standard Library}: Python has a large collection of modules in its standard library, therefore due to its multitudionous nature, it would be unreasonable to even attempt to include any significant proportion of the Python standard library in the prototype language. 
\end{itemize}

% \section{Guidance}
% Make it clear how you derived the constrained form of your problem via a clear and logical process. 

%==================================================================================================================================
\chapter{Design}

% How is this problem to be approached, without reference to specific implementation details? 

In this dissertation, the focus lies on crafting a prototype programming language that serves as a bridge between the clarity of easily readable imperative languages and the robustness of less approachable functional programming paradigms.
\textbf{Previous chapters, if applicable, are revisited to highlight the foundational elements they contribute to this endeavor.}

Central to the development of this language is the emphasis on achieving low coupling between its internal modules.
As a prototype, it must possess the flexibility necessary for future extensions and modifications.
Even seemingly minor design alterations can yield substantial differences in user experience, underscoring the critical importance of maintaining low coupling from the outset of development.

\section{Frontend Syntax Design}

The frontend of the system draws inspiration from Python, aligning with the language's renowned emphasis on accessibility, particularly for novice programmers.
At its core, the design prioritizes ease of use, catering to individuals new to programming who may find traditional languages intimidating.
This emphasis on simplicity permeates the syntax of the language, reflecting Python's renowned straightforwardness.
By adopting a simple syntax, the language aims to lower the barrier to entry, allowing newcomers to focus more on solving problems and less on deciphering complex code structures.
This approach mirrors Python's philosophy of readability and simplicity, with the aim of fostering a welcoming environment for learners to explore programming concepts with confidence.

Quite significantly the language will be expression based instead of statement-based.
In this inital version there will be no inclusion of non-expression statements.
The only structures in the language that are not expression based are the function declarations which will be interpreted internally as bindings and therefore expressions.


% \clearpage
% \subsection{Syntax and Semantics}

% Pyfunc Operations
\begin{table}[h!]
    \caption{The standard table of operations in PyFunc, along with their syntax and accepted types. It should be noted that though operations can accept multiple types, they \textbf{must} match in an expression.}\label{tab:operators}
    %\tt 
    \rowcolors{2}{}{gray!3}
    \begin{center}
    \begin{tabular}{@{}|l|l|l|@{}}
    \hline
    %\toprule
    \textbf{Operation}    & \textbf{Syntax}       &   \textbf{Valid Types}    \\ %\midrule % optional rule for header
    \hline
    Addition              & \texttt{a + b}        &   \texttt{float, integer} \\
    Subtraction           & \texttt{a - b}        &   \texttt{float, integer} \\
    Multiplication        & \texttt{a * b}        &   \texttt{float, integer} \\
    Division              & \texttt{a / b}        &   \texttt{float, integer} \\
    Positive              & \texttt{+ a}          &   \texttt{float, integer} \\
    Negation (Arithmetic) & \texttt{- a}          &   \texttt{float, integer} \\
    Modulo                & \texttt{a \% b}       &   \texttt{float, integer} \\
    % \hline
    And                   & \texttt{a and b}      &   \texttt{boolean}        \\
    Or                    & \texttt{a or b}       &   \texttt{boolean}        \\
    Negation (Logical)    & \texttt{not a}        &   \texttt{boolean}        \\
    % \hline
    Less                  & \texttt{a < b}        &   \texttt{float, integer} \\
    Greater               & \texttt{a > b}        &   \texttt{float, integer} \\
    Less or Equal         & \texttt{a <= b}       &   \texttt{float, integer} \\
    Greater or Equal      & \texttt{a >= b}       &   \texttt{float, integer} \\
    Equality              & \texttt{a == b}       &   \texttt{float, integer, boolean} \\
    Not Equality          & \texttt{a != b}       &   \texttt{float, integer, boolean} \\
    Exponentiation        & \texttt{a ** b}       &   \texttt{float, integer} \\
    % \hline
    Cons                  & \texttt{a :: b}       &   \texttt{$\forall \alpha$.$\alpha$, $\alpha \rightarrow$ List $\alpha \rightarrow$ List $\alpha$} \\
    % Concatenation         & \texttt{a + b}        &   \texttt{list, string}   \\
    % Indexing              & \texttt{a{[}k{]}}     &   \texttt{list, string}   \\
    % Sequence Repetition   & \texttt{a * i}        &   \texttt{list, string}   \\
    % Slicing               & \texttt{a{[}i:j{]}}   &   \texttt{list, string}   \\
    % \bottomrule
    \hline
    \end{tabular}
    \end{center}
\end{table}


\begin{table}[!h]
    \caption{Table of additional syntax in PyFunc.}
    \begin{center}
    \rowcolors{2}{}{gray!3}

    \begin{tabular}{@{}|l|p{5cm}|p{4cm}|@{}}
        \hline
        \textbf{Expression}  &   \textbf{Syntax}     &   \textbf{Additional Notes}   \\
        \hline
        \hline
        If else expressions & if <condition>: \newline \-\ <expression\_a> \newline else: \newline \-\ <expression\_b> & <condition> evaluate to a boolean. If <condition> is true, then expression\_a is evaluated, otherwise expression\_b is evaluated.\\
        \hline
        Immutable binding   & \texttt{a := b}   & For all subsequent expressions in a given scope \texttt{a} will alias \texttt{b}. Variable \texttt{a} cannot be rebound. \\
        % \hline
        % Mutable binding & \texttt{a = b} & For all subsequent expressions in a given scope \texttt{a} will alias \texttt{b}. Variable \texttt{a} \emph{can} be rebound.\\
        \hline
        Function Definition & def <name>(<p$_1$>, ..., <p$_n$>): \newline \-\ <expression> & Function definition\\
        \hline
        Lambda Definition & lambda(<p$_1$>, ..., <p$_n$>): <expression> & Lambda definition \\
        \hline
        
    \end{tabular}
    \end{center}
\end{table}

An example program in PyFunc might look like this:
\begin{lstlisting}[language=python, caption={The simple recursive function to calculate a given fibonacci number.}, label=lst:callahan]
    # Calculate a given Fibonacci number.
    def fibonacci(n):
        if n <= 1:
            n
        else:
            fibonacci(n - 1) + fibonacci(n - 2)

    # Program entry point.
    def main():
        fibonacci(7) # Returns 8
\end{lstlisting}

% \clearpage
\section{IR Specification}
% \subsection{To IR or not to IR}
The decision to use an Intermediate Representation (IR) was a particularly easy one for PyFunc.
IRs offer several significant advantages, but come with an additional step of compiliation.
The main advantage for using an IR in PyFunc is the ease of development of the interpreter \emph{and} the type checker.
The IR effectively abstracts the complexities of the syntax of PyFunc away, leaving a simple and far easier to process language.
The specification of the IR however is somewhat atypical, as they are often designed with performance in mind whereas for PyFunc the focus is ease-of-development.

Specifically, the ease-of-development of the Type System.
To this end, the IR is an extended-lambda calculus.

\subsection{PyFunc-IR: An Extended Lambda Calculus}
As described in (\ref{sec:lambda-review} A Brief Overview of Lambda Calculus) the lambda calculus is extremely minimal, so for it to be a more workable IR, extensions are required.

The purpose of extending lambda calculus lies in enriching its expressiveness and capabilities to model more complex computations and features found in modern programming languages and computer science.
Lambda calculus, in its basic form, consists of only variables, function abstraction (lambda abstraction), and function application.
Extensions to lambda calculus introduce additional constructs and features to better capture various programming paradigms and computational concepts.

% \textbf{Some common extensions (that will be present) and their purposes include:}
\textbf{Lambda calculus extensions that will need to be included:}
\begin{enumerate}
    \item \textbf{Constants and primitive data types:} Introducing constants and primitive data types allows for the removal of all requisite encodings to make lambda calculus compute recognisable data structures. (E.g, church encodings are no longer required to hanldle data representation.)
    \item \textbf{Conditionals:} Adding a if-then-else expression allows for the easy expression of branchin logic and decision making in the IR.
    \item \textbf{Recursion:} Extending the lambda calculus to allow recursion allows for algorithms utlising self reference and iterative computations.
    In other words, without recursion, looping of any kind generates comparatively immense lambda expressions which is not just unweildy but often difficult to implement.
    \item \textbf{Let bindings:} The ability to bind a variable to lambda expressions allows for more concise and readable lambda expressions.
    Additonally, it also allows for variables to be represented in the IR, which has benefits for the later PyFunc-to-IR conversion. 
\end{enumerate}

First, let's deal outline the primitive data types that PyFunc's IR supports:

\begin{table}[h]
\caption{Table of PyFunc-IR's constant types.}
\begin{center}
\rowcolors{2}{}{gray!3}
\begin{tabular}{@{}|l|l|l|@{}}
    \hline
    \textbf{Type}        & \textbf{Description}      & \textbf{Syntax Examples}  \\
    \hline
    \texttt{Bool}        & Boolean type              & \texttt{True}       \\
    \texttt{Int}         & Integer type              & \texttt{42}         \\
    \texttt{Float}       & Floating-point type       & \texttt{3.141}      \\
    \texttt{String}      & String type               & \texttt{"hello"}    \\
    \texttt{Unit}        & The Unit type             & \texttt{$()$}       \\ 
    \hline
\end{tabular}
\end{center}
\end{table}

With just these five primitive types, a large majority of the types in the IR can be convered.
However, there exisits a few special cases that need to be considered to allow the IR to be fully well typed.

\begin{table}[h]
\caption{Table of PyFunc-IR's type constructs.}
\begin{center}
\rowcolors{2}{}{gray!3}
\begin{tabular}{@{}|l|l|l|@{}}
    \hline
    \textbf{Type}        & \textbf{Description}      & \textbf{Type Construction}  \\
    \hline
    \texttt{Function}    & Function type             & \texttt{$\tau_a$ $\rightarrow$ $\tau_b$} \\
    \texttt{Pair}        & Pair type                 & \texttt{$(\tau_a, \tau_b)$} \\
    \texttt{List}        & List type                 & \texttt{List $\tau$} \\
    \texttt{Variable}    & Type Variable             & \texttt{$\sigma$} \\
    \hline
\end{tabular}
\end{center}
\end{table}

\emph{- To build an abstract syntax tree for the IR, a full description of the possible tree nodes needs to be created.
- Using the above defined primitive types and additionally including the previously outlined common extensions to the lambda calculus we can write a specification for the abstract syntax tree.}

\subsection*{Abstract Syntax}

To construct an abstract syntax tree (AST) for the intermediate representation (IR), it is imperative to meticulously delineate all potential tree nodes.
This comprehensive description encompasses both primitive types and the incorporation of prevalent extensions to lambda calculus.
By amalgamating the specified primitive types with the previously outlined lambda calculus extensions, a detailed specification for the AST can be formulated.
This specification serves as a foundational blueprint, guiding the development of the AST and ensuring its alignment with the intended representation of the IR.
Through this systematic approach, the AST can accurately capture the intricacies of the IR, facilitating efficient analysis and manipulation within the context of the specified computational model.

The abstract syntax give below describes the extended lambda calculus used as PyFunc's IR.

\begin{lstlisting}[language=Caml, caption=The Abstract Syntax for the IR]
type tree = 
    | ExprVar of variable
    | ExprConst of Constant.t
    | ExprLet of (binder * tree * tree)
    | ExprLetRec of (binder * tree * tree) (* Binder, e, and e'*)
    | ExprOpUnary of (OpUnary.t * tree)
    | ExprOpBinary of (OpBinary.t * tree * tree)
    | ExprFunc of (binder * tree) (* Var binder, body*)
    | ExprApplic of (tree * tree) (* Arg, Func *)
    | ExprIf of (tree * tree * tree)
    | ExprPair of (tree * tree)
    | ExprLetPair of (variable * variable * tree * tree)
    | ExprFirst of tree
    | ExprSecond of tree
    | ExprList of tree list
\end{lstlisting}


This \texttt{Expr} type can completely represent the IR extended lambda calculus. 
The scope of the IR's syntax is relatively minimal, but due to the scope of the project, further extension of this IR would, one, encroach on the frontend language and, two, further complicate the type checker.

This level of complexity, being greater than the standard lambda calculus yet not becoming too distant from it seemed the most reasonable option.

\subsection*{IR Operators}
With the IR being very regularly structured, and differing rather minimally from the PyFunc frontend AST, it seems appropriate to include all of the available operators in the frontend in the IR itself.
This would include both binary operators and unary operators.

% \begin{table}[]
%     \begin{center}
%         \rowcolors{2}{}{gray!3}
%         \begin{tabular}{|lll|}
%             \hline
%             Left-hand Type   & Binary Operator & Right-hand Type  \\ \hline
%             Int, Float       & +               & Int, Float       \\
%             Int, Float       & -               & Int, Float       \\
%             Int, Float       & $\times$        & Int, Float       \\
%             Int, Float       & ÷       & Int, Float       \\
%             Int, Float       & \%              & Int, Float       \\
%             Int, Float       & **              & Int, Float       \\
%             Int, Float       & \textless{}     & Int, Float       \\
%             Int, Float       & \textgreater{}  & Int, Float       \\
%             Int, Float       & <=    & Int, Float       \\
%             Int, Float       & >= & Int, Float       \\
%             Int, Float, Bool & ==              & Int, Float, Bool \\
%             Int, Float, Bool & !=              & Int, Float, Bool \\
%             $\alpha$         & ::              & List $\alpha$    \\ \hline
%         \end{tabular}
%         \caption{Table of valid binary operators in PyFunc IR with accepted types.}
%     \end{center}
% \end{table}
% The IR is very regularly structured.
% The inclusion of all available operators in the frontend in the IR seemed an effective strategy to minimise conversion complexity.
% It easy to add new operators to the IR
% It has little overhead


% \begin{itemize}
%     \item Variable
%     \item Constants
%     \begin{itemize}
%         \item Int
%         \item Float
%         \item String
%         \item Bool
%         \item Unit
%     \end{itemize}
%     \item Let expression
%     \item Let Rec expression
%     \item Unary operation 
%     \begin{itemize}
%         \item Positive
%         \item Negative
%         \item Logical Not
%     \end{itemize}
%     \item Binary operation
%     \begin{itemize}
%         \item Add
%         \item Subtract
%         \item Multiply
%         \item Divide
%         \item Modulo
%         \item Less
%         \item Greater
%         \item Less Equal
%         \item Greater Equal
%         \item Equal (Comparison)
%         \item Exponentiation
%         \item Concatenation
%     \end{itemize}
%     \item Lambda expression
%     \item Lambda application
%     \item If expression
%     \item Pair
%     \item Let Pair
%     \item First
%     \item Second
%     \item List
% \end{itemize}
\section{Language Pipeline}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/Language_Pipeline.png}
    \caption{A flow chart of the steps in the language pipeline, with general error types.}
\end{figure}


The general structure of the language pipeline is relatively simple by design.
The frontend language is parsed into an abstract syntax tree (AST) before being passed onto the IR converter.
The converter takes the frontend AST and maps it onto a valid IR AST in a consistent manner.
All desugaring is aimed to take place during the IR conversion process.

After, the conversion phase the AST is passed first to the type checker, which raises an error if the program has invalid types.
If there are no invalid types, the type checker simply does not raise an error.
The next step after passing the type check is interpretation, the actual running of the program.
Interpretation is final step and can be relatively simple.


\subsection{IR Conversion}

The process of IR conversion will be the integral bridge between the frontend syntax and the simpler extended lambda calculus that the IR consists of.
The conversion aims to handle many aspects of the frontend and getting them into an appropriate format for the IR.
Some important examples of this are:
\begin{itemize}
    \item The handling of function bindings.
    \item Multiple parameter functions.
    \item Program entry point.
    \item List literal processing.
    \item Immutable and mutalbe assignments.
\end{itemize}

The most important of these is the handling of function bindings as the actual structure of PyFunc is dictated by this.
The language is aimed to be largely expression-based, but the necessary inclusion of declarations in the form of fuction definitions adds a layer of complication.
So, as a solution to this, the IR-Converter does the following:
\begin{enumerate}
    \item Take the list of bindings from the AST.
    \item Find the program entry point, i.e., the \texttt{main} function binding.
    \item Convert all other bindings into \texttt{let-rec} expressions with the \texttt{main} being the "in-scope".
\end{enumerate}

\section{Type Inference and Polymorphism}
\textbf{Inference}

Type theory is like a set of rules for organising and categorising things in programs based on what they are and what operations can be done to them.
It is not disimilar to how we categorise objects in real life, like sorting fruits into distinct groups of apples, oranges and bananas.

\textbf{Non-technical explanation of type theory and inference}

Now, type inference is the process of deducing the types of things without being explicitly told.
Imagine there is a game, where someone gives you clues about an object, but they cannot say what it is explicitly.
For example, they might say "It's round, red and grows on trees."
From these clues you might \emph{infer} that they are talking about an apple, even if they never said the word "apple" directly. 
(Though the ambiguity in this example would likely hinder accurate inferences.)

In programming, type inference works similarly.
Instead of explicitly stating (usually, via type annotations) the types of variables, the programming language's type system tries to guess what they are based on their usage in the code.

\textbf{Give a practical example}

Here's a practical example: Let's say you are writing a program in a language with type inference, and you have a variable $x$ that you set to $5$.
Now, without explicitly saying it, the type inference system can figure out that $x$ is an integer because you assigned it a whole number.
Later in the program, if you try to do something like $x +$ \texttt{"hello"}, the type system would catch that $x$ is an integer and \texttt{"hello"} is a string, so adding them together does not make sense, thereby raising an error.

\textbf{Now say why HM is better. (Type inference without annotations. Complete. Decideable.)}

It should be noted that there are many different styles of type systems with their own inference algorithms, each of which have their own strengths and weaknesses.

The Hindley-Milner type system is highly regarded for its simplicity, efficiency, and ability to infer principal types accurately without requiring explicit type annotations.
It provides strong type safety guarantees, catching errors at compile time and reducing the likelihood of runtime errors.
Additionally, Hindley-Milner supports type polymorphism, allowing for concise and expressive code while maintaining type correctness. 

\textbf{How does it work? (Baby steps...)}

So how does it work?
For Hindley-Milner to make much sense as a type-system, the first thing to understand is how types work on lambda calculus at all.

The lambda calculus is agnostic to typing, much in the way binary is.
The lambda calculus does not consider "type" at all and in its purest form does not have any inherent conception of values at all, which gave rise to concepts such as Church encodings\footnote{Church encodings are representations of data structures and operations solely through functions and function application, originating from lambda calculus. They enable the representation of primitive data types and operations using only functions.}. 

There is however, an extension of the lambda calculus, called the \textbf{simply typed lambda calculus}, where types are considered.
There a four typing rules in the simply typed lambda calculus:

\textbf{Simply typed lambda calculus and monomorphic types.}

\begin{equation} \label{eq:stlc-primitives-rule}
    \frac{x\text{ is constant of type }T}{\Gamma \vdash x:T} \;\;\text{Primitive}
\end{equation}

\begin{equation} \label{eq:stlc-var-rule}
    \frac{x:\sigma \in \Gamma}{\Gamma \vdash x:\sigma} \;\;\text{Var}
\end{equation}

\begin{equation} \label{eq:stlc-abstraction}
    \frac{\Gamma, x:\sigma \vdash e:\tau}{\Gamma \vdash (\lambda x:\sigma, e): (\sigma \rightarrow \tau)} \;\;\text{Abstraction}
\end{equation}

\begin{equation} \label{eq:stlc-application}
    \frac{\Gamma \vdash e_1:\sigma\rightarrow\tau \;\;\; \Gamma \vdash e_2:\sigma}{\Gamma \vdash e_1 \; e_2:\tau} \text{Application}
\end{equation}

A key description of the simply typed lambda calculus is as a monomorphic (lit., one form) type system.
In type theory, monomorphic types refer to types where variables can only represent a single, specific type.
In other words, there is no polymorphism present in monomorphic type systems. Each variable in one is associated with a fixed, concrete type, and that type cannot change.

For example, in a monomorphic type system, a variable x might be declared to have type Int, meaning it can only hold integer values.
If you attempt to assign a non-integer value to x, or use x in a context where integers are not expected, it would result in a type error.

Monomorphic types are in contrast to polymorphic (lit., many form) types, where variables can represent multiple types.

% $3 : Number$ and $add$ $3$ $4$ : $Number$ and $add : Number \rightarrow Number \rightarrow Number$



% You can add non numeric types in languages like C, but they lose any sort of reasonable significance in the process.

% (Unless one is being very clever indeed.)

% Now as contrast to monomorphic (lit. of one form, or in other words atomic) types, we have polymorphic types (lit. of many forms).

In general, polymorphism means that operations accept values of multiple types. For example, the id function:

\begin{align*} 
   \text{id-type} \; :& \; \sigma \rightarrow \sigma\\
   \text{id-body} \; :& \; \lambda x . x
\end{align*}

In the above equation, the type the id function accepts and returns is polymorphic.
The Hindley-Milner type system allows what is known as \emph{parametric} polymorphism, which can effectively give a function a "generic" type.
Then the function can be instantiated with a particular type when needed. 
For example:

\begin{align*}\label{eq:lambda-id-instantiated} 
    \text{id-type} \; :& \; int \rightarrow int \\
    \text{id-body} \; :& \; \lambda x . x
\end{align*}

The monomorphic instance of \texttt{id} is created as a result of a polymorphic type being called with an integer.
Thereby instantiating an integer version of id in the context of the application.

% \textbf{Polymorphic types also give rise to the quantification of polymorphic types.}

% For example, \texttt{cons : forall a . a -> List a -> List a}

% a is quantified in such a way that irrespective of what a is, cons only cares that both the first parameter (a) and the second (List a) have the \emph{same} a.

% Now in lambda calculus computation is represented by substitution.
% You sub-in values incrementally to compute expressions, and in the case of type-inference this means that as subsitutions occur their polymorphic types get constrainted repeatedly until they are monomorphic (or in some cases they remain polymorphic if the constraints never reach the point of monomorphic).

% For example, a monomorphic instance of id: \texttt{id' : String -> String} as id has been supplied a string argument, the function is a monomorphic function of string to string.

% Most programming languages approach type systems from the monomorphic perspective, where atmoic types are outlined, and evermore complex constructions out of them build a type system.
% % (In some languages this is more explicit like C, where making structs is in effect building new monomorphic types as composites of other atomic types.)
% % \footnote{As an interesting side note, union type structs in C are not disimilar to polymorphic types in this analogy, though they are only as polymorphic as they are explicitly made.}

% % HM is designed from the opposite direction where parametric polymorphism is emphasised.
% % A key thing to know before hand is that whilst HM is polymorphic, it does have a fundamental restriction.

% Now, typed-lambda calculus is all well and good, but there is a fundamental problem. Type inference in polymorphic lambda calculus is not decideable. (In logic, in a true/false decision problem is only decideable if there exists an effective method for derviving the correct answer.)

% \texttt{($\lambda$ id . ... (id 3) ... (id "text") ...) ($\lambda$ x . x)}

% So, HM does the next best thing: let-polymorphism.

% \texttt{let id = $\lambda$ x . x in ... (id 3) ... (id "text") ...}

% Let-polymorphism restricts the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e., are polymorphic, while parameters in lambda-abstractions are treated as monomorphic.

\subsection{Type System Formalims and Definitions}

% \subsection*{Monotypes}
\textbf{Monotypes}

Monotypes, represented by $\tau$, are fundamental types that designate specific types.
They can be categorized into two main groups: type constants such as \texttt{int}, \texttt{string}, and \texttt{bool}, and type functions like \texttt{int $\rightarrow$ string}.
It's important to note that monotypes should not be confused with monomorphic types; monotypes also include variables, which are not necessarily monomorphic.

% \subsection*{Polytypes}
\textbf{Polytypes}

Polytypes, or type schemes, are types that contain variables bound by zero or more for-all ($\forall$) quantifiers, as seen in examples like $\forall \alpha . \alpha \rightarrow \alpha$.
A function with the polytype $\forall \alpha . \alpha \rightarrow \alpha$ can map any value of the same type to itself, effectively representing the identity function.
Equality of polytypes is checked by reordering quantifications and performing $\alpha$-conversion.
Additionally, quantified variables not occurring in the monotype can be dropped.

% \subsection*{Typing Contexts}
\textbf{Typing Contexts}

Context is crucial for combining syntax expressions and types.
Syntactically, a context ($\Gamma$) is a list of pairs $(x : \sigma)$, known as bindings, where $x$ has type $\sigma$.
A typing judgement, denoted by $\Gamma \vdash e : \sigma$, asserts that under the assumptions in $\Gamma$, the expression $e$ has type $\sigma$.

% \subsection*{Free Type Variables}
\textbf{Free Type Variables}

In a type like $\forall a_1 \dots \forall a_n \cdot \tau$, the $\forall$ quantifies the type variables $a_i$ in the monotype $\tau$.
Type variables bound in this way are called quantified, and any occurrence of a quantified type variable in $\tau$ is considered \emph{bound}, while unbound variables are termed \emph{free}.
Type variables can be bound by appearing in the context ($\Gamma$), with the opposite effect on the right-hand side of the $\vdash$.
In the context of this project, all type variables are implicitly "all-quantified," so in PyFunc, the type $a \rightarrow a$ would be assumed to be quantified as $\forall a.a \rightarrow a$.

\subsection{Hindley-Milner}

\begin{itemize}
    \item Monotypes [Already explained, either remove this or incorporate it elsewhere.]
    
    Monotypes are base types.
    They always designate a specific type.
    They are represented by $\tau$.
    There are broadly two categories of monotypes, type constants like \texttt{int, string, bool}, and type functions like \texttt{int $rightarrow$ string}.
    Monotypes are not to be confused with \emph{monomorphic} types.
    Monotypes include things like variables which are not monomorphic.

    \item Polytypes [Already discussed, and this is not very useful.]
    
    Polytypes or type schemes are types containing variables bound by zero or more for-all ($\forall$) quantifiers, eg. $\forall \alpha . \alpha \rightarrow \alpha$.
    A function with polytype $\forall \alpha . \alpha \rightarrow \alpha$ can map any value of the same type to itself, which is by definiton the identity function.
    The way to check equality of polytypes is up to reordering the quantification and $\alpha$-conversion.
    Further, quantified variables not occuring in the monotype can be dropped.
    \item Context and typing [Pretty useful, keep around, maybe move it]
    
    To join together syntax expressions and types a third piece of the puzzle is desperately needed: context.
    Syntactically, a context is a list of pairs $(x : \sigma)$, called bindings.
    The example binding states that $x$ has type $\sigma$.
    A typing judgement has the form $\Gamma \vdash e : \sigma$, stating that under the assumptions $\Gamma$, the expression $e$ has type $\sigma$.
    % \item Free type variables []
    
    % In a type $\forall a_1 \dots \forall a_n \cdot \tau$, the $\forall$ is the quanitifer binding the type variables $a_i$ in the monotype $\tau$.
    % The variables $a_i$ are called quantified and any occurance of a quantified type variable in $\tau$ is called \emph{bound}.
    % The unbound variables are called \emph{free}.
    % Type variables can be bound by occuring in the given context ($\Gamma$), but with the inverse effect on the right hand side of the $\vdash$.
    % In our case, all type variables are implicitly as "all-quantified".
    % So in PyFunc, the type $a \rightarrow a$ would be quantified as: $\forall a.a \rightarrow a$.
    \item Type Order
    
    Polymorphism means that one expression can have an arbitrarily large number of types.
    But there is a constraint to this number of possible types.
    $\lambda x . x$ can have $\forall a . a \rightarrow a$ as its type as well as \texttt{string $\rightarrow$ string} or \texttt{int $\rightarrow$ int} and more.
    It cannot however have the type \texttt{int $\rightarrow$ string}.
    The most general type for this function is $\forall a . a \rightarrow a$, while the others are more specific and can be derived from the most general type via substituting a type for the type parameter ($a$).

    Formally, in HM, a type $\sigma ' $ is more general than $\sigma$, if some quantified variable in $\sigma'$ is consistently subsittuted such that one gains $\sigma$.
    Formally the statement, $\sigma ' $ is more general than $\sigma$, can be written as $\sigma' \sqsubseteq \sigma$.

    An example substitution with the given syntax would be:
    \[(\forall a . a \rightarrow a) \sqsubseteq (\texttt{string} \rightarrow \texttt{string})\]
    \item Principal types
    
    The "principal" type of a type scheme or polytype is the most general form of the type.
    \item Substitution in typings
    
    Type subsitutions that arise from the above specified type-order are consistent in their replacement.
    % \item Natural Deduction
    \item Typing rules
    \item Let Polymorphism
    \item Generalisation
    \item Inference Algorithm
    \item Unification 
    \item Algorithm J
    
\end{itemize}


% \begin{equation} \label{eq:HM-Var}
%     \frac{x : \sigma \in \Gamma}{\Gamma \vdash_D x : \sigma}\texttt{[Var]}
% \end{equation}

% \begin{equation} \label{eq:HM-App}
%     \frac{\Gamma \vdash_D e_0 : \tau \rightarrow \tau' \;\;\; \Gamma \vdash_D e_1 : \tau}{\Gamma \vdash_D e_0 e_1 : \tau'}\text{[Application]}
% \end{equation}

% \begin{equation} \label{eq:HM-Abs}
%    \frac{\Gamma, x : \tau \vdash_D e:\tau'}{\Gamma \vdash_D \lambda x.e:\tau \rightarrow \tau'}\text{[Absrtaction]}
% \end{equation}

% \begin{equation} \label{eq:HM-Let}
%     \frac{\Gamma\vdash_D e_0:\sigma \;\;\; \Gamma ,x:\sigma \vdash_D e_1 :\tau}{\Gamma \vdash_D \text{let }x = e_0\text{ in } e_1 : \tau}\text{[Let]}
% \end{equation}

% \begin{equation} \label{eq:HM-Inst}
%    \frac{\Gamma \vdash_D e:\sigma ' \;\;\; \sigma ' \sqsubseteq \sigma}{\Gamma \vdash_D e:\sigma}\text{[Instantiation]}
% \end{equation}

% \begin{equation} \label{eq:HM-Gen}
%    \frac{\Gamma \vdash_D e:\sigma \;\;\; \alpha \notin \text{free}(\Gamma)}{\Gamma \vdash_D e: \forall \alpha . \sigma}\text{[Generalisation]}
% \end{equation}

\begin{equation}
    \frac
    {x : \sigma \in \Gamma \;\;\; \tau = inst(\sigma)}
    {\Gamma \vdash x : \tau}
    \texttt{[Variable]}
\end{equation}

\begin{equation}
    \frac
    {\Gamma \vdash e_0 : \tau_0 \;\;\; \Gamma \vdash e_1 : \tau_1 \;\;\; \tau' = newvar \;\;\; unify(\tau_0, \tau_1 \rightarrow \tau')}
    {\Gamma \vdash e_0 e_1 : \tau'}
    \texttt{[Application]}
\end{equation}

\begin{equation}
    \frac
    {\tau = newvar \;\;\; \Gamma, x : \tau \vdash e: \tau'}
    {\Gamma \vdash \lambda x . e : \tau \rightarrow \tau'}
    \texttt{[Abstraction]}
\end{equation}

\begin{equation}
    \frac
    {\Gamma \vdash e_0 : \tau \;\;\; \Gamma, x : \bar \Gamma (\tau) \vdash e_1 : \tau '}
    {\Gamma \vdash \texttt{let } x = e_0 \texttt{ in } e_1 : \tau'}
    \texttt{[Let]}
\end{equation}
    
    
\newpage

\newpage
\subsection{Unification Algorithm}

The requiste unification algorithm in Hindley-Milner is not terribly extensive due to the constraints of the type system.

When unifying two types there are different ways to handle unification depending on the types themselves.

In the most basic case, unification of two monotypes will result in either an error or no error.

\[\tau_a \text{ unify } \tau_b \rightarrow \text{ if } \tau_a = \tau_b \text{ then success}\]

% \begin{lstlisting}[language=Caml]
%     let rec unify type_a type_b = 
%     match type_a, type_b with
%       | type_a, type_b when type_a = type_b -> () 
%       | TypeFunc (a1, a2), TypeFunc (b1, b2) | TypePair (a1, a2), TypePair (b1, b2) -> 
%         unify a1 b1; unify a2 b2
%       | TypeVar type_var, t | t, TypeVar type_var -> 
%         in_type_check type_var t; 
%         if Resolved
%         (
%           match UnionFind.get res_state with
%           | Unresolved -> UnionFind.set res_state (Resolved t)
%           | Resolved type_b -> unify type_b t
%         )
%       | TypeList a, TypeList b -> unify a b
%       | _, _ -> raise (UnificationError type_a type_b)
% \end{lstlisting}

\subsection{Typechecking Rules}
\section{Evaluation}
% \section{Guidance}
% Design should cover the abstract design in such a way that someone else might be able to do what you did, but with a different language or library or tool.

%==================================================================================================================================
\chapter{Implementation}
What did you do to implement this idea, and what technical achievements did you make?

The language of choice for implementation was OCaml.

All aspects of the project exist by default in OCaml.

Menhir for parsing.

OCamllex for tokenisation.

Everything else was built from scratch in the language.

\section{Parsing, Lexing and the Abstract Syntax Tree}

% To aid in a flexible yet easily modifiable syntax the following was done:
% 0. An initial pass that converts white space identation into explicit scopes for later parsing.
% 1. OCamllex was used for tokenisation.
% 2. Menhir was used as a parser generator.
% 3. A set of constructor methods are used to generate an abstract syntax tree of the frontend program.

The parsing of Pyfunc is the first major step in the language pipeline, taking the PyFunc code and converting it step by step into the intermediate represention.
This process was broken down into three steps:
\begin{enumerate}
    \item An initial pass that converts whitespace identation into explicit scopes for later parsing.
    \item Tokenisation is performed using OCamllex.
    \item The PyFunc parser converts the token stream into the frontend's abstract syntax.
\end{enumerate}

\subsection{Whitespace Indentation Conversion}

Whitespace indentation is a formatting technique used in programming languages to visually structure code.
By indenting blocks of code with consistent spaces or tabs, developers create a clear hierarchy that reflects the relationships between different parts of the code.
In many languages this indentation has no functional impact on the code's behavior but greatly enhances readability, making it easier to understand the flow of control and identify nested blocks like loops and conditionals.
Python enforces strict indentation rules as a means to gain these advantages and to explicitly delineate block scopes for structures like loops and conditionals.

% \[Python Example of whitespace indentation levels\]
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{lstlisting}[language=Python]
            def factorial(n):
                if n == 1:
                    return 1
                else:
                    return n * factorial(n - 1)
        \end{lstlisting}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{lstlisting}[language=Python]
            def factorial(n):{
                if n == 1:{
                    return 1}
                else:{
                    return n * factorial(n - 1)}}
        \end{lstlisting}
    \end{subfigure}
        
\end{figure}

PyFunc similarly enforces this rule of white space indentation for the delineation of scopes.

The specific rule is as follows:

\begin{center}
Each layer of indentation is indicated by \emph{4} spaces (tabs are \emph{not} accepted as of yet).
\end{center}

The implementation for this rule was relatively simple from an algorithm perspective:

\begin{enumerate}
    \item Take the source as a list of strings, each element representing a line of code.
    \item For each element in the list of strings:
    \begin{enumerate}
        \item Get the indentation of the line.
        \item Calculate the difference in indentation to the previous line.
        \item If the new indentation is bigger then insert a '$\{$' character.
        \item Else if the new indentation is smaller then insert a '$\}$' character.
        \item Else, the indentation is the same move to the next line.
    \end{enumerate}
    \item Return the modified source code.
\end{enumerate}

In the actual implementation the converter looked like this:

\begin{lstlisting}[language=Caml, caption=The 'preprocess' function that handles all whitespace scoping in PyFunc.]
let preprocess source =
    let get_indent str = 
      let rec count_space_helper idx = 
        if idx >= String.length str || str.[idx] <> ' ' then
          idx
        else
          count_space_helper (idx + 1)
      in
      let indent = count_space_helper 0 in
      if (indent mod 4 = 0) then
        indent / 4
      else
        raise (Parse.parse_error "Invalid indentation! 4 space indentation required.")
    in
    let rec loop prev_indent acc = function
      | [] -> 
        if prev_indent > 0 then
          let closing_braces = String.make prev_indent '}' in
          List.rev (closing_braces :: acc)
        else 
          List.rev acc
      | line :: rest ->
        let indent = get_indent line in
        let diff = abs (prev_indent - indent) in
        let amended_line =
          if indent > prev_indent then
            (String.make diff '{') ^ line
          else if indent < prev_indent then
          (String.make diff '}') ^ line
          else
            line
        in
        loop indent (amended_line :: acc) rest
    in
    let output = String.concat "\n" (loop 0 [] source) in
    output
\end{lstlisting}

Whilst the implementation is rather crude, the language itself does not have any cases currently that make this algorithm fail.
It would likely be an insufficent solution in future when the language is extended further, but this is completely functional for the language in its current state.


\subsection{Tokenisation with OCamllex}

Tokenization is the process of splitting a block of text into smaller, meaningful units called tokens.
These tokens can represent individual words, punctuation marks, numbers, or even phrases, depending on the context and requirements of the task.
For example, in the sentence "The cat is sleeping", tokenisation could break it down into three tokens: "The", "cat", and "is sleeping".
It could also break it down into four: "The", "cat", "is", and "sleeping".
Tokenisation can break down text arbitrarily, meaning that to get meaningful output there needs to exist a specification for the process of tokenisation.

The component that does this tokenisation is the lexical analyser (lexer).
The lexer reads the characters of the source code input and groups them into tokens based on predefined rules called lexical rules or tokenization rules.
These rules define how to recognize and classify different parts of the source code.

For the sake of development ease and flexibility in altering lexer rules, OCamllex, a lexer generator, was utilized, although this component can be manually created.

OCamllex takes a given specification of lexer rules and produces the lexer which implements them.

PyFunc's lexer has the following specification:

\begin{lstlisting}[language=C, caption=PyFunc's lexer specification for OCamllex., keepspaces=true]
rule token = parse
    (* Source Cleaners *)
    | [' ' '\t' '\r' '\n']* { token lexbuf }
    | '#'[^'\n']*           { token lexbuf }  (* Single line comments. *)
    | "def"     {DEFINE}
    | "True"    {TRUE}
    | "False"   {FALSE}
    | "if"      {IF}
    | "elif"    {ELIF}
    | "else"    {ELSE}
    | "None"    {UNITVAL}
    | '('       {LPAREN}
    | ')'       {RPAREN}
    | '{'       {LBRACE}
    | '}'       {RBRACE}
    | '['       {LBRACK}
    | ']'       {RBRACK}
    | '='       {EQ}
    | "and"     {AND}
    | "or"      {OR}
    | "not"     {NOT}
    | '<'       {LT}
    | '>'       {GT}
    | ">="      {GEQ}
    | "<="      {LEQ}
    | "=="      {EQQ}
    | "!="      {NEQ}
    | '+'       {PLUS}
    | '-'       {MINUS}
    | '*'       {STAR}
    | '/'       {DIVIDE}
    | "::"      {CONS}
    | '%'       {MOD}
    | ','       {COMMA}
    | ':'       {COLON}
    | eof       {EOF}
    | ['A'-'Z' 'a'-'z' '_']['A'-'Z' 'a'-'z' '_' '0'-'9' '\'']*  {ID (lexeme lexbuf)}
    | '"'       {read_string (Buffer.create 17) lexbuf}
    | ['0'-'9']+ as num_string {INTVAL (int_of_string num_string)}
    | _         { raise (lexical_error ("Illegal character: " ^ lexeme lexbuf))}
    and read_string buffer = parse
        | '"'           {STRINGVAL (Buffer.contents buffer)}
        | '\\' '/'      {Buffer.add_char buffer '/'; read_string buffer lexbuf}
        | '\\' '\\'     {Buffer.add_char buffer '\\'; read_string buffer lexbuf}
        | '\\' 'b'      {Buffer.add_char buffer '\b'; read_string buffer lexbuf}
        | '\\' 'f'      {Buffer.add_char buffer '\012'; read_string buffer lexbuf}
        | '\\' 'n'      {Buffer.add_char buffer '\n'; read_string buffer lexbuf}
        | '\\' 'r'      {Buffer.add_char buffer '\r'; read_string buffer lexbuf}
        | '\\' 't'      {Buffer.add_char buffer '\t'; read_string buffer lexbuf}
        | [^ '"' '\\']+ {Buffer.add_string buffer (Lexing.lexeme lexbuf);read_string buffer lexbuf}
        | _             {raise (Lexical_error ("Illegal string character: " ^ Lexing.lexeme lexbuf))}
        | eof           {raise (Lexical_error ("String is not terminated"))}
\end{lstlisting}

What appears in quotes is the token that is recognised in the source text.
The capitalised values in the curly braces are the tokens that are being output by the lexer.
At the bottom of the ruleset the series of buffer actions are simply the parsing of string literals into single contiguous tokens refering to the literal value of the string.

\subsection{Parsing with Menhir}

Parsing is the process of analyzing a sequence of tokens according to the rules of a formal grammar to determine its structure and meaning.
In simpler terms, parsing is like understanding the syntax of a sentence in a language.
Just as we follow grammar rules to understand the structure of sentences in human languages, parsers in computer science follow the rules of a formal grammar to understand the structure of code written in programming languages.
The parser checks whether the sequence of tokens conforms to the syntax rules defined by the grammar of the language.
If the sequence of tokens forms a valid structure according to the grammar, it is considered syntactically correct.
And in this case the parser will generate the frontend language's AST from its formal grammar.

For convenience the parser generator tool Menhir was used.
Menhir facilitates the creation of parsers by automatically generating efficent parsing code based on the provided grammar rules, much like OCamllex, though somewhat more indepth.

In PyFunc the formal grammar for the fronend language is relatively simple, with only a few constructs included in its current state.

\textbf{BREAK THIS DOWN IN TO SMALLER PIECES AND DISCUSS EACH INDIVIDUALLY (SUMMARISING ANYTHING REPETATIVE)}

\begin{lstlisting}[caption=PyFunc's full frontend grammar rules for Menhir., keepspaces=true]
    start:
    | binding+ EOF {makeProgram $1}

binding:
    | DEFINE ID LPAREN param_list RPAREN COLON scope {makeBinding $2 (makeFunc $2 $4 $7)}
    | ID EQ expr {makeBinding $1 $3}

scope: LBRACE expr RBRACE   {$2}

expr:
    | base_expr                                 {$1}
    | if_expr                                   {$1}

base_expr:
    | expr OR expr      {makeOpBinary OpBinary.And $1 $3}
    | expr AND expr     {makeOpBinary OpBinary.Or $1 $3}
    | expr NEQ expr     {makeOpBinary OpBinary.NotEqual $1 $3}
    | expr EQQ expr     {makeOpBinary OpBinary.Equal $1 $3}
    | expr LT expr      {makeOpBinary OpBinary.Less $1 $3}
    | expr LEQ expr     {makeOpBinary OpBinary.LessEqual $1 $3}
    | expr GT expr      {makeOpBinary OpBinary.Greater $1 $3}
    | expr GEQ expr     {makeOpBinary OpBinary.GreaterEqual $1 $3}
    | expr MINUS expr   {makeOpBinary OpBinary.Subtract $1 $3}
    | expr PLUS expr    {makeOpBinary OpBinary.Add $1 $3}
    | expr DIVIDE expr  {makeOpBinary OpBinary.Divide $1 $3}
    | expr STAR expr    {makeOpBinary OpBinary.Multiply $1 $3}
    | expr MOD expr     {makeOpBinary OpBinary.Mod $1 $3}
    | unary             {$1}

// Control Flow

if_expr:
    | IF expr COLON scope elif_expr   {makeIf $2 $4 $5}
    | IF expr COLON scope else_expr   {makeIf $2 $4 $5}

elif_expr:
    | ELIF expr COLON scope elif_expr {makeIf $2 $4 $5}
    | ELIF expr COLON scope else_expr {makeIf $2 $4 $5}

else_expr:
    | ELSE COLON scope {$3}

unary:
    | NOT unary   {makeOpUnary OpUnary.Not $2}
    | MINUS unary {makeOpUnary OpUnary.Negative $2}
    | PLUS unary  {makeOpUnary OpUnary.Positive $2}
    | list_op     {$1}

list_op:
    | unary CONS unary {makeOpBinary OpBinary.Cons $1 $3}
    | call {$1}

call:
    | ID LPAREN expr_list RPAREN {makeCall $1 $3}    // Call with params.
    | primary {$1}

primary:
    | LPAREN expr RPAREN {$2}
    | LBRACK expr_list RBRACK {makeList $2}
    | ID                 {try find var_table $1 with Not_found -> makeVar ($1)}
    | STRINGVAL          {makeConst (ConstString $1)}
    | INTVAL             {makeConst (ConstInt $1)}
    | TRUE               {makeConst (ConstBool true)}
    | FALSE              {makeConst (ConstBool false)}
    | UNITVAL            {makeConst ConstUnit}

// List literals.

param_list:
    | xs = separated_list(COMMA, ID) { xs }

expr_list:
    | xs = separated_list(COMMA, expr) { xs }
\end{lstlisting}

\section{IR Converter}

To take the frontend abstract syntax tree and convert it into the equivalent AST in the IR, a conversion process was necessary.
To do this conversion several things had to be given special attention (whilst others converted almost one-to-one):

1. Bindings are difficult, in that they exist in the top-level where the expression based-nature of the language raises a question: where do you start the program from.
Python starts the program quite simply at the topmost statement in the running file.
PyFunc ideally would do something like this but actually implementing this fell to the wayside when trying to correctly convert the bindinggs to an appropriate IR in the correct order.
Likely having a semantic analysis pass for collecting bindings would be a viable solution, but in the end the easy and effective option is to require a "main" function declaration to be the program entry point.

2. Functions themselves are rather complicated.
The IR has only lambdas which can take only a single argument, but the frontend allows for an arbitrary number of parameters.
To deal with this a conversion helper-function was devised.
This function constructs a let rec binding (let rec to allow recursive function definitions) from the given parameters.
Inside the let rec a series of lambdas are created for each parameter in reverse order (so as to have partial applications apply the parameters in the same order they appear in the definition).

\[CODE EXAMPLE\]

3. Assignment statements are relatively simple, yet somewhat difficult.
Having an immutable assignment is very simple to convert, as an immutable assignment or rather binding is just a let expression.
Having a mutable assignment is one, dangerous to the type system and therefore disallowed... FOR Now


\section{Intermediate Representation}

The IR is very much as designed.

Here is the AST.

It also includes these few features.

\section{Hindley-Milner Typechecker}

The type checker is the most technically difficult aspect of this project.
It required several structures and the implementation of significant pieces of the HM theory already outlined to be encoded into OCaml.

\subsection{Type Variables}

A naive description of a type variable could be something like: "A place holder that represents an unknown type".
This is true, but from an implementation standpoint is a minor issue.
Fundamentally, types can be monotypes or polytypes as previously outlined.
The distinction of polytypes and free type variables needs to be explicit:

\[\text{Polytypes are resolved types whereas free type variables are unresolved types.}\]

% Internally handling a monotype that has a singlular concrete type and a polytype that has a quantifier and an a possibly indeterminate type is an added layer of complication.
\dots

BLARG



Before type variables can be implemented, polytypes and the concept of quantifiers needs to be described.
The very simple Quantifier module does just that.

\begin{lstlisting}[language=Caml]
  module Quantifier = struct
    type t = string
    let make x = x
    let of_typeVar (tv, _) = tv (*Ignore this for now.*)
    let compare = String.compare
  end
\end{lstlisting}

The first step in the implementation of type variables, is itself describing the types that type variables will depend on.

\begin{lstlisting}[language=Caml]
  type resolution_state = Unresolved | Resolved of t
  and typevar = string * (resolution_state UnionFind.elem)
  and monoType = 
    | TypeInt
    | TypeBool
    | TypeString
    | TypeFunc of (t * t) (* A recursive definition refering to monoType. *)
    | TypePair of (t * t)
    | TypeUnit
    | TypeVar of typevar
    | TypeList of t
  and polyType = quantifier list * monoType
  and t = monoType
\end{lstlisting}

This includes some definintions that need a little clarifying:
\begin{itemize}
    \item The type \texttt{resolution\_state} is a simple way of tracking if type variables are resolved or not.
    If it is unresolved then it carries no variable, whereas if it is resolved, it carries the monotype it has been resolved to.
    \item The type \texttt{typevar} is a simple pair of an identifier, which
\end{itemize}

\begin{lstlisting}[language=Caml, caption=The TypeVar module. The module specifies the utility functions for the management of type variables.]
  module TypeVar = struct
    let id_count = ref 0
    let reset () = id_count := 0
    
    let fresh_TV ?(scope_prefix="_") () =
      let id = !id_count in
      let () = incr id_count in
      let resolution_state = UnionFind.make Unresolved in
      (scope_prefix ^ (string_of_int id), resolution_state)

    let var = fst
    let resolution_state = snd
    let compare (typeVar_a,_) (typeVar_b,_) = String.compare typeVar_a typeVar_b
  end
\end{lstlisting}

\subsection{Type-Environments and Why We Need Them}
\subsection{Unification}
\subsection{Generalisation}
\subsection{The Typerchecker}
% \subsection{The Not So Simple Typechecking}


\section{Interpreter}

The interpreter is the final stage of the pipeline.
It is in a way the simplest part of the project, being a tree-walk interpreter, but a few unusal cases made this marginally harder than expected in some areas.

\subsection{Interpreted Values}

Having an interpreted value type was necessary but also got in the way in one key case: Partial application.

Due to the function not having the sufficent parameters to totally evaluate, the interpreter would need to stop evaluating the expression and leave it be until it had all the necessary arguments.

However, this was very difficult with the structure of the interpreter being a recursive depth-first search of the IR-AST.
Initally as a patch, the option for a value to be a VTree was added.
A VTree is literally a segment of the code that will cause an error if evaluated wrapped in the value type.

But for the case of partial application it works very well.

For example, [Example with f(a, b, c) -> let f = $\lambda a. \lambda b. \lambda c. a + b + c$]

With partial application what happens is: [f (1) = VTree ($\lambda b. \lambda c. 1 + b + c$)]

When another application occurs on VTree then the VTree value is unwrapped and applied.
As it still isn't fully evaluated, it is again wrapped in the value type as a VTree.

When all the requisite applications are done it returns a value type other than a VTree.

Whilst this is still a somewhat clunky solution, it most certainly works effectively.

\subsection{Environment}

Contrary to the evironments in the type checker which were only passed down the ast as an argument, the interpreter uses a reference type to handle the env.

A important reason for using a reference type for the env was because of the complications regarding scope of values in the interpreter.

With a passed env the previous sections use of VTree would be impossible.
For example, (EXAMPLE) Basically whenever a set of tree nodes get cast a VTree, the env no longer gets passed down the recursive calls, which means that the added bindings relating to the parameters are completely lost in every partial application, resulting unresolvable errors for bindings being in and out of scope.

Using a reference env allows us to completely circumnavigate this issue entirely and set the parameters into scope and remove them only when the function has been fully applied and interpreted.

\subsection{General Rules}
Beyond these unusal edge cases which required special attention, the rest of the interpreter was largely straightforward evaluations of syntax.

Here are excepts indicating the implementations of the following rules:

1. Unary and Binary operations

2. Yada yada yada.

% \section{Guidance}
% You can't talk about everything. Cover the high level first, then cover important, relevant or impressive details.



% % \section{General points}

% These points apply to the whole dissertation, not just this chapter.



% % \subsection{Figures}
% \emph{Always} refer to figures included, like Figure \ref{fig:relu}, in the body of the text. Include full, explanatory captions and make sure the figures look good on the page.
% You may include multiple figures in one float, as in Figure \ref{fig:synthetic}, using \texttt{subcaption}, which is enabled in the template.



% % Figures are important. Use them well.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/relu.pdf}    

%     \caption{In figure captions, explain what the reader is looking at: ``A schematic of the rectifying linear unit, where $a$ is the output amplitude,
%     $d$ is a configurable dead-zone, and $Z_j$ is the input signal'', as well as why the reader is looking at this: 
%     ``It is notable that there is no activation \emph{at all} below 0, which explains our initial results.'' 
%     \textbf{Use vector image formats (.pdf) where possible}. Size figures appropriately, and do not make them over-large or too small to read.
%     }

%     % use the notation fig:name to cross reference a figure
%     \label{fig:relu} 
% \end{figure}


% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{images/synthetic.png}
%         \caption{Synthetic image, black on white.}
%         \label{fig:syn1}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%       %(or a blank line to force the subfigure onto a new line)
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{images/synthetic_2.png}
%         \caption{Synthetic image, white on black.}
%         \label{fig:syn2}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%     %(or a blank line to force the subfigure onto a new line)    
%     \caption{Synthetic test images for edge detection algorithms. \subref{fig:syn1} shows various gray levels that require an adaptive algorithm. \subref{fig:syn2}
%     shows more challenging edge detection tests that have crossing lines. Fusing these into full segments typically requires algorithms like the Hough transform.
%     This is an example of using subfigures, with \texttt{subref}s in the caption.
%     }\label{fig:synthetic}
% \end{figure}

% \clearpage

% % \subsection{Equations}

% Equations should be typeset correctly and precisely. Make sure you get parenthesis sizing correct, and punctuate equations correctly 
% (the comma is important and goes \textit{inside} the equation block). Explain any symbols used clearly if not defined earlier. 

% For example, we might define:
% \begin{equation}
%     \hat{f}(\xi) = \frac{1}{2}\left[ \int_{-\infty}^{\infty} f(x) e^{2\pi i x \xi} \right],
% \end{equation}    
% where $\hat{f}(\xi)$ is the Fourier transform of the time domain signal $f(x)$.

% % \subsection{Algorithms}
% Algorithms can be set using \texttt{algorithm2e}, as in Algorithm \ref{alg:metropolis}.

% % NOTE: line ends are denoted by \; in algorithm2e
% \begin{algorithm}
%     \DontPrintSemicolon
%     \KwData{$f_X(x)$, a probability density function returing the density at $x$.\; $\sigma$ a standard deviation specifying the spread of the proposal distribution.\;
%     $x_0$, an initial starting condition.}
%     \KwResult{$s=[x_1, x_2, \dots, x_n]$, $n$ samples approximately drawn from a distribution with PDF $f_X(x)$.}
%     \Begin{
%         $s \longleftarrow []$\;
%         $p \longleftarrow f_X(x)$\;
%         $i \longleftarrow 0$\;
%         \While{$i < n$}
%         {
%             $x^\prime \longleftarrow \mathcal{N}(x, \sigma^2)$\;
%             $p^\prime \longleftarrow f_X(x^\prime)$\;
%             $a \longleftarrow \frac{p^\prime}{p}$\;
%             $r \longleftarrow U(0,1)$\;
%             \If{$r<a$}
%             {
%                 $x \longleftarrow x^\prime$\;
%                 $p \longleftarrow f_X(x)$\;
%                 $i \longleftarrow i+1$\;
%                 append $x$ to $s$\;
%             }
%         }
%     }
    
% \caption{The Metropolis-Hastings MCMC algorithm for drawing samples from arbitrary probability distributions, 
% specialised for normal proposal distributions $q(x^\prime|x) = \mathcal{N}(x, \sigma^2)$. The symmetry of the normal distribution means the acceptance rule takes the simplified form.}\label{alg:metropolis}
% \end{algorithm}

% % \subsection{Tables}

% If you need to include tables, like Table \ref{tab:operators}, use a tool like https://www.tablesgenerator.com/ to generate the table as it is
% extremely tedious otherwise. 

% \begin{table}[]
%     \caption{The standard table of operators in Python, along with their functional equivalents from the \texttt{operator} package. Note that table
%     captions go above the table, not below. Do not add additional rules/lines to tables. }\label{tab:operators}
%     %\tt 
%     \rowcolors{2}{}{gray!3}
%     \begin{tabular}{@{}lll@{}}
%     %\toprule
%     \textbf{Operation}    & \textbf{Syntax}                & \textbf{Function}                            \\ %\midrule % optional rule for header
%     Addition              & \texttt{a + b}                          & \texttt{add(a, b)}                                    \\
%     Concatenation         & \texttt{seq1 + seq2}                    & \texttt{concat(seq1, seq2)}                           \\
%     Containment Test      & \texttt{obj in seq}                     & \texttt{contains(seq, obj)}                           \\
%     Division              & \texttt{a / b}                          & \texttt{div(a, b) }  \\
%     Division              & \texttt{a / b}                          & \texttt{truediv(a, b) } \\
%     Division              & \texttt{a // b}                         & \texttt{floordiv(a, b)}                               \\
%     Bitwise And           & \texttt{a \& b}                         & \texttt{and\_(a, b)}                                  \\
%     Bitwise Exclusive Or  & \texttt{a \textasciicircum b}           & \texttt{xor(a, b)}                                    \\
%     Bitwise Inversion     & \texttt{$\sim$a}                        & \texttt{invert(a)}                                    \\
%     Bitwise Or            & \texttt{a | b}                          & \texttt{or\_(a, b)}                                   \\
%     Exponentiation        & \texttt{a ** b}                         & \texttt{pow(a, b)}                                    \\
%     Identity              & \texttt{a is b}                         & \texttt{is\_(a, b)}                                   \\
%     Identity              & \texttt{a is not b}                     & \texttt{is\_not(a, b)}                                \\
%     Indexed Assignment    & \texttt{obj{[}k{]} = v}                 & \texttt{setitem(obj, k, v)}                           \\
%     Indexed Deletion      & \texttt{del obj{[}k{]}}                 & \texttt{delitem(obj, k)}                              \\
%     Indexing              & \texttt{obj{[}k{]}}                     & \texttt{getitem(obj, k)}                              \\
%     Left Shift            & \texttt{a \textless{}\textless b}       & \texttt{lshift(a, b)}                                 \\
%     Modulo                & \texttt{a \% b}                         & \texttt{mod(a, b)}                                    \\
%     Multiplication        & \texttt{a * b}                          & \texttt{mul(a, b)}                                    \\
%     Negation (Arithmetic) & \texttt{- a}                            & \texttt{neg(a)}                                       \\
%     Negation (Logical)    & \texttt{not a}                          & \texttt{not\_(a)}                                     \\
%     Positive              & \texttt{+ a}                            & \texttt{pos(a)}                                       \\
%     Right Shift           & \texttt{a \textgreater{}\textgreater b} & \texttt{rshift(a, b)}                                 \\
%     Sequence Repetition   & \texttt{seq * i}                        & \texttt{repeat(seq, i)}                               \\
%     Slice Assignment      & \texttt{seq{[}i:j{]} = values}          & \texttt{setitem(seq, slice(i, j), values)}            \\
%     Slice Deletion        & \texttt{del seq{[}i:j{]}}               & \texttt{delitem(seq, slice(i, j))}                    \\
%     Slicing               & \texttt{seq{[}i:j{]}}                   & \texttt{getitem(seq, slice(i, j))}                    \\
%     String Formatting     & \texttt{s \% obj}                       & \texttt{mod(s, obj)}                                  \\
%     Subtraction           & \texttt{a - b}                          & \texttt{sub(a, b)}                                    \\
%     Truth Test            & \texttt{obj}                            & \texttt{truth(obj)}                                   \\
%     Ordering              & \texttt{a \textless b}                  & \texttt{lt(a, b)}                                     \\
%     Ordering              & \texttt{a \textless{}= b}               & \texttt{le(a, b)}                                     \\
%     % \bottomrule
%     \end{tabular}
%     \end{table}
% % \subsection{Code}

% Avoid putting large blocks of code in the report (more than a page in one block, for example). Use syntax highlighting if possible, as in Listing \ref{lst:callahan}.

% \begin{lstlisting}[language=python, float, caption={The algorithm for packing the $3\times 3$ outer-totalistic binary CA successor rule into a 
%     $16\times 16\times 16\times 16$ 4 bit lookup table, running an equivalent, notionally 16-state $2\times 2$ CA.}, label=lst:callahan]
%     def create_callahan_table(rule="b3s23"):
%         """Generate the lookup table for the cells."""        
%         s_table = np.zeros((16, 16, 16, 16), dtype=np.uint8)
%         birth, survive = parse_rule(rule)

%         # generate all 16 bit strings
%         for iv in range(65536):
%             bv = [(iv >> z) & 1 for z in range(16)]
%             a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p = bv

%             # compute next state of the inner 2x2
%             nw = apply_rule(f, a, b, c, e, g, i, j, k)
%             ne = apply_rule(g, b, c, d, f, h, j, k, l)
%             sw = apply_rule(j, e, f, g, i, k, m, n, o)
%             se = apply_rule(k, f, g, h, j, l, n, o, p)

%             # compute the index of this 4x4
%             nw_code = a | (b << 1) | (e << 2) | (f << 3)
%             ne_code = c | (d << 1) | (g << 2) | (h << 3)
%             sw_code = i | (j << 1) | (m << 2) | (n << 3)
%             se_code = k | (l << 1) | (o << 2) | (p << 3)

%             # compute the state for the 2x2
%             next_code = nw | (ne << 1) | (sw << 2) | (se << 3)

%             # get the 4x4 index, and write into the table
%             s_table[nw_code, ne_code, sw_code, se_code] = next_code

%         return s_table

% \end{lstlisting}

%==================================================================================================================================
\chapter{Evaluation} 
How good is your solution? How well did you solve the general problem, and what evidence do you have to support that?

\section{Direct Comparison Code Comparison}

% \section{Guidance}
% \begin{itemize}
%     \item
%         Ask specific questions that address the general problem.
%     \item
%         Answer them with precise evidence (graphs, numbers, statistical
%         analysis, qualitative analysis).
%     \item
%         Be fair and be scientific.
%     \item
%         The key thing is to show that you know how to evaluate your work, not
%         that your work is the most amazing product ever.
% \end{itemize}
\section{Testing Agianst The CPython Test-Suite}

% \section{Evidence}
% Make sure you present your evidence well. Use appropriate visualisations, reporting techniques and statistical analysis, as appropriate.

% If you visualise, follow the basic rules, as illustrated in Figure \ref{fig:boxplot}:
% \begin{itemize}
% \item Label everything correctly (axis, title, units).
% \item Caption thoroughly.
% \item Reference in text.
% \item \textbf{Include appropriate display of uncertainty (e.g. error bars, Box plot)}
% \item Minimize clutter.
% \end{itemize}

% See the file \texttt{guide\_to\_visualising.pdf} for further information and guidance.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/boxplot_finger_distance.pdf}    

%     \caption{Average number of fingers detected by the touch sensor at different heights above the surface, averaged over all gestures. Dashed lines indicate
%     the true number of fingers present. The Box plots include bootstrapped uncertainty notches for the median. It is clear that the device is biased toward 
%     undercounting fingers, particularly at higher $z$ distances.
%     }

%     % use the notation fig:name to cross reference a figure
%     \label{fig:boxplot} 
% \end{figure}


% %==================================================================================================================================


\chapter{Conclusions}    
Summarise the whole project for a lazy reader who didn't read the rest (e.g. a prize-awarding committee).
% \section{Guidance}
\begin{itemize}
    \item
        Summarise briefly and fairly.
    \item
        You should be addressing the general problem you introduced in the
        Introduction.        
    \item
        Include summary of concrete results (``the new compiler ran 2x
        faster'')
    \item
        Indicate what future work could be done, but remember: \textbf{you
        won't get credit for things you haven't done}.
\end{itemize}

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}

\chapter{Appendices}

\section{User Guide}
\subsection{Install}
\subsection{Shell Commands}
\subsection{Example Programs}
\section{Full Parser Grammar}

Typical inclusions in the appendices are:

\begin{itemize}
\item
  Copies of ethics approvals (required if obtained)
\item
  Copies of questionnaires etc. used to gather data from subjects.
\item
  Extensive tables or figures that are too bulky to fit in the main body of
  the report, particularly ones that are repetitive and summarised in the body.

\item Outline of the source code (e.g. directory structure), or other architecture documentation like class diagrams.

\item User manuals, and any guides to starting/running the software.

\end{itemize}

\textbf{Don't include your source code in the appendices}. It will be
submitted separately.

\end{appendices}

%==================================================================================================================================
%   BIBLIOGRAPHY   

% The bibliography style is abbrvnat
% The bibliography always appears last, after the appendices.

\bibliographystyle{abbrvnat}

\bibliography{l4proj}



\end{document}
