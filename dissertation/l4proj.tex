% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.

\documentclass{l4proj}

    
%
% put any additional packages here
%

\begin{document}

%==============================================================================
%% METADATA
\title{Pythonic Syntax, Functional Core}
\author{Maxwell A. Wraith-Whiting}
\date{January 8th, 2024}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    % Every abstract follows a similar pattern. Motivate; set aims; describe work; explain results.
    % \vskip 0.5em
    % ``XYZ is bad. This project investigated ABC to determine if it was better. 
    % ABC used XXX and YYY to implement ZZZ. This is particularly interesting as XXX and YYY have
    % never been used together. It was found that  
    % ABC was 20\% better than XYZ, though it caused rabies in half of subjects.''


    % (Introduction) - The goal of the project is to produce a proof-of-concept functional programming language that utilises a Hindley-Milner type system.
    %  This is for the explicit purpose of utilising Python's ease of use whilst leveraging functional programming's correctness and type-safety.
    % (Methods) - In this project I made the prototype language and ran it in contrast to Python 3.* over several test benchmarks.
    % (Results) - The new language proved to be [GOOD / BAD]
    % (Discussion) - The future relevance of this type of language is significant as the the computer science world turns its attentions to the functional paradigm.


    % The allowance of state in modern high-level programming is a mistake.
    % Many of the most popular programming languages of today are riddled with state, for example: Python.
    % This project aims to explore and mitigate the issue of state whilst rendering runtime-errors an unlikely possiblity instead of an inevitability.
    % The [functional language] was created purely in OCaml, implementing the full language pipeline.
    % FPL performed in a [similar/disimilar] manner to Python in the selected test-suites, though as a prototype, it could not compare for more advanced features.

    In modern software development there is a search for languages that offer the expressiveness, readability, and robustness.
    Python is one of the most popular first languages among new learners standing as a highly readable, simple, versatile introduction to programming.
    However, despite its popularity Python suffers from its imperative nature and poses challenges for developers embracing the increasingly popular functional paradigm.
    This paper introduces a novel functional programming language inspired by Python’s syntax but with additional catering to functional programming.
    The language aims to bridge the gap between the accessibility of Python and the robustness of languages like Haskell and OCaml.

\end{abstract}

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
%\def\consentname {My Name} % your full name
%\def\consentdate {20 March 2018} % the date you agree
%
\educationalconsent


%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
% Do not alter the bibliography style.
%
% The first Chapter should then be on page 1. You are allowed 40 pages for a 40 credit project and 30 pages for a 
% 20 credit report. This includes everything numbered in Arabic numerals (excluding front matter) up
% to but excluding the appendices and bibliography.
%
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
%
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however. 
%
%==================================================================================================================================
\chapter{Introduction}



% reset page numbering. Don't remove this!
\pagenumbering{arabic} 

% STARTSTART
\begin{center}
    -------------------
\end{center}


% \textbf{This is drivel. What is the idea? Why should anyone care? Does it have any value?}

% [\emph{The C-like syntax of imperative languages stands as the predominant and extensively utilized programming syntax, prized for its readability and capacity to succinctly express sequences of instructions.
% Conversely, functional programming languages represent a departure from this "conventional" syntax, tracing their lineage back to predecessors like Lisp and ML rather than their widely adopted descendant.
% The functional paradigm, though lauded for its robustness and expressiveness, often presents a formidable barrier to entry for novice programmers, lacking the intuitive accessibility characteristic of more familiar programming styles.
% In contrast, Python emerges as an approachability in the programming realm, frequently heralded as an ideal introductory language owing to its user-friendly syntax, clear structure, and potent features.
% It is against this backdrop that the present project idea arises—a simple inquiry into the feasibility of bridging the gap between the safety and potency inherent in functional programming and the familiar terrain of Python-like syntax.
% Can the principles of the functional paradigm be rendered more accessible to programmers accustomed to Python's simplicity, and conversely, do Python-like languages stand to gain from incorporating these features?}]

This project aims to address the inherent limitations of Python for functional programming while leveraging its renowned readability and simplicity.
Python is popular, and commonly a new programmer's first programming language.

% Python, although popular, \textbf{lacks native support for functional constructs} and a robust type system, making it challenging for developers to embrace the functional paradigm fully.
In response, this paper proposes a novel functional programming language (PyFunc) inspired by Python's syntax but enhanced with features tailored for functional programming, such as being expression-based, including first-class functions, and having a Hindley-Milner type system.
By melding Python's approachability with the advantages of a robust type system, such as enhanced code reliability and ease of maintenance, the language seeks to lower the barrier to entry for newcomers to functional style programming while providing a prototype of a powerful tool for building large-scale, maintainable applications.


% This project does not aim to introduce groundbreaking ideas or sophisticated algorithms. 
% Instead, it's about combining existing an exisiting functional programming base with Python's user-friendly and efficient nature.

Python is a dynamically typed language, which generally means that the type of a variable is determined at runtime.

For example, here is a simple python program:
\begin{lstlisting}[language=Python, caption=A simple python program to find the factorial of a given number (n).]
    def factorial(n):
        if n == 1:
            return 1
        else:
            return n * factorial(n - 1)
    
    print(factorial(5)) # 120
\end{lstlisting}

This is a valid Python program that would run without type errors.
Typer errors themseleves are, in trivial cases, relatively easy to spot, i.e., \texttt{"hello" + 15}.
There is no definitive way to compute the result of an addition between a string and an integer, therefore in most languages this raises a type error. 
Though Python recognises these errors at runtime, it does not give any guarantees \emph{before} running the program that there are no type errors, even trivial ones.

Now in contrast to dynamic type systems, there are static type systems.
The value of a static typing system is that they \emph{do} guarantee a lack of type errors \emph{before} executing the code.
Now, the introduction of a more sophiticated type system to Python would be all well and good, but most static type systems require some level of type annotations.
An easy example of this would be the Java programming language, where every variable, function, and other construct need explicit and repetative annotations of their types.
Such annotations would be destructive to the effectiveness of the syntax and create a new set of problems for readability.

There is, however, a production-proven type system that can take code similar to the existing python code and \emph{infer} the types of the variables by how they are used.
That type system is the Hindley-Milner type system.
There is no major reason why this syntax, or at least a close expression-based equivalent, cannot have a relatively sophisticated type system.
% The inclusion of a type system onto this fundamental structure would prove only to be beneficial
The only real major caveat to the use of Hindley-Milner on Python would be the soft-requirement that the language be expression based as opposed to Python's current statement based design.
Arguably, this would be an additional benefit to the resulting programming language as the inclusion of statements into programming languages generally introduces mutability and or side-effects: two things the functional paradigm avoids.

The proposed modification to the familiar language would only marginally change the syntax:

\begin{lstlisting}[language=Python, caption=A proposed PyFunc program to find the factorial of a given number (n).]
    def factorial(n):
        if n == 1:
            1
        else:
            n * factorial(n - 1)
\end{lstlisting}


% Python is known for being easy to learn and quick to write code in, making it a solid starting point for read-able syntax.
% On the other hand, functional programming offers powerful tools but can be hard to grasp for some due to its terse syntax.
% By blending functional programming principles into Python, I hope to make these concepts more accessible to a wider audience.
% However, whether this integration will effectively simplify functional programming remains an open question.

\textbf{Contributions}

The project's core contributions lie in the development and implementation of features like a friendly Python-like syntax, a Hindley-Milner type system, and simple functional constructs.
Through research, language design, and an implementation in OCaml, the project aims to deliver a language that offers the best of both worlds, thereby facilitating smoother transitions to functional programming paradigms and improving the overall quality and maintainability of software systems.

The core contributions of this project lie in the development and implementation of the language’s features:

\begin{itemize}
    \item The simple Pythonic language frontend.
    \begin{enumerate}
        \item Design of the syntax: \ref{sec:frontend-syntax-design}
        \item Implementation of the language frontend: \ref{sec:parsing-lexing-and-the-ast}
    \end{enumerate}
    \item Hindley-Milner Type System.
    \begin{enumerate}
        \item A brief introduction introduction to natural deduction: \ref{subsec:typing-systems-and-notation}
        \item A discussion of type-inference in general and specifically in the case of Hindley-Milner: \ref{sec:type-inference-and-polymorphism}
        \item The project's implementation of the Hindley-Milner type system as a type checker: \ref{sec:hindley-milner-typechecker}
    \end{enumerate}
    \item Support for simple functional programming.
\end{itemize}

% \begin{center}
%     -------------------
% \end{center}
% ENDEND


% \textbf{Why should the reader care about what are you doing and what are you actually doing?}

% The C-like syntax of imperative languages is by far the most read and used programming syntax.
% It lends itself to simplicity and variably terse series of instructions.

% Functional programming languages are however completely distinct from this syntax style.
% They often follow predecessors languages (Lisp, ML, etc.), more closely than their ubiquitous descendant.

% The functional paradigm is one not generally approached by novice programmers.
% It is not a place that allows easy learning for those inexperienced with programming in general.

% Python is often cited [] as being a particularly easy first programming language, due to its friendly syntax, simple style, and powerful constructs.
% The idea behind this project is a simple one.
% Can the inherent safety and power of the functional paradigm be made more accessible to programmers more familiar with Python-like syntax and do Python-like languages benefit from these features.

% WHY!
% The functional paradigm itself suffers for lack of novice programmers and its academic ouvre.
% Functional languages have much to offer beyond their "catch-phrase" features.
% There exists no truly C-like functional language that does not approach academic malaise.

% I have a dream: That the safety and power, nay the \emph{joy} of programming in a functional language can be made accessible even to those only just learning to think like a programmer.
% That the complexity of certain structures in the languages do not breed the sort of terror that things like the word \emph{monad} can evoke in a Java programmer.

% \section{Guidance}

% \textbf{Motivate} first, then state the general problem clearly. 

% % \section{Writing guidance}
% % \subsection{Who is the reader?}

% This is the key question for any writing. Your reader:

% \begin{itemize}
%     \item
%     is a trained computer scientist: \emph{don't explain basics}.
%     \item
%     has limited time: \emph{keep on topic}.
%     \item
%     has no idea why anyone would want to do this: \emph{motivate clearly}
%     \item
%     might not know \emph{anything} about your project in particular:
%     \emph{explain your project}.
%     \item
%     but might know precise details and check them: \emph{be precise and
%     strive for accuracy.}
%     \item
%     doesn't know or care about you: \emph{personal discussions are
%     irrelevant}.
% \end{itemize}

% Remember, you will be marked by your supervisor and one or more members
% of staff. You might also have your project read by a prize-awarding
% committee or possibly a future employer. Bear that in mind.

% % \subsection{References and style guides}
% There are many style guides on good English writing. You don't need to
% read these, but they will improve how you write.

% \begin{itemize}
%     \item
%     \emph{How to write a great research paper} \cite{Pey17} (\textbf{recommended}, even though you aren't writing a research paper)
%     \item
%     \emph{How to Write with Style} \cite{Von80}. Short and easy to read. Available online.
%     \item
%     \emph{Style: The Basics of Clarity and Grace} \cite{Wil09} A very popular modern English style guide.
%     \item
%     \emph{Politics and the English Language} \cite{Orw68}  A famous essay on effective, clear writing in English.
%     \item
%     \emph{The Elements of Style} \cite{StrWhi07} Outdated, and American, but a classic.
%     \item
%     \emph{The Sense of Style} \cite{Pin15} Excellent, though quite in-depth.
% \end{itemize}

% \subsubsection{Citation styles}

% \begin{itemize}
% \item If you are referring to a reference as a noun, then cite it as: ``\citet{Orw68} discusses the role of language in political thought.''
% \item If you are referring implicitly to references, use: ``There are many good books on writing \citep{Orw68, Wil09, Pin15}.''
% \end{itemize}

% There is a complete guide on good citation practice by Peter Coxhead available here: \url{http://www.cs.bham.ac.uk/~pxc/refs/index.html}. 
% If you are unsure about how to cite online sources, please see this guide: \url{https://student.unsw.edu.au/how-do-i-cite-electronic-sources}.

% % \subsection{Plagiarism warning}

% \begin{highlight_title}{WARNING}
    
%     If you include material from other sources without full and correct attribution, you are commiting plagiarism. The penalties for plagiarism are severe.
%     Quote any included text and cite it correctly. Cite all images, figures, etc. clearly in the caption of the figure.
% \end{highlight_title}


%==================================================================================================================================
\chapter{Background}
% \textbf{What did other people do, and how is it relevant to what you want to do?}

The realm of programming languages boasts a rich diversity, offering numerous options known for their straightforward and easily comprehensible syntax.
Notable examples include Ruby and Python, celebrated for their simplicity and readability.
Additionally, a significant number of languages integrate elements of the Hindley-Milner type system, particularly within functional programming paradigms.
This system, renowned for its robust type inference capabilities, serves as the backbone for languages that prioritize expressive and concise code.
Academic research has made substantial contributions to the field of type inference systems, exploring various approaches and techniques to enhance type checking and inference processes.
Furthermore, scholarly endeavors have delved into language implementations, such as MiniML, to explore the intricacies of language design and implementation.
This section aims to offer a cohesive overview of these distinct areas, providing the foundation from which this project derives inspiration and insight.

% This sections intends to give a brief overview of the background ideas this project relies on, namely:
% \begin{itemize}
%     \item Python's Syntax
%     \item Functional Programming
%     \item Hindley-Milner Type Systems
% \end{itemize}

\section{Strengths of Pythonic Syntax}

% Python is popular.
% Python's continuing popularity is well-established, evident from its consistent top ranking in the TIOBE index for February 2024. [\cite{TIOBE}]
% So the language's popularity is not a question, but rather: \emph{Why} is Python so popular? 

% There are many reasons to dislike the language, as it has many 'flaws' that other languages do not.
% For example, Machine Learning and Data Science are fields that from an purely intuitive perspective would value speed over everything.

% Performance is something Python fundamentally lacks due to its internal structure, being a dynamically typed interpreted language. (Both traits adding significant runtime processing requirements.)
% It would be a lie to say Python cannot be relatively fast, but performance is not its \emph{raison d'être
% }.

% Python's popularity is well-established, evidenced by its consistent top ranking in the \cite{TIOBE} index for February 2024 and PYPL Popularity Index (March 2024). So, for what reason is Python so popular and why?

% [THIS IS PURILE FLUFF]

% While Python has garnered widespread adoption, it is not without its critics, especially regarding performance concerns.
% Python's dynamic typing and interpreted nature inherently limit its performance capabilities, resulting in significant runtime processing requirements.
% However, Python's popularity in fields like machine learning and data science seems paradoxical given its sluggishness.

% The answer is somewhat counter-intuitive: \emph{Because Python is fast.}


% The answer lies in its surprising speed: Python's strength lies not in pure CPU execution time but in its efficiency in coding, making changes, and quickly producing functional systems.
% Python's simplicity makes it accessible to beginners while remaining powerful enough for complex projects.
% Despite its reputation for slowness, Python's ability to construct programs rapidly often outweighs the CPU time it consumes.
% Readability is another significant factor in Python's appeal; its syntax, influenced by the C-family, provides a familiar structure for programmers and aids comprehension.
% Python's success as a beginner language stems from its ability to teach fundamental programming syntax.
% In essence, Python's strengths lie in its syntax and semantics rather than its performance, emphasizing its value in producing easily comprehensible code.


Python's popularity is undeniable, evident from its consistent top rankings on indices like the Tiobe Index (February 2024)\citep{TIOBE} and the PYPL Index (March 2024) .
But what makes Python so widely favored? One of its key strengths lies in its unparalleled agility, enabling rapid iteration and fostering a development environment conducive to experimentation and quick prototyping.
Moreover, Python's simplicity and readability contribute significantly to its appeal, making it accessible to both novice and experienced programmers alike.
The language's syntax, often resembling plain English, further enhances its beginner-friendly nature, minimizing the learning curve for newcomers.
However, despite its myriad advantages, Python is not without its critics.
Some argue that its performance is subpar, although it's important to note that performance isn't the primary focus of the language.
Additionally, Python's type system, characterized by duck typing, may be considered less sophisticated, leading to potential type errors at runtime.
Nevertheless, the essence of Python's allure lies not in its implementation, but rather in the inherent strengths of its syntax and semantics.

To highlight the benefits of the syntax, the following examples will contrast the same program in both Python and OCaml.


% Speed of execution is not the only bottleneck in producing systems.
% The speed of writing code, the speed of making changes, the speed of just attempting to get a working system is far more important than pure CPU execution time.
% Python performs extremely well in these areas.
% A user's time is more valuable than CPU time.
% It's really easy to get started with.
% Python is simple enough for anyone to get started in, but still powerful enough to hanlde large complex projects.

% Python is considered slow, but often it is neglected to mention "In what context? Compared to what?"

% It has reached the point of a stereotype that Python is slow.
% Oftentimes, the ability to construct a program fast, to iterate quickly, is far more valuable than the CPU time it incurs.

% The readablilty also play a significant part in the value of the language.
% It is often stated that code will be read far more than it will be written.
% The statement itself makes it intuitive that code should be inherently aimed at readability.

% Python is a descendant of the C-family of syntaxes, with a few unique twists of its own, such as white-space indentation.
% This ancestry gives programmers unfamiliar with the language a grounding, or more accurately an expectation, of how the code will behave based on its syntax.
% Obviously, the exact semantics differ, but certain universals remain.

% It is to Python's benefit that there is an effective understanding of the syntax of the language.
% It is also likely why Python is so successful as a beginner language, as it informs new programmers of one of the most prevalent syntax's the collection of all of programming languages.

% So to short: Python's strengths lie in its syntax and semantics, not necessarily its implementation.
% The value that this project aims to extract from it is not in its underlying structure but its front-facing interface as a means to producing easily comprehensible code.

\section{Benefits of the Functional Paradigm}

Functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions.
It avoids mutable state, often by enforcing data immutability.
[Way to state the obvious...]

Some of the key concepts include:

\begin{itemize}
    \item \textbf{Pure Functions:} Functions that, given the same input, always return the same output.
    This requires the restriction of access to mutable state inside functions that could interfere in the functions execution.
    (I.e., Functions do not depend on or modify variables outside of their scope.)
    \item \textbf{Immutable Data:} Data cannot be changed in-situ after creation.
    Instead of modification of data, functional programming encourages the creation of new variables with the desired changes.
    \item \textbf{Higher Order Functions:} Functions that can take functions as arguments or return them as results.
    A simple example is \emph{Map} which maps a function onto a give structure.
    \item \textbf{Referential Transparency:} The property that an expression can be replaced with its value without changing the program's behaviour. 
\end{itemize}

All of these concepts can be considered spectrums rather than hard rules, as many popular functional languages bend these concepts for usabilities sake.\footnote{OCaml for instance is not a \emph{purely} functional programming language. It technically is a functional, imperative, and object-oritened programming language.}
Though these languages do make behaviour outside of the norm specified by these rules explicit and force the programmer to be deliberate with their forays outside of the paradigm.

Functional programming offers several advantages and addresses certain challenges inherent in other paradigms, such as imperative and object-oritened programming. 

\begin{itemize}
    \item \textbf{Clarity and Maintainability:} Functional programming emphasises (and sometimes enforces) pure functions and immutable data.
    By avoiding side-effects and mutable state, functional programs tend to be easier to reason about and maintain.
    \item \textbf{Conciseness and Expressiveness:} Functional programming languages often provide high-level abstractions and expressive syntax for common programming patterns, such as map, filter, and reduce operations.
    This can lead to shorter, more concise code that is easier to write and understand.
    \item \textbf{Robustness and Predictability:} Pure functions and immutable data make functional programs more robust and predictable.
    Since pure functions have no side effects, they can be unit-tested and debugged easily.
    Additionally, immutable data structures can help prevent common bugs related to unintended modifications of data.
    \item \textbf{Formal Verification:} Functional programming languages and techniques are often amenable to formal verification techniques, which can provide guarantees about the correctness of a program.
    This is a valuable trait in safety-critical and high-assurance applications.
\end{itemize}

These numerous powerful and useful features work across many contexts, so why are functional programming languages not as commonly used as other languages like Python, Java, or JavaScript?

There is no definitive answer, but the general sentiment is that functional programming is \emph{hard.}

Most FP languages do not allow careless side-effects; most make mutability (if even possible) explicit; most virtually require the use of higher order functions. 
These features are not, in my opinion, what makes it difficult to learn functional programming. 

\emph{It is the fact that you need to know what these concepts are, before you start.}

In most popular languages, the barrier to entry is not conceptual, but syntactical.
Python, as our standard example, requires only rudimentary knowledge of the syntax to hack together something that works out of the box.

Whilst getting a functional language to print something to the standard output is often like pulling teeth by comparison.
\section{Type Systems and Hindley-Milner}

% \section{Guidance}
% \begin{itemize}    
%     \item
%       Don't give a laundry list of references.
%     \item
%       Tie everything you say to your problem.
%     \item
%       Present an argument.
%     \item Think critically; weigh up the contribution of the background and put it in context.    
%     \item
%       \textbf{Don't write a tutorial}; provide background and cite
%       references for further information.
% \end{itemize}

% What are type systems?

Type systems are a fundamental concept in programming languages.
They define a set of rules and constraints for categorising and manipulating data based on the type of data it is.
A type refers to a classification that specifies what kind of data a variable can hold and what operations can be performed on it.

% What is a Hindley-Milner Type System?

The Hindley-Milner (HM) Type System is a type system used in statically typed functional languages to automatically deduce the types of expressions without explicit type annotations.
It also supports polymorphic functions 
$\tilde{}$ It was independantly developed by Rogder Hindley in () and later extended by Robin Milner in ().
The most well known implementation of the system is in the programming language ML (Meta Language), though it exists in many languages, such as Haskell, OCaml, and Clean.

\subsection{Typing Systems and Notation} \label{subsec:typing-systems-and-notation}

In type theory, a typing rule (also called an inference rule) describes how a type system assigns types.
In a general sense, a type system is a set of typing rules which when applied to a program indicate whether the program is well-typed. (I.e., A valid program.)

The notation of these typing rules is somewhat unusual but relatively straight forward.
A given rule specifies the structure of syntax to the appropriate type.
The most basic part of the notation is a type relation:

\[3 : Int\]
\[e : \tau\]

Literal values have a definitive type, $3$ is \emph{literally} an Int value, as \texttt{true} is a boolean.
The more general rule for $e$ being of type $\tau$ is not particularly useful in this context but can easily be read as "$e$ is of type $\tau$", where $e$ refers to some expression and $\tau$ to some type.

Now, in proof theory there already exists a notation for inference rules called \textbf{natural deduction}, which typing rules use extensively.
The general structure of a rule is as a collection of propositions on the top, and the conclusions below.

\begin{equation} \label{eq:inference-syntax-example}
\frac{Proposition\: A \;\; Proposition\: B \;\; ... \;\; Proposition\: N}{Conclusion\: A \;\; Conclusion\: B \;\; ... \;\; Conclusion\: N}
\end{equation}

Here is a simple example of a type inference rule:
\begin{equation} \label{eq:inference-example}
\frac{A : true \;\;\; B : true}{(A \wedge B) : true}
\end{equation}

This rule can be read as: 'If A is \texttt{true} and B is \texttt{true} we can conclude A \emph{and} B is \texttt{true}'.

For an additional typing example, the following type rules specify the typing relations for a simple integer addition:

\begin{equation} \label{eq:add-type-rule}
\frac{}{5 : int} \;\;\; \frac{}{7 : int} \;\;\; \frac{e_1 : int \;\;\; e_2 : int}{e_1 + e_2 : int}
\end{equation}

The first two rules have no propositions and therefore can be universally concluded, which makes them \textbf{axioms}.
The third rule is an inference rule, with two integer expressions given as propositions.

This brief outline of the natural deduction typing notation should suffice for the extent of this project. 

% The final piece of notation to understand, which is of key importance to programming languages, is binding.
% The type of a variable depends on where is is bound, which requires context-sensitive typing rules.
% These rules are given by a typing judgements (sometimes refered to as assertions), usually written in the form:

% \begin{equation} \label{eq:context-example}
%     \Gamma \vdash e : \tau
% \end{equation}

% which can be read as 'expression $e$ has type $\tau$ under the typing context $\Gamma$'. On occasion it is necessary for there to be cases where these typing contexts are shown to contain specific typed variables.

% \begin{equation} \label{eq:context-introduction}
%     \frac{e:\tau \in \Gamma}{\Gamma \vdash x:\tau}
% \end{equation}

% \ref{eq:context-introduction} shows the relatively simple inference that if expression $e$ of type $\tau$ is a member of context $\Gamma$ then it can be infered that context $\Gamma$ contains expression $e$ of type $\tau$.



% ================================================================================

\section{A Brief Review of Lambda Calculus} \label{sec:lambda-review}

The IR of PyFunc is an extended lambda calculus, and therefore a brief review of the lambda calculus seems appropriate before extending it.

The lambda calculus is a formal system designed by Alonzo Church as a foundational way of expressing computation.
The system is based on \emph{function abstraction} and \emph{application}, using variable binding and subsitution.

What this means in short is very simple: You can define functions and you can apply said functions.
These functions also bind variables for their inputs.
Here is an example: 

\begin{equation} \label{eq:lambda-example}
    (\lambda x . x) y
\end{equation}

The expression in \ref{eq:lambda-example} can be read simply as a function $\lambda$ applied to variable $y$.
Inside the function when called $x$ is bound to the input value of $y$ making the expression: $(y)$.
If for example the value of $y$ was $5$ then the lambda would be $5$.
That is because the lambda in \ref{eq:lambda-example} is the identity function which returns whatever it recieves.
It also contains and demonstrates all three elements the lambda calculus as explained above.

\begin{table}[!h]
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}|l|l|l|l|@{}}
        \hline
        \textbf{Name}       &   \textbf{Syntax}     &   \textbf{Example}    &   \textbf{Explanation} \\
        \hline
        Variable & <name> & $x$ & a variable named "x" \\
        Abstraction & $\lambda$<parameter>$.$<body> & $\lambda x . x$ & a function with parameter "x" and body "x" \\
        Application & <function><variable or function> & $(\lambda x . x) y$    & calling the function "$\lambda x . x$" with arugment "y"\\
        \hline
    \end{tabular}}
\end{table}

Now here is the more explicit formulation of the lambda calculs:
\begin{enumerate}
    \item A variable is a character of string representing a parameter. 
    \item A lambda abstraction, $(\lambda x . y)$, is a function defintion, taking as input the bound variable $x$ (between the $\lambda$ and the dot) and returning the body $y$.
    \item An application of a function, $(X Y)$, is simply applying function $X$ to an argument $Y$. Both $X$ and $Y$ are lambda terms.
\end{enumerate}

Now with these three structures, there are two further rules for actually running the computations:
\begin{itemize}
    \item $(\lambda x . x) \rightarrow (\lambda y . y)$: $\alpha$-conversion, renaming bound variables in the expression. The rule is that so long as bindings are renamed consistently, they are equivalent. $\alpha$-converison is used to avoid name collisions.
    \item $((\lambda x . M) N) \rightarrow (M [x := N])$: $\beta$-reduction, replacing bound variables with the argument expression in the body of the abstraction. In this example $x$ is bound to the value $N$.
\end{itemize}


%==================================================================================================================================
\chapter{Analysis/Requirements}
% What is the problem that you want to solve, and how did you arrive at it?

% [NOTE]: What is the problem
% The dominant programming paradigm is imperative, no matter the uptick of interest in functional programming.
% The current state of functional programming does not lend itself to easy entry for new programmers who will be the future of the paradigm.
% The functional paradigm has many pitfalls for new developers:
% \begin{itemize}
%     \item Abstract concepts
%     \item Mathematical foundations
%     \item Lack of familiarity
%     \item Limited resourses and support
%     \item Complex error messages
%     \item Perceived lack of practical applications
%     \item steep learning curve
% \end{itemize}

% \section{Aims and Justification}

The aim of the project is to make functional \emph{prototype} language, where the foundational aspects of the language are set up.
This section should emphasise the actual scope for the project and create the boundaries that determine whether a feature is inside or outside the scope of the project.
There are several interesting and valuable aspects to Python, Functional programming, and type-systems that this entire project could focus on.
Yet, with feasibility in mind, the extent to which these topics can be covered has to be rigourously delineated.

\section{Minimal Viable System}

The Minimal Viable System (MVS) for the prototype language, adhering to the outlined specifications, will prioritise essential features to demonstrate core functionality while deferring more advanced or less critical features.
To do this, certain constraints must be put in place:

\begin{itemize}
    \item First, the frontend does not need to replicate each and every piece of Python syntax.
    Python has many interesting syntactic ideas which would add excessive focus onto the structure of the grammar and parsing it. 
    \item The type system needs to be fully functional over the entire language, otherwise it cannot be called a valid type system.
    \item The interpreter does not need to do anything more than run the code.
    Whilst many powerful optimisations could be implemented, it would go beyond the proof-of-concept prototype that this is.
\end{itemize}

\subsection{Frontend Syntax}

To attempt to include all the features of a mature programing language would be far greater than the expected scope of this project.
Therefore, to rein in expected features of the project, limitations will be imposed on the extent of the frontend syntax.
These limitations should, in effect, propagate through the requirements of the whole language, whilst retaining the fundamental concepts that make the language worth building.

First, it must be considered what a programming language cannot do without.
There are a few things no useful language does not have, which include: arithmetic, control structures, functions.
Arithmetic is mandatory and not a major constraint as the base implementation of operators will be necessary, but attention should be paid to making sure the implementation is extensible.

There are many possible control structures that could be used, but for the sake of simplicity, only the if-expressions will be guaranteed to be included. 

Of the things that the language cannot live without, functions are at the top of the list.
It would be ironic if a functional style language was bereft of functions.
However, whilst functions must be included, not all of Python's varied uses of them are necessary.

\textbf{Closures: }For example, Python includes the ability to create "closures" which can recall values in their defined scope even if they are not present anymore with the \texttt{nonlocal} keyword.

\begin{lstlisting}[language=Python, caption={An example of a closure in Python. See the use of the 'nonlocal' keyword to pull the surrounding defintion of 'current\_count' into the scope of 'inner\_function'.}, label={lst:py-closure}]
# Create a counter.
def make_counter():
    current_count = 0
    
    def inner_function():
        nonlocal current_count # Pull the variable into scope.
        current_count += 1
        return current_count

    return inner_function

counter = make_counter()
print(counter()) # 1
print(counter()) # 2
print(counter()) # 3
\end{lstlisting}

% Whilst it would be possible to include such a feature, the priority of such a task would be of a much lower priority than others.

\textbf{Higher Order Functions:}
A language may want to include higher-order functions to support more expressive and flexible programming paradigms. 
Higher-order functions allow functions to take other functions as arguments or return functions as results.
This enables powerful programming techniques such as function composition, partial application, and passing functions as arguments to other functions, which can lead to more concise, modular, and reusable code.

\textbf{Inner Functions:}
The necessity of including inner functions into PyFunc is not apparent.
Whilst they are of great utility for encapsulation of logic, the potential increase in complexity of bindings makes them a lesser priority than other frontend features like Higher Order Functions.

\subsection{Type-System}
The Hindley-Milner Type System offers several advantages in programming:

\begin{itemize}
    \item \textbf{Type Inference:} Hindley-Milner allows for type inference, which means that you don't have to explicitly annotate the types of every variable or function in your code.
    The type checker can often automatically infer the most general types, reducing the burden on the programmer and making the code more concise.
    \item \textbf{Type Safety:} Programs written in languages with Hindley-Milner type systems tend to have fewer type-related bugs because the compiler or interpreter can catch many type errors at compile time.
    This leads to more robust and reliable code.
    \item \textbf{Polymorphism:} Hindley-Milner supports parametric polymorphism, also known as generics in some languages.
    This allows you to write functions and data structures that can operate on a wide range of types without sacrificing type safety.
    This idea fundamentally aligns with the ideas behind a human-readable syntax, minimising 'boiler-plate' code whilst remaining expressive.
    \item \textbf{Expressiveness:} Despite being statically typed, Hindley-Milner type systems are quite expressive and flexible.
    They can handle complex type constraints and provide powerful abstractions for structuring programs.
    \item \textbf{Efficiency:} The type inference algorithm used in Hindley-Milner systems is efficent, making compilation or interpretation fast compared to more complex type systems.
\end{itemize}


Despite all of these advantageous features, it would be naive to dismiss other type systems out of hand.
For the possible type systems listed below there has been consideration given to whether they would suit such a project as this and been found inappropriate in this specific context:

\begin{itemize}
    \item \textbf{Dynamic Typing:} In languages with dynamic typing, such as Python itself, there is no static typechecking at compile time.
    Instead, types are checked at runtime, which can lead to type-related errors occuring during program execution rather than compile time. 
    While dynamic typing offers flexibility and rapid development, it can also make code harder to understand and maintain, and it may result in runtime type errors that could have been caught at compile time.
    This would be an underutilisation of the expression based PyFunc and its somewhat functional paradigm.
    Furthermore, it would leave the language with issues that Python already suffers from, like upredictable type errors at runtime.
    \item \textbf{Dependent Typing:} Dependent type systems, found in languages like Idris and Agda, allow types to depend on values.
    This allows programmers to provide very strong guarantees about program correctness and behaviour but can be complex to work with and may require explicit type annotations from the programmer.
    Adding type annotations can often dominate the syntax of the language, which defeats the point of aiming for a 'Pythonic' syntax. \footnote{Wells, Joe B. "Typability and type checking in System F are equivalent and undecidable." Annals of Pure and Applied Logic 98.1-3 (1999): 111-156.}
    Beyond this is the further problem of complexity.
    Dependent type systems are complicated, to the point of not being dissimilar from proof assistants, which are far beyond the aims of this project.
    \item \textbf{Gradual Typing:} Graudal typing combines elements from static and dynamic typing, allowing programmers to choose when to use type annotations and when to rely on dynamic typing.
    This can be useful when working in areas where static typing is impractical or overly restricitive. 
    Gradual typing could also be a relatively useful approach to take with a project like this, allowing the flexibility of dynamic typing and static typing whenever the programmer determines it is appropriate.
    But the inclusion of type annotations does go somewhat against the Pythonic style this project is aiming for.\footnote{Python \emph{does} have optional type-annotations, though it is something of a misnomer.
    PEP 3107 introduced a way to add metadata to functions and PEP 484 enhanced this to provide standard definitions.
    However, the authors have emphasised that Python shall remain a dynamically typed language.}
\end{itemize}

% In cases where no type system is used at all, programmers have complete freedom but lose the benefits of type checking, such as early error detection and improved code documentation.
% This approach can work well for small projects with dedicated teams but can become problematic as codebases grow larger and more complex.

% \subsection{Language Features}

\section{Proposed Minimal Requirements}

After the previous analysis it was determined that the MVS should include the following:

MUST HAVE:
\begin{itemize}
    \item \textbf{Functional REPL and shell tool:} Without this there is not much of a tool.
    \item \textbf{Full Language Pipeline:} A fully incorporated pipeline for all of the disparate features of the language is a must have.
    \item \textbf{Basic Primitives:} The fundamental primitives that must be included in the language are: Integers, Strings, Floats, Booleans, and a Unit\footnote{Much like in OCaml, the Unit value or () is the equivalent of a null type in many ways, but disimilar in that expressions treat evaluating to () as being voided.} value.
    \item \textbf{Basic Arithmetic:} The inclusion of basic arithmetic is an obvious inclusion but one worth stating as the type-system requires appropriate operations for types.
    (I.e., Integer addition and float addition internally are different operations, that appear the same in the frontend.)
    To be explicit the arithmetic operations to include are: +, -, $\times$, ÷, ==, >=, <=, >, <, !=.
    \item \textbf{Basic Control Structures:} Basic controls structures are a must-have feature. They must include: If-else expressions, match-expressions.
    \item \textbf{Functions:} Simple non-recursive functions and recursive functions must be included. Without recursive functions the language cannot support looping.
    \item \textbf{Closures:} Being an unusually hidden piece of syntax, closures are not as commonly used in Python though it supports them.
    Closures are undeniably very powerful, but much like inner functions, the scoping rules involved introduces additional complexity into the implementation of function scopes. 
    \item \textbf{Lists and Appropriate Operations:} List are the major data structure in Python and for any resemblanace to be accurate the prototype language must implement them in atleast a basic form.
    \item \textbf{Hindley-Milner Type System:} The type system is a fundamental structure that the benefits of this language hinge upon, and therefore it is an essential feature.
\end{itemize}

SHOULD HAVE:
\begin{itemize}
    \item \textbf{String Manipulation:} Whilst any true language implementation should include some measure of string manipulation, for this prototype it falls outside of truly critical featuers, whilst being an important addition to potentially add.
    \item \textbf{Basic IO:} A program that cannot take input or produce an output is fundamentally useless, and therefore some amount of IO (no matter how minimal) should be included to this prototype.
\end{itemize}

COULD HAVE:
\begin{itemize}
    \item \textbf{Inner Functions:} Inner functions are a useful tool for encapsulating relevant functions within other function scopes.
    However, managing the potentially complex scoping involved might require semantic analysis and more work than they are worth in a prototype language.
    \item \textbf{Standard Functions:} Python includes an array of keywords to do common tasks, the prototype language should do something similar.
    (E.g., include keywords such as \verb|len| or \verb|any| to simplify code.)
    \item \textbf{Dictionaries and Sets:} Whilst a very commonly used feature, dictionaries and sets are somewhat beyond the core scope of this project, or at least enough so to not be a primary implementation concern.
    \item \textbf{List Comprehensions:} List comprehensions are an incredibly useful feature of Python that fit nicely into the functional ethos whilst being relatively easy to read and parse.
    Such a useful construct would be a boon to the prototype languages though not critical.
\end{itemize}

WILL NOT HAVE:
\begin{itemize}
    \item \textbf{Objects:} Implementing objects into the prototype language would be closer to what users would expect of Python. However, the functional paradigm is the focus and it would defeat the point of the project to focus on Objects.
    \item \textbf{Standard Library}: Python has a large collection of modules in its standard library, therefore due to its multitudionous nature, it would be unreasonable to even attempt to include any significant proportion of the Python standard library in the prototype language. 
\end{itemize}

% \section{Guidance}
% Make it clear how you derived the constrained form of your problem via a clear and logical process. 

%==================================================================================================================================
\chapter{Design}

% How is this problem to be approached, without reference to specific implementation details? 

In this dissertation, the focus lies on crafting a prototype programming language that serves as a bridge between the clarity of easily readable imperative languages and the robustness of less approachable functional programming paradigms.

Central to the development of this language is the emphasis on achieving low coupling between its internal modules.
As a prototype, it must possess the flexibility necessary for future extensions and modifications.
Even seemingly minor design alterations can yield substantial differences in user experience, underscoring the critical importance of maintaining low coupling from the outset of development.

The following chapter will outline the frontend language of PyFunc, its intermediate representation (IR), the type checker and finally a brief overview of the interpreter.

\section{Frontend Syntax Design} \label{sec:frontend-syntax-design}

The frontend of the system draws inspiration from Python, aligning with the language's renowned emphasis on accessibility, particularly for novice programmers.
At its core, the design prioritizes ease of use, catering to individuals new to programming who may find traditional languages intimidating.
This emphasis on simplicity permeates the syntax of the language, reflecting Python's renowned straightforwardness.
By adopting a simple syntax, the language aims to lower the barrier to entry, allowing newcomers to focus more on solving problems and less on deciphering complex code structures.
This approach mirrors Python's philosophy of readability and simplicity, with the aim of fostering a welcoming environment for learners to explore programming concepts with confidence.

Quite significantly the language will be expression based instead of statement-based.
In this inital version there will be no inclusion of non-expression statements.
The only structures in the language that are not expression based are the function declarations which will be interpreted internally as bindings and therefore expressions.


% \clearpage
% \subsection{Syntax and Semantics}

% Pyfunc Operations
\begin{table}[h!]
    \caption{The standard table of operations in PyFunc, along with their syntax and accepted types. Note that though operations can accept multiple types, the left and right arguments \textbf{must} match in an expression.}\label{tab:operators}
    %\tt 
    \rowcolors{2}{}{gray!3}
    \begin{center}
    \begin{tabular}{@{}|l|l|l|@{}}
    \hline
    %\toprule
    \textbf{Operation}    & \textbf{Syntax}       &   \textbf{Valid Types}    \\ %\midrule % optional rule for header
    \hline
    Addition              & \texttt{a + b}        &   \texttt{float, integer} \\
    Subtraction           & \texttt{a - b}        &   \texttt{float, integer} \\
    Multiplication        & \texttt{a * b}        &   \texttt{float, integer} \\
    Division              & \texttt{a / b}        &   \texttt{float, integer} \\
    Positive              & \texttt{+ a}          &   \texttt{float, integer} \\
    Negation (Arithmetic) & \texttt{- a}          &   \texttt{float, integer} \\
    Modulo                & \texttt{a \% b}       &   \texttt{float, integer} \\
    % \hline
    And                   & \texttt{a and b}      &   \texttt{boolean}        \\
    Or                    & \texttt{a or b}       &   \texttt{boolean}        \\
    Negation (Logical)    & \texttt{not a}        &   \texttt{boolean}        \\
    % \hline
    Less                  & \texttt{a < b}        &   \texttt{float, integer} \\
    Greater               & \texttt{a > b}        &   \texttt{float, integer} \\
    Less or Equal         & \texttt{a <= b}       &   \texttt{float, integer} \\
    Greater or Equal      & \texttt{a >= b}       &   \texttt{float, integer} \\
    Equality              & \texttt{a == b}       &   \texttt{float, integer, boolean} \\
    Not Equality          & \texttt{a != b}       &   \texttt{float, integer, boolean} \\
    Exponentiation        & \texttt{a ** b}       &   \texttt{float, integer} \\
    % \hline
    Cons                  & \texttt{a :: b}       &   \texttt{$\forall \alpha$.$\alpha$, $\alpha \rightarrow$ List $\alpha \rightarrow$ List $\alpha$} \\
    % Concatenation         & \texttt{a + b}        &   \texttt{list, string}   \\
    % Indexing              & \texttt{a{[}k{]}}     &   \texttt{list, string}   \\
    % Sequence Repetition   & \texttt{a * i}        &   \texttt{list, string}   \\
    % Slicing               & \texttt{a{[}i:j{]}}   &   \texttt{list, string}   \\
    % \bottomrule
    \hline
    \end{tabular}
    \end{center}
\end{table}


\begin{table}[!h]
    \caption{Table of additional syntax in PyFunc.}
    \begin{center}
    \rowcolors{2}{}{gray!3}

    \begin{tabular}{@{}|l|p{5cm}|p{4cm}|@{}}
        \hline
        \textbf{Expression}  &   \textbf{Syntax}     &   \textbf{Additional Notes}   \\
        \hline
        \hline
        If else expressions & if <condition>: \newline \-\ <expression\_a> \newline else: \newline \-\ <expression\_b> & <condition> evaluates to a boolean. If <condition> is true, then expression\_a is evaluated, otherwise expression\_b is evaluated.\\
        \hline
        Immutable binding   & \texttt{a := b}   & For all subsequent expressions in a given scope \texttt{a} will alias \texttt{b}. Variable \texttt{a} cannot be rebound. \\
        % \hline
        % Mutable binding & \texttt{a = b} & For all subsequent expressions in a given scope \texttt{a} will alias \texttt{b}. Variable \texttt{a} \emph{can} be rebound.\\
        \hline
        Function Definition & def <name>(<p$_1$>, ..., <p$_n$>): \newline \-\ <expression> & Function definition\\
        \hline
        Lambda Definition & lambda(<p$_1$>, ..., <p$_n$>): <expression> & Lambda definition \\
        \hline
        
    \end{tabular}
    \end{center}
\end{table}

An example program in PyFunc might look like listing \ref{lst:design-frontend-example}, which shows the similarity of the synatx between Python and PyFunc.
\begin{lstlisting}[language=python, caption={The simple recursive function to calculate a given fibonacci number.}, label=lst:design-frontend-example]
    # Calculate a given Fibonacci number.
    def fibonacci(n):
        if n <= 1:
            n
        else:
            fibonacci(n - 1) + fibonacci(n - 2)

    # Program entry point.
    def main():
        fibonacci(7) # Returns 8
\end{lstlisting}

To have a frontend like Python, a properly described abstract syntax is necessary.
For this frontend, a list of the included features is sufficent.
\begin{itemize}
    \item Program (A list of functions.)
    \item Binding (A binding.)
    \item Variable (String for delineating variables.)
    \item Constant
    \item If expression (Condition, if-expression, else-expression.)
    \item Function (Binder, Parameters, Function body.)
    \item Parameters (An variable that an argument is bound to.)
    \item Call (A call for execution of a function.)
    \item Assignment
    \item Binary Operation
    \item Unary Operation
    \item Lists
\end{itemize}

Whilst these features might seem somewhat bare-bones, most programs do not extend significantly outside these features.
Additionally, anything more specific than this will be beyond the inital scope of the project, though quite possibly nice to have.

The specific lexicon of the frontend will be determined in the implementation.

% \clearpage
\section{IR Specification}
% \subsection{To IR or not to IR}
The decision to use an Intermediate Representation (IR) was a particularly easy one for PyFunc.
IRs offer several significant advantages, but come with an additional step of compiliation.
The main advantage for using an IR in PyFunc is the ease of development of the interpreter \emph{and} the type checker.
The IR effectively abstracts the complexities of the syntax of PyFunc away, leaving a simple and far easier to process language.
The specification of the IR however is somewhat atypical, as they are often designed with performance in mind whereas for PyFunc the focus is ease-of-development.
Specifically, the ease-of-development of the Type System.
To this end, the IR is an extended-lambda calculus.

\subsection{PyFunc-IR: An Extended Lambda Calculus}
As described in (\ref{sec:lambda-review} \emph{— A Brief Overview of Lambda Calculus}) the lambda calculus is extremely minimal, so for it to be a more workable IR, extensions are required.

The purpose of extending lambda calculus lies in enriching its expressiveness and capabilities to model more complex computations and features found in modern programming languages.
Lambda calculus, in its basic form, consists of only variables, function abstraction, and function application.
Going forward we will refer to these ideas as simply abstraction and application.

Extensions to lambda calculus introduce additional constructs and features to better capture various programming paradigms and computational concepts.

% \textbf{Some common extensions (that will be present) and their purposes include:}
\textbf{Lambda calculus extensions that will need to be included:}
\begin{enumerate}
    \item \textbf{Constants and primitive data types:} Introducing constants and primitive data types allows for the removal of all requisite encodings to make lambda calculus compute recognisable data structures. (E.g, Church encodings are no longer required to handle data representation.)
    \item \textbf{Conditionals:} Adding an if-then-else expression allows for the easy expression of branching logic and decision making in the IR.
    \item \textbf{Recursion:} Extending the lambda calculus to allow recursion allows for algorithms utilising self referencing and iterative computations.
    In other words, without recursion, looping of any kind generates comparatively immense lambda expressions which is not just unweildy but often difficult to implement.
    \item \textbf{Let bindings:} The ability to bind a variable to lambda expressions allows for more concise and readable lambda expressions.
    Additonally, it also allows for variables to be represented in the IR, which has benefits for the later PyFunc-to-IR conversion and the type checker. 
\end{enumerate}

\subsection{PyFunc-IR Types}

First, let's outline the primitive data types that PyFunc's IR supports:

\begin{table}[h]
\caption{Table of PyFunc-IR's constant types.}
\label{tab:pyfunc-ir-constants}
\begin{center}
\rowcolors{2}{}{gray!3}
\begin{tabular}{@{}|l|l|l|@{}}
    \hline
    \textbf{Type}        & \textbf{Description}      & \textbf{Syntax Examples}  \\
    \hline
    \texttt{Bool}        & Boolean type              & \texttt{True}       \\
    \texttt{Int}         & Integer type              & \texttt{42}         \\
    \texttt{Float}       & Floating-point type       & \texttt{3.141}      \\
    \texttt{String}      & String type               & \texttt{"hello"}    \\
    \texttt{Unit}        & The Unit type             & \texttt{$()$}       \\ 
    \hline
\end{tabular}
\end{center}
\end{table}

With just these five primitive types, a large majority of the types in the IR can be convered.
However, there exisits a few special cases that need to be considered to allow the IR to be fully typed.

\begin{table}[h]
\caption{Table of PyFunc-IR's type constructs.}
\label{tab:pyfunc-ir-type-constructs}
\begin{center}
\rowcolors{2}{}{gray!3}
\begin{tabular}{@{}|l|l|l|@{}}
    \hline
    \textbf{Type}        & \textbf{Description}      & \textbf{Type Construction}  \\
    \hline
    \texttt{Function}    & Function type             & \texttt{$\tau_a$ $\rightarrow$ $\tau_b$} \\
    \texttt{Pair}        & Pair type                 & \texttt{$(\tau_a, \tau_b)$} \\
    \texttt{List}        & List type                 & \texttt{List $\tau$} \\
    \texttt{Variable}    & Type Variable             & \texttt{$\sigma$} \\
    \hline
\end{tabular}
\end{center}
\end{table}

The syntax used in the type construction section has important significance to the type system we will be implementing eventually.
For clarity's sake, $\tau$ refers to a specific type that is not explicitly written, whilst $\sigma$ refers to a polytype. 

Without these type constructs it becomes impossible to infer types or reason about certain aspects of the IR.

As a simple example we have functions.
If the type checker knows what type a function takes and what type the function produces, it becomes trivial to check the validity of a function.

% \emph{- To build an abstract syntax tree for the IR, a full description of the possible tree nodes needs to be created.
% - Using the above defined primitive types and additionally including the previously outlined common extensions to the lambda calculus we can write a specification for the abstract syntax tree.}

\subsection{Abstract Syntax}

To construct an abstract syntax tree (AST) for the intermediate representation (IR) of PyFunc, the various syntax constructions need to be delineated as possible variants of a given type.
This type description will have to encompass the language's syntax in its entirety.

A simple way to construct this description is to have additional types that contain repetative information.
For example, we can take the constant types (table \ref{tab:pyfunc-ir-constants}) and wrap them as variants of a type called \texttt{Constant}.
The same can be done for the various binary and unary operators supported by the IR.
(OpBinary and OpUnary, repectively.)
With the IR being very regularly structured, and differing rather minimally from the PyFunc frontend AST, it seems appropriate to include all of the available operators in the frontend in the IR itself.

See table \ref{tab:operators} for the previously outlined operators.

Beyond just using these collated types, the AST also needs to consider type constructs (table \ref{tab:pyfunc-ir-type-constructs}).
Convinently these type constructs are effectively structured combinations of more primitive types.
This means that the AST can represent elements of the IR in much the same way.

By simply combining the previously specified collated types with the lambda calculus extensions discussed (Let, LetRec, If), a detailed AST specification can be formulated.
% To construct an abstract syntax tree (AST) for the intermediate representation (IR), it is imperative to meticulously delineate all potential tree nodes.
% This comprehensive description encompasses both primitive types and the incorporation of prevalent extensions to lambda calculus.
% By amalgamating the specified primitive types with the previously outlined lambda calculus extensions, a detailed specification for the AST can be formulated.
% This specification serves as a foundational blueprint, guiding the development of the AST and ensuring its alignment with the intended representation of the IR.
% Through this systematic approach, the AST can accurately capture the intricacies of the IR, facilitating efficient analysis and manipulation within the context of the specified computational model.

% The abstract syntax give below describes the extended lambda calculus used as PyFunc's IR.
\clearpage
\begin{lstlisting}[language=Caml, breaklines=false, label=ir-ast, caption=The Abstract Syntax for PyFunc's IR written in OCaml.]
type tree = 
    | ExprVar of variable (* String binding. *)
    | ExprConst of Constant.t (* A constant primitive. *)
    | ExprLet of (binder * tree * tree) (* Binding, let body, continuation. *)
    | ExprLetRec of (binder * tree * tree) (* Binder, e, and e'*)
    | ExprOpUnary of (OpUnary.t * tree) (* Unary Op, body *)
    | ExprOpBinary of (OpBinary.t * tree * tree) (* Binary Op, left, right *)
    | ExprFunc of (binder * tree) (* Var binder, body*)
    | ExprApplic of (tree * tree) (* Arg, Func *)
    | ExprIf of (tree * tree * tree) (* Condition, if-true, if-false *)
    | ExprPair of (tree * tree)
    | ExprLetPair of (variable * variable * tree * tree)
    | ExprFirst of tree
    | ExprSecond of tree
    | ExprList of tree list (* An OCaml list of tree nodes. *)
\end{lstlisting}


This \texttt{tree} type can completely represent the IR extended lambda calculus. 
The scope of the IR's syntax is relatively minimal, but due to the scope of the project, further extension of this IR would, one, encroach on the frontend language and, two, further complicate the type checker.

This level of complexity, being greater than the standard lambda calculus yet not becoming too distant from it seemed the most reasonable option.

In an ideal world, this would be processed further to a more optimised and compressed representation to improve the performance of the language.
However, the time to design and build an interpreter for such a representation is beyond the reasonable scope of this project.


% \begin{table}[]
%     \begin{center}
%         \rowcolors{2}{}{gray!3}
%         \begin{tabular}{|lll|}
%             \hline
%             Left-hand Type   & Binary Operator & Right-hand Type  \\ \hline
%             Int, Float       & +               & Int, Float       \\
%             Int, Float       & -               & Int, Float       \\
%             Int, Float       & $\times$        & Int, Float       \\
%             Int, Float       & ÷       & Int, Float       \\
%             Int, Float       & \%              & Int, Float       \\
%             Int, Float       & **              & Int, Float       \\
%             Int, Float       & \textless{}     & Int, Float       \\
%             Int, Float       & \textgreater{}  & Int, Float       \\
%             Int, Float       & <=    & Int, Float       \\
%             Int, Float       & >= & Int, Float       \\
%             Int, Float, Bool & ==              & Int, Float, Bool \\
%             Int, Float, Bool & !=              & Int, Float, Bool \\
%             $\alpha$         & ::              & List $\alpha$    \\ \hline
%         \end{tabular}
%         \caption{Table of valid binary operators in PyFunc IR with accepted types.}
%     \end{center}
% \end{table}
% The IR is very regularly structured.
% The inclusion of all available operators in the frontend in the IR seemed an effective strategy to minimise conversion complexity.
% It easy to add new operators to the IR
% It has little overhead


% \begin{itemize}
%     \item Variable
%     \item Constants
%     \begin{itemize}
%         \item Int
%         \item Float
%         \item String
%         \item Bool
%         \item Unit
%     \end{itemize}
%     \item Let expression
%     \item Let Rec expression
%     \item Unary operation 
%     \begin{itemize}
%         \item Positive
%         \item Negative
%         \item Logical Not
%     \end{itemize}
%     \item Binary operation
%     \begin{itemize}
%         \item Add
%         \item Subtract
%         \item Multiply
%         \item Divide
%         \item Modulo
%         \item Less
%         \item Greater
%         \item Less Equal
%         \item Greater Equal
%         \item Equal (Comparison)
%         \item Exponentiation
%         \item Concatenation
%     \end{itemize}
%     \item Lambda expression
%     \item Lambda application
%     \item If expression
%     \item Pair
%     \item Let Pair
%     \item First
%     \item Second
%     \item List
% \end{itemize}
\section{Language Pipeline}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/Language_Pipeline.png}
    \caption{A flow chart of the steps in the language pipeline, with general error types.}
\end{figure}


The general structure of the language pipeline is relatively simple by design.
The frontend language is parsed into an abstract syntax tree (AST) before being passed onto the IR converter.
The converter takes the frontend AST and maps it onto a valid IR AST in a consistent manner.
All desugaring is aimed to take place during the IR conversion process.

After, the conversion phase the AST is passed first to the type checker, which raises an error if the program has invalid types.
If there are no invalid types, the type checker simply does not raise an error.
The next step after passing the type check is interpretation, the actual running of the program.
Interpretation is final step and can be relatively simple.


\subsection{IR Conversion}

The process of IR conversion will be the integral bridge between the frontend syntax and the simpler extended lambda calculus that the IR consists of.
The conversion aims to handle many aspects of the frontend and getting them into an appropriate format for the IR.

The key things the IR converter needs to handle are:
\begin{itemize}
    \item Function bindings.
    \item Multiple parameter functions.
    \item Program entry point.
    \item Immutable and mutalbe assignments.
\end{itemize}

These problems roughly fall into three categories: Function bindings and entry point, processing lists and multiple parameters, and assignments.

\subsection*{Function Bindings and Program Entry Point}

The structure of a PyFunc program will be a tree akin to the tree in figure \ref{fig:ast-binding-conversion-tree}, with a list of top-level function bindings.
\begin{figure}[H]
\begin{center}
    \begin{forest}
        [Program 
        [Function A [* [x] [x]]]
        [Function B [** [y] [2]]]
        [Function C [- [Call A [2]] [Call B [2]]]]
        ]
    \end{forest}
\end{center}
\caption{Hypothetical PyFunc frontend abstract syntax tree.}
\label{fig:ast-binding-conversion-tree}
\end{figure}

There is no definitive way to decide between these functions which should be the program's starting point.
It could be reasonably argued that Function C has calls to the other functions and therefore it should be the entry point, but this would require some sort of entry point resolution rubric.
And beyond that, there will inevitably be ambiguious programs, not even considering recursion and the difficulties it could introduce.

So to avoid all of this complication of implicit determining of the entry point, let's make it explicit.

\begin{center}
    \emph{For a program to run, there must be a function called \texttt{main} which will be executed first.}
\end{center}

This seems to be a reasonable solution at first glance, but there is a problem.
Functions have their own scopes. 
The \texttt{x} variable in Function A only exists with in the scope of function A.
In fact, Function A \emph{does not} exist in the scope of our new entry point, Function C.

So to deal with this issue, the IR-converter must perform one additional step to handle this top-level scope.
The function bindings outside of Function C must be entered into its scope.
This conceptually looks like a lot like let expression: $\text{let } A = (\lambda x. x \times x) \text{ in } C$

So the converted IR would have to look something like figure \ref{fig:ir-binding-conversion-tree} with a series of let rec expressions (rec is to allow recursive functions at the top-level).

\begin{figure}[H]
    \begin{center}
    \begin{forest}
        [Let rec B [** [y] [2]] 
            [Let rec A [* [x][x]]
                [Let rec main [- [A [2]] [B [2]]]]
            ]
        ]
    \end{forest}
    \label{fig:ir-binding-conversion-tree}
    \caption{The tree in figure \ref{fig:ast-binding-conversion-tree} having undergone the outlined conversion process to PyFunc-IR.}
    \end{center}
\end{figure}

To explicitly state the process of conversion:

% The most important of these is the handling of function bindings as the actual structure of PyFunc is dictated by this.
% The language is aimed to be largely expression-based, but the necessary inclusion of declarations in the form of fuction definitions adds a layer of complication.
% So, as a solution to this, the IR-Converter should do the following:
\begin{enumerate}
    \item Take the list of bindings from the AST.
    \item Find the program entry point, i.e., the \texttt{main} function binding.
    \item Convert all other bindings into \texttt{let-rec} expressions with the \texttt{main} being the let expression's continuation.
\end{enumerate}

One final thing to note regarding this conversion is that if functions are calling one another in the top-level, they cannot allow forward referencing and should raise an error when used before their declaration.
However, they can be called if they are declared before their call in the top-level scope.

\begin{figure}[]
\begin{center}
    \begin{subfigure}[]{0.65\textwidth}
        \begin{lstlisting}[language=Python, caption=Example of PyFunc program that will raise an error due to a forward reference of the function B.]
            def A(y):
            y * B(2)
            
            def B(x):
            x ** 2
            
            def main():
            A(2) - B(2)
        \end{lstlisting}
    \end{subfigure}
\end{center}
\end{figure}


\subsection*{Multi-Parameter Functions}

In the IR, Multi-parameter functions are relatively easy to construct from a few distinct parts.
The IR is limited to single argument anonymous lambda functions internally.
This means that a function is \emph{not} represented by a single lambda or even just a collection of lambdas, as they do not provide any bindings outside of their expression.

% The way a function can be defined in the IR is relatively simple.
As a way to resolve this function binding problem, a let binding can be used. (Or a let rec in the case of recursive functions.)
A let binding represents the function binding with a value of a series of lambda expressions (one for each parameter of the function).
The body of this series of lambda expressions is the same as the body of the function.
% An explicit demonstration of this is in figure \ref{fig:function-to-ir}.

\begin{figure}[H]
    \begin{subfigure}{0.45\textwidth}
        \begin{center}
            \begin{lstlisting}[language=Python, keepspaces=true]
            # A simple multiparameter equation with three inputs.
            def A(a, b, c):
                a + b + c
            \end{lstlisting}
        \end{center}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \begin{center}
            \begin{equation*}
                let \; A \; = \lambda a . \lambda b. \lambda c. a + b + c \; in \dots
            \end{equation*}
        \end{center}
    \end{subfigure}
    \caption{An example of the produced IR for a multiparameter function.}
    \label{fig:function-to-ir}
\end{figure}

% To achieve the conversion of a simple function from PyFunc's frontend to the IR, a few things must happen.

\subsection*{Assignments}

Of the two kinds of assignments, immutable assignments are the easiest to implement.
An immutable assignment is in-effect just the binding of a variable to a code segment.
This means that the entire assignment is in effect a \emph{let} binding and can be treated as such.
However, there is the problem of defining the continuation of this let binding.

\begin{figure}[H]
    \begin{subfigure}{0.45\textwidth}
        \begin{center}
            \begin{lstlisting}[language=Python, keepspaces=true]
                def A():
                    # Immutable assignment of x.
                    x := 15 * 2 
                    x + 15 # 45
            \end{lstlisting}
        \end{center}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \begin{center}
            \begin{lstlisting}[language=Python, keepspaces=true]
                def A():
                    x := 15 * 2 {
                    x + 15
                    }      
            \end{lstlisting}
        \end{center}
    \end{subfigure}
    \caption{Example of immutable binding and how the continuation scope is determined.}
    \label{fig:immutable-binding-scope}
\end{figure}

The scope of the immutable binding is the scope after the binding at the same level as the definition.

\section{Type Inference and Polymorphism} \label{sec:type-inference-and-polymorphism}

Type theory is like a set of rules for organising and categorising things in programs based on what they are and what operations can be done to them.
It is not disimilar to how we categorise objects in real life, like sorting fruits into distinct groups of apples, oranges and bananas. In a way type systems are rule sets for the reasonable use of the language they are in.
They stop programmers making non-sensical programs by mistake.

\textbf{Not sure about this ($\downarrow$) paragraph.}

Type inference is an extension of this where types are infered instead of explicitly stated.
Parametric Polymorphism is a complicated way of saying that functions that can take many different types can be created. This is obviously a somewhat reductionist simplification, but it does get the gist of the feature.
The acutal power of it is somewhat harder to quantify without examples.

% \textbf{Non-technical explanation of type theory and inference:}
\subsection{A Non-Technical Explanation of Inference}
\label{sec:non-technical-inference-explanation}

Type inference is the process of deducing the types of things without being explicitly told.
Imagine there is a game, where someone gives you clues about an object, but they cannot say what it is explicitly.
For example, they might say "It's round, red and grows on trees."
From these clues you might \emph{infer} that they are talking about an apple, even if they never said the word "apple" directly. 
(Though the ambiguity in this example would likely hinder accurate inferences.)

In programming, type inference works similarly, thought with rigourous rules that are proven correct.
Instead of explicitly stating (usually, via type annotations) the types of variables, the programming language's type system tries to deduct what they are based on their usage in the code.

% \textbf{A practical example:}

Here's a practical example: Let's say you are writing a program in a language with type inference, and you have a variable $x$ that you set to $5$.
Now, without explicitly saying it, the type inference system can figure out that $x$ is an integer because you assigned it a whole number.
Later in the program, if you try to do something strage like $x \;\: +$ \texttt{"hello"}, the type system would catch that $x$ is an integer and \texttt{"hello"} is a string, so adding them together does not make sense, thereby raising an error.




Monomorphic types are in contrast to polymorphic (lit., many form) types, which can represent various types similtaneously.

% $3 : Number$ and $add$ $3$ $4$ : $Number$ and $add : Number \rightarrow Number \rightarrow Number$



% You can add non numeric types in languages like C, but they lose any sort of reasonable significance in the process.

% (Unless one is being very clever indeed.)

% Now as contrast to monomorphic (lit. of one form, or in other words atomic) types, we have polymorphic types (lit. of many forms).

In general, polymorphism means that operations accept values of multiple types. For example, the id function:
\begin{figure}[H]
\begin{align*} 
   \text{id-type} \; :& \; \sigma \rightarrow \sigma\\
   \text{id-body} \; :& \; \lambda x . x
\end{align*}
\caption{The identity function with "type-scheme" (polytype) types, as described by \citet{Hindley_1969}.}
\end{figure}

In the above equation, the type $\sigma$ the id function accepts and returns is polymorphic.
The Hindley-Milner type system allows what is known as \emph{parametric} polymorphism, which can effectively give a function a "generic" type.
Then the function can be instantiated with a particular type when needed. 
For example:

\begin{align*}\label{eq:lambda-id-instantiated} 
    \text{id-type} \; :& \; int \rightarrow int \\
    \text{id-body} \; :& \; \lambda x . x
\end{align*}

The monomorphic instance of \texttt{id} is created as a result of a polymorphic type being called with an integer.
This just means that the exact same function has been instantiated with a more restrictive type substituted for $\sigma$, which in the above example is $int$.
% Thereby instantiating an integer version of id in the context of the application.
By doing this, we have written one \texttt{id} function that can be used on many different types whilst still being predictable.

\emph{That is the benefit of parametric polymorphism.}

% \textbf{Polymorphic types also give rise to the quantification of polymorphic types.}

% For example, \texttt{cons : forall a . a -> List a -> List a}

% a is quantified in such a way that irrespective of what a is, cons only cares that both the first parameter (a) and the second (List a) have the \emph{same} a.

% Now in lambda calculus computation is represented by substitution.
% You sub-in values incrementally to compute expressions, and in the case of type-inference this means that as subsitutions occur their polymorphic types get constrainted repeatedly until they are monomorphic (or in some cases they remain polymorphic if the constraints never reach the point of monomorphic).

% For example, a monomorphic instance of id: \texttt{id' : String -> String} as id has been supplied a string argument, the function is a monomorphic function of string to string.

% Most programming languages approach type systems from the monomorphic perspective, where atmoic types are outlined, and evermore complex constructions out of them build a type system.
% % (In some languages this is more explicit like C, where making structs is in effect building new monomorphic types as composites of other atomic types.)
% % \footnote{As an interesting side note, union type structs in C are not disimilar to polymorphic types in this analogy, though they are only as polymorphic as they are explicitly made.}

% % HM is designed from the opposite direction where parametric polymorphism is emphasised.
% % A key thing to know before hand is that whilst HM is polymorphic, it does have a fundamental restriction.

% Now, typed-lambda calculus is all well and good, but there is a fundamental problem. Type inference in polymorphic lambda calculus is not decideable. (In logic, in a true/false decision problem is only decideable if there exists an effective method for derviving the correct answer.)

% \texttt{($\lambda$ id . ... (id 3) ... (id "text") ...) ($\lambda$ x . x)}

% So, HM does the next best thing: let-polymorphism.

% \texttt{let id = $\lambda$ x . x in ... (id 3) ... (id "text") ...}

% Let-polymorphism restricts the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e., are polymorphic, while parameters in lambda-abstractions are treated as monomorphic.

\

\subsection{The Simply Typed Lambda-Calculus}

% \textbf{Now say why HM is better. (Type inference without annotations. Complete. Decideable.)}
A key description of the simply typed lambda calculus is as a monomorphic (lit., one form) type system.
In type theory, monomorphic types refer to types where variables can only represent a single, specific type.
In other words, there is no polymorphism present in monomorphic type systems. Each variable in one is associated with a fixed, concrete type, and that type cannot change.

For example, in a monomorphic type system, a variable x might be declared to have type Int, meaning it can only hold integer values.
If you attempt to assign a non-integer value to x, or use x in a context where integers are not expected, it would result in a type error.


% \textbf{How does it work? (Baby steps...)}

\emph{So how does it work? And how would it be implemented?}

For the simply typed lambda calculus to make much sense as a type-system, the first thing to understand is how types work on the lambda calculus at all.

The lambda calculus is agnostic to typing, much in the way binary is.
The lambda calculus does not consider "type" at all and in its purest form does not have any inherent conception of values at all, which gave rise to concepts such as Church encodings\footnote{Church encodings are representations of data structures and operations solely through functions and function application, originating from lambda calculus. They enable the representation of primitive data types and operations using only functions.}. 

There is however, an extension of the lambda calculus, called the \textbf{simply typed lambda calculus}, originally devised by \citet{Church_1940}, where types are incorporated.

There are four\footnote{This is not \emph{technically} true, as the "primitive" rule represents an arbitrary number of rules that indicate the base types for the STLC.} major rules that can be derived for the simply typed lambda calculus\citep{Stlc_upenn}:

% \textbf{Simply typed lambda calculus and monomorphic types.}

\begin{equation*} \label{eq:stlc-primitives-rule}
    \frac{e\text{ is constant of type }T}{\Gamma \vdash e:T} \;\;\text{[Primitive]}
\end{equation*}

\begin{equation*} \label{eq:stlc-var-rule}
    \frac{x:\sigma \in \Gamma}{\Gamma \vdash x:\sigma} \;\;\text{[Variable]}
\end{equation*}

\begin{equation*} \label{eq:stlc-abstraction}
    \frac{\Gamma, x:\sigma \vdash e:\tau}{\Gamma \vdash (\lambda x:\sigma, e): (\sigma \rightarrow \tau)} \;\;\text{[Abstraction]}
\end{equation*}

\begin{equation*} \label{eq:stlc-application}
    \frac{\Gamma \vdash e_1:\sigma\rightarrow\tau \;\;\; \Gamma \vdash e_2:\sigma}{\Gamma \vdash e_1 \; e_2:\tau} \text{[Application]}
\end{equation*}

These rules are all very simple when written out in words:

\begin{enumerate}
    \item \textbf{Primitive rule:} If a given variable $x$ is a constant of type $T$ then it can be inferred from the context that $x$ is of type $T$.
    
    Whilst this rule may seem redundant, without it type inference cannot occur, as it specifies the assignment of base types which would be infered.
    \item \textbf{Variable rule:} If $x$ is of type $\sigma$ in the context $\Gamma$ then it can be inferred from the context that $x$ is of type $\sigma$.
    
    This rule shows how the context can hold type assignments.
    Think of $\Gamma$ as a list of these assignments.
    \item \textbf{Abstraction Rule:} If in a given context $\Gamma$, $x$ is of type $\sigma$, related to $e$ of type $\tau$ then it can be inferred in the context $\Gamma$ that the $\lambda$ function $x$ has type $\sigma \rightarrow \tau$.
    
    This rule might seem a little convoluted, but all it means is that if a lambda expression takes $\sigma$ and returns $\tau$ it has the type $\sigma \rightarrow \tau$.

    \item \textbf{Application rule:} If in the context $\Gamma$ $e_1$ has the type $\sigma \rightarrow \tau$ \emph{and} $e_2$ has the type $\sigma$, it can be inferred that the application of $e_1 e_2$ has the type $\tau$.
    
    This rule is nice and easy.
    You have a function that takes $\sigma$ and returns $\tau$.
    If you give it $\sigma$ you get $\tau.$
    It is that simple.
\end{enumerate}

Now with just these rules the lambda calculus can be typed.

Here is an example usage of the simply typed lambda calculus to prove that $(\lambda x:\textbf{int}. x) 2$ is well-typed.
\begin{figure}[H]
    \small
    \begin{mathpar}
        \inferrule*[right=App.]{
            \inferrule*[right=Abs.]{
                \inferrule*[right=Var.]
                { } {x:\textbf{int} \vdash x : \textbf{int}}
            }
            { \vdash (\lambda x  : \textbf{int} . x ): \textbf{int} \rightarrow \textbf{int}}\;\;
            \inferrule*[right=Primitive]
            { } { \vdash 2 : \textbf{int}}
        }
        {
            \vdash (\lambda x : \textbf{int}. x) 2 : \textbf{int}
        }
    \end{mathpar}
    \label{fig:stlc-proof-example}
    \caption{A proof using the simply typed lambda calculus that $(\lambda x:\textbf{int}. x) 2$ is a well-typed expression.}
\end{figure}

It should be evident that whilst functional, this style of typing is insufficent for the goal of annotation-less programs, as the lambda expression required an $int$ annotation.
The next few steps from here are some of the most difficult in the project as a whole.

% \subsection{Type System Formalims and Definitions [Might kill this sec]}

% \textbf{Monotypes}

% Monotypes, represented by $\tau$, are fundamental types that designate specific types.
% They can be categorized into two main groups: type constants such as \texttt{int}, \texttt{string}, and \texttt{bool}, and type functions like \texttt{int $\rightarrow$ string}.
% It's important to note that monotypes should not be confused with monomorphic types; monotypes also include variables, which are not necessarily monomorphic.

% \textbf{Polytypes}

% Polytypes, or type schemes, are types that contain variables bound by zero or more for-all ($\forall$) quantifiers, as seen in examples like $\forall \alpha . \alpha \rightarrow \alpha$.
% A function with the polytype $\forall \alpha . \alpha \rightarrow \alpha$ can map any value of the same type to itself, effectively representing the identity function.
% Equality of polytypes is checked by reordering quantifications and performing $\alpha$-conversion.
% Additionally, quantified variables not occurring in the monotype can be dropped.

% \textbf{Typing Contexts}

% Context is crucial for combining syntax expressions and types.
% Syntactically, a context ($\Gamma$) is a list of pairs $(x : \sigma)$, known as bindings, where $x$ has type $\sigma$.
% A typing judgement, denoted by $\Gamma \vdash e : \sigma$, asserts that under the assumptions in $\Gamma$, the expression $e$ has type $\sigma$.

% \textbf{Free Type Variables}

% In a type like $\forall a_1 \dots \forall a_n \cdot \tau$, the $\forall$ quantifies the type variables $a_i$ in the monotype $\tau$.
% Type variables bound in this way are called quantified, and any occurrence of a quantified type variable in $\tau$ is considered \emph{bound}, while unbound variables are termed \emph{free}.
% Type variables can be bound by appearing in the context ($\Gamma$), with the opposite effect on the right-hand side of the $\vdash$.
% In the context of this project, all type variables are implicitly "all-quantified," so in PyFunc, the type $a \rightarrow a$ would be assumed to be quantified as $\forall a.a \rightarrow a$.

\subsection{Hindley-Milner}
% It should be noted that there are many different styles of type systems with their own inference algorithms, each of which have their own strengths and weaknesses.


% It provides strong type safety guarantees, catching errors at compile time and reducing the likelihood of runtime errors.
% Additionally, Hindley-Milner supports type polymorphism, allowing for concise and expressive code while maintaining type correctness. 

Now to get from the simply typed lambda calculus (STLC) to Hindley-Milner (HM), one needs to take another look at type annotations.
The Hindley-Milner type system is highly regarded for its simplicity, efficiency, and ability to infer principal types accurately \emph{without requiring explicit type annotations}.

In the previous section $(\lambda x:\textbf{int}. x) 2$ was processed under the STLC and deemed well-typed.
If the \textbf{int} annotation is removed then the STLC will not be able to determine the type of $x$ and it therefore would be deemed ill-typed.

If we want to remove the annotation, we must consider the type of the lambda to be something more general.
We previously described polytypes regarding the identity function \texttt{id}, which encompass the idea of a more general type than a monotype, allowing for specific instantiation of more specified types.
That is one of the key rules that allows HM to function.

\citet{Milner_1978} outlined the requiste typing rules for an efficent implementation of this polymorphic type system, known as Algorithm W.

Algorithm W has six rules which are very similar to those of the STLC, but there are two additional rules that handle the specificity of polymorphic types directly.

As a brief note on the syntax of these rules; polytypes can be ordered by how "general" they are, for example, $\sigma_a$ could be called more general than $\sigma_b$ if some substitution into $\sigma_a$ can transform it into $\sigma_b$.
The formal syntax for this is $\sigma_a \sqsubseteq \sigma_b$.

\begin{mathpar}
    \inferrule*[right=Specialisation]
    {\Gamma \vdash e: \sigma_a \;\;\; \sigma_a \sqsubseteq \sigma_b}
    {\Gamma \vdash e: \sigma_b}
\end{mathpar}

\begin{mathpar}
    \inferrule*[right=Generalisation]
    {\Gamma \vdash e : \sigma \;\;\; \alpha \notin \mathit{free}(\Gamma)}
    {\Gamma \vdash e : \forall \alpha. \sigma}
\end{mathpar}

With the first of these two typing rules, a given polytype can be instantiated, i.e., made into a specific type, as seen in the \texttt{id} example before (\ref{sec:non-technical-inference-explanation}).

Generalisation in this case is quite simple in that it checks whether the type if free within the given context ($\Gamma$) and if not allows the type to be polymorphic.

For example, $\texttt{id} : \lambda x . x $ has no specifically typed reference to the type it takes. I.e., the $+$ operation for example requires an integer.
As, \texttt{id} does not have any type imposed on $x$, it can be generalised into a polytype.

\subsection*{Algorithm J}

Now, whilst algorithm W is the algorithm we have been talking about, there is another implementation called algorithm J \citep{Milner_1978} \citep{Damas_Milner_1982}.
Algorithm J is technially not a "proper" set typing of typing rules as it allows for side-effects.
However, Algorithm J is an efficent implementation of the type inference algorithm.

Algorithm J has six typing rules in its definition which are:
\begin{itemize}
    \item Variable Rule
    \item Application Rule
    \item Abstraction Rule
    \item Let Rule
    \item Instantiation Rule
    \item Generalisation Rule
\end{itemize}

However, \citet{Clement_Despeyroux_Kahn_Despeyroux_1986} simplified the typing rules\footnote{It should be noted that this simplification did reduce the completeness of the rules, though only marginally.} further until there was only four by combining specialisation into the variable rule and generalisation into the let rule.

Here are a simplified interpretetation of the outlined rules:

\begin{mathpar}
    \inferrule*[right=Variable]
    {x : \sigma \in \Gamma \;\;\; \tau = \mathit{instance}(\sigma)}
    {\Gamma \vdash x : \tau}
\end{mathpar}

\begin{mathpar}
    \inferrule*[right=Application]
    {\Gamma \vdash e_0 : \tau_0 \;\;\; \Gamma \vdash e_1 : \tau_1 \; \tau'=\mathit{fresh\_var} \;\;\; \mathit{unify}(\tau_0, \tau_1 \rightarrow \tau')}
    {\Gamma \vdash e_0 e_1 : \tau'}
\end{mathpar}

\begin{mathpar}
    \inferrule*[right=Abstraction]
    {\tau=fresh\_var \;\;\; \Gamma, x : \tau \vdash e : \tau'}
    {\Gamma \vdash \lambda x . e : \tau \rightarrow \tau '}
\end{mathpar}

\begin{mathpar}
    \inferrule*[right=Let]
    {\Gamma \vdash e_0: \tau \; \Gamma, x : \bar \Gamma(\tau) \vdash e_1 : \tau'}
    {\Gamma \vdash \text{let } x = e_0 \text{ in } e_1:\tau'}
\end{mathpar}

The $instance(\sigma)$ operation takes $\sigma$ and relaces all bound type variables with a new monotype (i.e., implementing a new type instance for the variable.)

The $fresh\_var$ operation quite simply produces a new monotype on demand.

$\bar \Gamma(\tau)$ refers to generalisation, as specified previously, where the given type is quantified to its most general form.
It should be noted however, that \emph{unification} is the process that takes place here to determine the most general form of the given terms.

Now with these four typing rules, the lambda caculus can now infer types for polymorphic functions.

% \begin{itemize}
%     \item Monotypes [Already explained, either remove this or incorporate it elsewhere.]
    
%     Monotypes are base types.
%     They always designate a specific type.
%     They are represented by $\tau$.
%     There are broadly two categories of monotypes, type constants like \texttt{int, string, bool}, and type functions like \texttt{int $rightarrow$ string}.
%     Monotypes are not to be confused with \emph{monomorphic} types.
%     Monotypes include things like variables which are not monomorphic.

%     \item Polytypes [Already discussed, and this is not very useful.]
    
%     Polytypes or type schemes are types containing variables bound by zero or more for-all ($\forall$) quantifiers, eg. $\forall \alpha . \alpha \rightarrow \alpha$.
%     A function with polytype $\forall \alpha . \alpha \rightarrow \alpha$ can map any value of the same type to itself, which is by definiton the identity function.
%     The way to check equality of polytypes is up to reordering the quantification and $\alpha$-conversion.
%     Further, quantified variables not occuring in the monotype can be dropped.
%     \item Context and typing [Pretty useful, keep around, maybe move it]
    
%     To join together syntax expressions and types a third piece of the puzzle is desperately needed: context.
%     Syntactically, a context is a list of pairs $(x : \sigma)$, called bindings.
%     The example binding states that $x$ has type $\sigma$.
%     A typing judgement has the form $\Gamma \vdash e : \sigma$, stating that under the assumptions $\Gamma$, the expression $e$ has type $\sigma$.
%     % \item Free type variables []
    
%     % In a type $\forall a_1 \dots \forall a_n \cdot \tau$, the $\forall$ is the quanitifer binding the type variables $a_i$ in the monotype $\tau$.
%     % The variables $a_i$ are called quantified and any occurance of a quantified type variable in $\tau$ is called \emph{bound}.
%     % The unbound variables are called \emph{free}.
%     % Type variables can be bound by occuring in the given context ($\Gamma$), but with the inverse effect on the right hand side of the $\vdash$.
%     % In our case, all type variables are implicitly as "all-quantified".
%     % So in PyFunc, the type $a \rightarrow a$ would be quantified as: $\forall a.a \rightarrow a$.
%     \item Type Order
    
%     Polymorphism means that one expression can have an arbitrarily large number of types.
%     But there is a constraint to this number of possible types.
%     $\lambda x . x$ can have $\forall a . a \rightarrow a$ as its type as well as \texttt{string $\rightarrow$ string} or \texttt{int $\rightarrow$ int} and more.
%     It cannot however have the type \texttt{int $\rightarrow$ string}.
%     The most general type for this function is $\forall a . a \rightarrow a$, while the others are more specific and can be derived from the most general type via substituting a type for the type parameter ($a$).

%     Formally, in HM, a type $\sigma ' $ is more general than $\sigma$, if some quantified variable in $\sigma'$ is consistently subsittuted such that one gains $\sigma$.
%     Formally the statement, $\sigma ' $ is more general than $\sigma$, can be written as $\sigma' \sqsubseteq \sigma$.

%     An example substitution with the given syntax would be:
%     \[(\forall a . a \rightarrow a) \sqsubseteq (\texttt{string} \rightarrow \texttt{string})\]
%     \item Principal types
    
%     The "principal" type of a type scheme or polytype is the most general form of the type.
%     \item Substitution in typings
    
%     Type subsitutions that arise from the above specified type-order are consistent in their replacement.
%     % \item Natural Deduction
%     \item Typing rules
%     \item Let Polymorphism
%     \item Generalisation
%     \item Inference Algorithm
%     \item Unification 
%     \item Algorithm J
    
% \end{itemize}


\subsection{Unification Algorithm}

Unification is the fundamental back bone of the type inference piece of the type checker.
But what is unification, in this context?

In this case, it is expedient to start with an example.
Suppose there are the functions 'resverse' and 'sum' with the following type signatures:

\begin{align*}
    \mathit{reverse} &: \forall \text{a} . [\text{a}] \rightarrow [\text{a}] \\
    \mathit{sum} &: [\text{Int}] \rightarrow \text{Int}
\end{align*}

The 'reverse' function is a polymorphic function that literally reverses a given list irrespective of the type inside the list.
The 'sum' function adds all of the terms in the list together.

Now with these functions we can create an example problem for the type checker:

\begin{align*}
    \lambda xs . \; \mathit{sum} \; (\mathit{reverse} \; xs) \\
\end{align*}

So we know that we do not know the type of $xs$ as it is a lambda variable, so we can place a type variable in its place to represent the unresolved type.
In this case we will say $(xs : \alpha)$.

First, we must typecheck 'reverse' $xs$.

For 'reverse' we must first instatitate the function as a specific type because it is polymorphic and therefore ranges over many types.
So, an instance of reverse will have a specific type replacing 'a' with a type variable ($\beta$), as a stand-in for the unknown type.

I.e, \[reverse' : [\beta] \rightarrow [\beta]\]


To finish constraint generation with 'reverse', first we understand that 'reverse' expected an argument of type [$\beta$] and the actual argument passed to 'reverse' was $xs$ whose type is $\alpha$.

With this information we can say that the constraint $\alpha = [\beta]$ is created.

For typechecking 'sum' we do not need to instantiate anything as the types involved are explicit types.
So all that needs doing for this function to generate the constraint is understand that the expected input type is [Int] and what is supplied to it is [$\beta$].
Therefore another constraint can be defined:
\[[\beta] = [\text{Int}]\]

With these constraints: $\alpha = [\beta]$ and $[\beta] = [\text{int}]$ it is \emph{almost} trivial to solve infer the types in the initial expression.

\[\lambda \underbrace{xs}_{xs : \text{[Int]}} . \; \overbrace{\mathit{sum}}^{sum : \text{[Int]} \rightarrow \text{Int}} \; (\underbrace{\mathit{reverse}}_{reverse : \text{[Int]} \rightarrow \text{[Int]}} \; xs)\]

Now that we have our constraints and it is relatively obvious how to use them to solve the types in a given expression — how do we algorithmically solve them?

\subsection*{Robinson's Unficiation Algorithm}

The process of taking these constraints and solving them for a substitution is called \emph{unification}\citep{Robinson_1965}.

The requiste unification algorithm in Hindley-Milner is not terribly extensive due to the possible constraints generated in the type system.
The classic Hindley Milner type system does not support things like higher-order types, or type functions, therefore, in this case, the unification algorithm does not need to handle anything beyond the inference of relatively basic types.

A well established solution to unification is Robinson's Unfication algorithm \citep{Robinson_1965}.
% The algorithm is presented in the form of pseudo-code below.
The algorithm aims to find the most general unifier for two terms, denoted by $t_1$ and $t_2$.
The algortithm is presented formally below.

% \begin{lstlisting}[language=Caml, caption=A pseudo code representation of Robinson's Unification Algorithm.]
% function unify(t1, t2):
%     if t1 is a variable and t2 is not:
%         return bind(t1, t2)
%     else if t2 is a variable and t1 is not:
%         return bind(t2, t1)
%     else if t1 is a variable and t2 is a variable:
%         if t1 == t2:
%             return {}
%         else:
%             return bind(t1, t2)
%     else if t1 and t2 are compound terms:
%         if t1's functor != t2's functor:
%             return FAILURE
%         else:
%             return unify(t1's arguments, t2's arguments)
%     else:
%         return FAILURE

% function bind(var, term):
%     if occurs_check(var, term):
%         return FAILURE
%     else:
%         return { var := term }

% function occurs_check(var, term):
%     if term is a variable:
%         return var == term
%     else if term is a compound term:
%         return any(occurs_check(var, subterm) for subterm in term's subterms)
%     else:
%         return false
% \end{lstlisting}



    % An example substitution with the given syntax would be:
    % \[(\forall a . a \rightarrow a) \sqsubseteq (\texttt{string} \rightarrow \texttt{string})\]
    % \item Principal types
    
    % An example substitution with the given syntax would be:
    % \[(\forall a . a \rightarrow a) \sqsubseteq (\texttt{string} \rightarrow \texttt{string})\]
    % \item Principal types
    
    % Type subsitutions that arise from the above specified type-order are consistent in their replacement.
    % % \item Natural Deduction
    % \item Typing rules
    % \item Let Polymorphism
    % \item Generalisation
    % \item Inference Algorithm
    % \item Unification 
    % \item Algorithm J

    % Type subsitutions that arise from the above specified type-order are consistent in their replacement.
    % % \item Natural Deduction
    % \item Typing rules
    % \item Let Polymorphism
    % \item Generalisation
    % \item Inference Algorithm
    % \item Unification 
    % \item Algorithm J

\begin{center}
\begin{minipage}{0.8\linewidth}%
\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$t_1$, $t_2$: two terms to unify. \; $\sigma$: an empty substitution.}
\KwResult{A substitution that is the most general substitution for $t_1$ and $t_2$. Otherwise an error.}
\Begin{
    $\sigma := \{\}$\;
    \uIf {$t_1$ is a variable and $t_2$ is not}
        {return \emph{bind}($t_1, t_2$)}
    \uElseIf {$t_2$ is a variable and $t_1$ is not}
        {return \emph{bind}($t_2$, $t_1$)}
    \uElseIf {$t_1$ is a variable and $t_2$ is a variable}
        {\uIf {$t_1$ = $t_2$}
            {return $\{\}$}
         \uElse
         {
            return \emph{bind}($t_1$, $t_2$)
         }
        }
    \uElseIf {$t_1$ and $t_2$ are compound terms}
        {
        \uIf {$t_1$'s functor $\neq$ $t_2$'s functor}
            {return FAILURE}
        \uElse {
            return \emph{unify}($t_1$'s arguments, $t_2$'s arguments)
        }
        }
    \uElse{
        return FAILURE
    }
}
\caption{An interpretation of Robinson's Unification Algorithm \citep{Robinson_1965}, \citep{Luger}. This algorithm can handle type constructions such as functions $a \rightarrow a$ by breaking down their arguments. The type constructions themselves are refered to as $functors$.}
\end{algorithm}
\end{minipage}
\end{center}

This definition relies on the function \emph{bind} which is defined as follows:
\begin{center}
\begin{minipage}{0.8\linewidth}%
\begin{algorithm}[H]
    \DontPrintSemicolon
    \Begin{
        \uIf {$var \notin \{t\}$}
            {return $var := t$}
        \uElse{
            return FAILURE
        }
    }
\caption{The binding function is a simple utility function for determining if a type is recursively defined. If it \emph{is} recursive, then return a failure. Otherwise, allow the type variable binding.}
\end{algorithm}
\end{minipage}
\end{center}

This algorithm outlines the steps involved in Robinson's Unification Algorithm, detailing how it handles different cases of terms and variables to find a most general unifier.
The algorithm ensures that the resulting substitution satisfies the unification constraints and resolves any potential recursive dependencies between variables and terms.

\subsection*{Deffered vs. On-the-Fly Unification}

When generating a set of constraints, an interesting dilemma becomes apparent.
One can unify types immediately upon encountering a constraint, or can defer attempting to unify them until after all the program's constraints have been generated.

There are justifications for deferring unification until all constraints are generated, as it is possible to construct type level code that cannot be infered on the fly.

In the case of on-the-fly unification, there is no need to avoid solving constraints as you go so long as the type system does not range out of simple inferrence.
For this project, on-the-fly unification is far easier to implement via the Union Find data structure and covers the intended outcomes of the project more than sufficently.

% When unifying two types there are different ways to handle unification depending on the types themselves.
% In the most basic case, unification of two monotypes will result in either an error or no error.

% \[\tau_a \text{ unify } \tau_b \rightarrow \text{ if } \tau_a = \tau_b \text{ then success}\]

% A unification algorithm is a procedure used to find a common substitution for variables in two or more expressions, such that the expressions can be made equivalent.
% For Hindley-Milner, unification is used to determine the most general types for variables and expressions.

% Robinson's Unification algorithm is an effective method for determining

\subsection*{Union Find}
The unification algorithm is closely related to Union Find data structure, which is a method for maintaining disjoint sets of elements and efficiently determining whether two elements belong to the same set.
In the context of type inference, Union Find is used to represent and manage equivalence classes of types.
During the unification process, Union Find is employed to merge sets of type variables that are determined to be equivalent based on type constraints, allowing for efficient resolution of type ambiguities and the determination of the most general types.

Using disjoint sets to represent classes of unifiable types is a clean and easy way to handle the tricky parts of substituting equivalent types for one another.
It also adds the possibility of utilising the Union Find algorithm to handle the searching of these disjoint sets.

% \begin{lstlisting}[language=Caml]
%     let rec unify type_a type_b = 
%     match type_a, type_b with
%       | type_a, type_b when type_a = type_b -> () 
%       | TypeFunc (a1, a2), TypeFunc (b1, b2) | TypePair (a1, a2), TypePair (b1, b2) -> 
%         unify a1 b1; unify a2 b2
%       | TypeVar type_var, t | t, TypeVar type_var -> 
%         in_type_check type_var t; 
%         if Resolved
%         (
%           match UnionFind.get res_state with
%           | Unresolved -> UnionFind.set res_state (Resolved t)
%           | Resolved type_b -> unify type_b t
%         )
%       | TypeList a, TypeList b -> unify a b
%       | _, _ -> raise (UnificationError type_a type_b)
% \end{lstlisting}

% \section{Evaluation}
% \section{Guidance}
% Design should cover the abstract design in such a way that someone else might be able to do what you did, but with a different language or library or tool.

%==================================================================================================================================
\chapter{Implementation}
% What did you do to implement this idea, and what technical achievements did you make?

This section gives an overview of PyFunc's implementation details and key decisions made during development.

% The language of choice for implementation was OCaml.
% All aspects of the project exist by default in OCaml.
First off, the language of choice for the implementation of PyFunc was OCaml.
The justification for this was much like the justification for the development of PyFunc.
OCaml offers a combination of strong typing, functional programming, and enough flexibility to allow imperative and / or object-oriented code for project edge cases.
 
% Menhir for parsing.
% OCamllex for tokenisation.
Additionally, a lexical analyser generator (OCamllex) and a parser generator (Menhir) were ustilised in development.

% Everything else was built from scratch in the language.
Beyond these dependancies, all other features were built from scratch in OCaml.

\section{Parsing, Lexing and the Abstract Syntax Tree} \label{sec:parsing-lexing-and-the-ast}

% To aid in a flexible yet easily modifiable syntax the following was done:
% 0. An initial pass that converts white space identation into explicit scopes for later parsing.
% 1. OCamllex was used for tokenisation.
% 2. Menhir was used as a parser generator.
% 3. A set of constructor methods are used to generate an abstract syntax tree of the frontend program.

The parsing of Pyfunc is the first major step in the language pipeline, taking the PyFunc code and converting it step by step into the intermediate represention.
This process was broken down into three steps:
\begin{enumerate}
    \item An initial pass that converts whitespace identation into explicit scopes for later parsing.
    \item Tokenisation is performed using OCamllex.
    \item The PyFunc parser converts the token stream into the frontend's abstract syntax.
\end{enumerate}

\subsection{Whitespace Indentation Conversion}

Whitespace indentation is a formatting technique used in programming languages to visually structure code.
By indenting blocks of code with consistent spaces or tabs, developers create a clear hierarchy that reflects the relationships between different parts of the code.
In many languages this indentation has no functional impact on the code's behavior but greatly enhances readability, making it easier to understand the flow of control and identify nested blocks like loops and conditionals.
Python enforces strict indentation rules as a means to gain these advantages and to explicitly delineate block scopes for structures like loops and conditionals.

% \[Python Example of whitespace indentation levels\]
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{lstlisting}[language=Python]
            def factorial(n):
                if n == 1:
                    return 1
                else:
                    return n * factorial(n - 1)
        \end{lstlisting}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{lstlisting}[language=Python]
            def factorial(n):{
                if n == 1:{
                    return 1}
                else:{
                    return n * factorial(n - 1)}}
        \end{lstlisting}
    \end{subfigure}
        
\end{figure}

PyFunc similarly enforces this rule of white space indentation for the delineation of scopes.

The specific rule is as follows:

\begin{center}
Each layer of indentation is indicated by \emph{4} spaces (tabs are \emph{not} accepted as of yet).
\end{center}

The implementation for this rule was relatively simple from an algorithm perspective:

\begin{enumerate}
    \item Take the source as a list of strings, each element representing a line of code.
    \item For each element in the list of strings:
    \begin{enumerate}
        \item Get the indentation of the line.
        \item Calculate the difference in indentation to the previous line.
        \item If the new indentation is bigger then insert a '$\{$' character.
        \item Else if the new indentation is smaller then insert a '$\}$' character.
        \item Else, the indentation is the same move to the next line.
    \end{enumerate}
    \item Return the modified source code.
\end{enumerate}

In the actual implementation the converter looked like this:

\begin{lstlisting}[language=Caml, caption=The 'preprocess' function that handles all whitespace scoping in PyFunc.]
let preprocess source =
    let get_indent str = 
      let rec count_space_helper idx = 
        if idx >= String.length str || str.[idx] <> ' ' then
          idx
        else
          count_space_helper (idx + 1)
      in
      let indent = count_space_helper 0 in
      if (indent mod 4 = 0) then
        indent / 4
      else
        raise (Parse.parse_error "Invalid indentation! 4 space indentation required.")
    in
    let rec loop prev_indent acc = function
      | [] -> 
        if prev_indent > 0 then
          let closing_braces = String.make prev_indent '}' in
          List.rev (closing_braces :: acc)
        else 
          List.rev acc
      | line :: rest ->
        let indent = get_indent line in
        let diff = abs (prev_indent - indent) in
        let amended_line =
          if indent > prev_indent then
            (String.make diff '{') ^ line
          else if indent < prev_indent then
          (String.make diff '}') ^ line
          else
            line
        in
        loop indent (amended_line :: acc) rest
    in
    let output = String.concat "\n" (loop 0 [] source) in
    output
\end{lstlisting}

Whilst the implementation is rather crude, the language itself does not have any cases currently that make this algorithm fail.
It would likely be an insufficent solution in future when the language is extended further, but this is completely functional for the language in its current state.


\subsection{Tokenisation with OCamllex}

Tokenization is the process of splitting a block of text into smaller, meaningful units called tokens.
These tokens can represent individual words, punctuation marks, numbers, or even phrases, depending on the context and requirements of the task.
For example, in the sentence "The cat is sleeping", tokenisation could break it down into three tokens: "The", "cat", and "is sleeping".
It could also break it down into four: "The", "cat", "is", and "sleeping".
Tokenisation can break down text arbitrarily, meaning that to get meaningful output there needs to exist a specification for the process of tokenisation.

The component that does this tokenisation is the lexical analyser (lexer).
The lexer reads the characters of the source code input and groups them into tokens based on predefined rules called lexical rules or tokenization rules.
These rules define how to recognize and classify different parts of the source code.

Lexical analysers are internally relatively simple and could quite simply be implemented manually, but in the case of this project, it seemed excessive and possibly prohibitive.
For the sake of development ease and flexibility in altering lexer rules, a lexer generator called OCamllex, was utilised instead.
OCamllex takes a given specification of lexer rules and produces the lexer which implements them.


\begin{lstlisting}[language=C, keepspaces=true, caption=A small extract of the PyFunc lexer\, showing the syntax of the lexer specification.]
    | "def"   {DEFINE} \\ The first string shows what is matched.
    | "True"  {TRUE}   \\ The value in the braces show what is emitted.
    | "False" {FALSE}
    | "if"    {IF}
    | "elif"  {ELIF}
    | "else"  {ELSE}
\end{lstlisting}
What appears in quotes is the token that is recognised in the source text.
The capitalised values in the curly braces are the tokens that are being emitted by the lexer.
% At the bottom of the ruleset the series of buffer actions are simply the parsing of string literals into single contiguous tokens refering to the literal value of the string.

The full PyFunc lexer specification can be found in the appendix. (\ref{lst:lexer-spec})


\subsection{Parsing with Menhir}

Parsing is the process of analyzing a sequence of tokens according to the rules of a formal grammar to determine its structure and meaning.
In simpler terms, parsing is like understanding the syntax of a sentence in a language.
Just as we follow grammar rules to understand the structure of sentences in human languages, parsers in code follow the rules of a formal grammar to understand the structure of code written in programming languages.
The parser checks whether the sequence of tokens conforms to the syntax rules defined by the grammar of the language.
If the sequence of tokens forms a valid structure according to the grammar, it is considered syntactically correct.
And in this case the parser will generate the frontend language's AST from its formal grammar.

For convenience the parser generator tool Menhir was used.
Menhir facilitates the creation of parsers by automatically generating efficent parsing code based on the provided grammar rules, much like OCamllex does for a lexical analyser, though somewhat more indepth.

In PyFunc the formal grammar for the fronend language is relatively simple, with only a few constructs included in its current state.

Here are a few of the more interesting parts of the parser grammer:
\begin{itemize}
    \item \textbf{Top-level Bindings}
    
    Each PyFunc program consists of a list of function bindings, with a specified entry point denoted by the function named \texttt{main}.
    In the AST the form this takes is as a list of bindings as well, so the start of each program parser beings with this grammar rule.

    \begin{lstlisting}[language=Caml, caption=PyFunc Parser's starting rule grammar.]
        start:
            | binding+ EOF {makeProgram $1}

        binding:
            | DEFINE ID LPAREN param_list RPAREN COLON scope {makeBinding $2 (makeFunc $2 $4 $7)}
            | ID EQ expr {makeBinding $1 $3}

    \end{lstlisting}

    The rule expects a series of bindings (indicated by the $+$) before the EOF (End Of File) token.
    These results bindings are passed to an OCaml constructor, which produces the AST as it parses the program.

    \item \textbf{Binary and Unary Operators}
    
    The operators of the language are all implemented in very similar ways to one another, so here is an abbreviated look at that grammar.
    \begin{lstlisting}[caption=An abrreviated version of PyFunc Parser's implementation of binary and unary operators.]
        base_expr:
            | expr OR expr      {makeOpBinary OpBinary.And $1 $3}
            | expr AND expr     {makeOpBinary OpBinary.Or $1 $3}
            | ...               {...}
            | unary             {$1}

        unary:
            | NOT unary   {makeOpUnary OpUnary.Not $2}
            | MINUS unary %prec UMINUS {makeOpUnary OpUnary.Negative $2}
            | PLUS unary  %prec UPLUS  {makeOpUnary OpUnary.Positive $2}
            | list_op     {$1}
    \end{lstlisting}

    An interesting feature of the unary rule is the presence of the \texttt{\%prec} keyword.
    For the unary versions of the PLUS and MINUS operations a different precedence and associativity is required, as all unary operators are right-associative whilst binary operations are left-associative.
    Menhir allows for an explicit declaration of the precedence and associativity of operators.
    For PyFunc this precedence looks like this:
    \begin{lstlisting}[caption=PyFunc's Menhir precedence and associativity declarations.]
        %right CONS NOT
        %right UPLUS UMINUS
        %left LT GT GEQ LEQ EQQ NEQ
        %left PLUS MINUS
        %left STAR DIVIDE INTDIVIDE
        %right EXPONENT
        %left AND OR
    \end{lstlisting}
    The \texttt{\%right} indicates right associativity whilst \texttt{\%left} does the opposite.
    The actual precedence of these operations s bottom to top, so AND and OR have the highest precedence, whilst CONS and NOT have the lowest.

    \item \textbf{List Processing}
    
    One final interesting thing in the grammar is how it handles list literals, parameter lists, and argument lists.
    \begin{lstlisting}[caption=PyFunc's Parser grammar for handling list literals.]
    call:
        | ID LPAREN expr_list RPAREN {makeCall $1 $3}
        | primary {$1}
    
    primary:
        | ...                     {...}
        | LBRACK expr_list RBRACK {makeList $2}
        | ...                     {...}

    param_list:
        | xs = separated_list(COMMA, ID) { xs }

    expr_list:
        | xs = separated_list(COMMA, expr) { xs }
    \end{lstlisting}

    Menhir includes some utility functions for the implementation of efficent solutions to common constructs.
    Poorly implemented lists could cause major bottlenecks in parser generators, so by using Menhir's seperated\_list construct all the parser needed to know was what to expect in the list.
    The \texttt{seperated\_list} function takes a token to represent what the list will be split by (COMMA, in this case), and also what parsing rule should be called upon for what it finds in this list.
    It should also be noted that \texttt{seperated\_list} allows for empty lists which are not uncommon in both list literals and functions.
\end{itemize}

The entire PyFunc grammar can be found in the appendix. (ADD REF)

\subsection{Constructors}

For clarities sake, the parser uses OCaml functions to generate AST nodes which refer to one another, constructing the tree as the parser reads the tokenised source code.

These constructors are relatively straight forward: they take information from the tokenised source code and pass it into the AST nodes they generate.

\begin{lstlisting}[language=Caml, keepspaces=true, caption=PyFunc frontend AST constructor functions.]
module Constructor              = struct
  let makeProgram binding_list  = Expr.Program binding_list
  let makeBinding binder expr   = Expr.Binding (binder, expr)
  let makeConst const           = Expr.Const const
  let makeIf condition if_expr else_expr = Expr.If (condition, if_expr, else_expr)
  let makeOpUnary op expr                = Expr.OpUnary (op, expr)
  let makeOpBinary op expr_a expr_b      = Expr.OpBinary (op, expr_a, expr_b)
  let makeDefineFunc id param func_expr  = Expr.Func (id, param, func_expr)
  let makeCall id args                   = Expr.Call (id, args)
  let makeAssign id value scope = Expr.Assignment (id, value, scope)
  let makeParam alias           = Expr.Param alias
  let makeVar var               = Expr.Var var
  let makeFunc id params expr   = Expr.Func (id, params, expr)
  let makeList args             = Expr.List args
end
\end{lstlisting}

% \textbf{BREAK THIS DOWN IN TO SMALLER PIECES AND DISCUSS EACH INDIVIDUALLY (SUMMARISING ANYTHING REPETATIVE)}

% \begin{lstlisting}[caption=PyFunc's full frontend grammar rules for Menhir., keepspaces=true]


% scope: LBRACE expr RBRACE   {$2}

% expr:
%     | base_expr                                 {$1}
%     | if_expr                                   {$1}

% base_expr:
%     | expr OR expr      {makeOpBinary OpBinary.And $1 $3}
%     | expr AND expr     {makeOpBinary OpBinary.Or $1 $3}
%     | expr NEQ expr     {makeOpBinary OpBinary.NotEqual $1 $3}
%     | expr EQQ expr     {makeOpBinary OpBinary.Equal $1 $3}
%     | expr LT expr      {makeOpBinary OpBinary.Less $1 $3}
%     | expr LEQ expr     {makeOpBinary OpBinary.LessEqual $1 $3}
%     | expr GT expr      {makeOpBinary OpBinary.Greater $1 $3}
%     | expr GEQ expr     {makeOpBinary OpBinary.GreaterEqual $1 $3}
%     | expr MINUS expr   {makeOpBinary OpBinary.Subtract $1 $3}
%     | expr PLUS expr    {makeOpBinary OpBinary.Add $1 $3}
%     | expr DIVIDE expr  {makeOpBinary OpBinary.Divide $1 $3}
%     | expr STAR expr    {makeOpBinary OpBinary.Multiply $1 $3}
%     | expr MOD expr     {makeOpBinary OpBinary.Mod $1 $3}
%     | unary             {$1}

% // Control Flow

% if_expr:
%     | IF expr COLON scope elif_expr   {makeIf $2 $4 $5}
%     | IF expr COLON scope else_expr   {makeIf $2 $4 $5}

% elif_expr:
%     | ELIF expr COLON scope elif_expr {makeIf $2 $4 $5}
%     | ELIF expr COLON scope else_expr {makeIf $2 $4 $5}

% else_expr:
%     | ELSE COLON scope {$3}

% unary:
%     | NOT unary   {makeOpUnary OpUnary.Not $2}
%     | MINUS unary {makeOpUnary OpUnary.Negative $2}
%     | PLUS unary  {makeOpUnary OpUnary.Positive $2}
%     | list_op     {$1}

% list_op:
%     | unary CONS unary {makeOpBinary OpBinary.Cons $1 $3}
%     | call {$1}

% call:
%     | ID LPAREN expr_list RPAREN {makeCall $1 $3}    // Call with params.
%     | primary {$1}

% primary:
%     | LPAREN expr RPAREN {$2}
%     | LBRACK expr_list RBRACK {makeList $2}
%     | ID                 {try find var_table $1 with Not_found -> makeVar ($1)}
%     | STRINGVAL          {makeConst (ConstString $1)}
%     | INTVAL             {makeConst (ConstInt $1)}
%     | TRUE               {makeConst (ConstBool true)}
%     | FALSE              {makeConst (ConstBool false)}
%     | UNITVAL            {makeConst ConstUnit}

% // List literals.

% param_list:
%     | xs = separated_list(COMMA, ID) { xs }

% expr_list:
%     | xs = separated_list(COMMA, expr) { xs }
% \end{lstlisting}

\section{IR Converter}

To take the frontend abstract syntax tree and convert it into the equivalent AST in the IR, a conversion process was necessary.
To do this conversion several things had to be given special attention (whilst others converted almost one-to-one):

\subsection{Bindings}
Bindings are difficult, in that they exist in the top-level where the expression based-nature of the language raises a question: where do you start the program from.
Python starts the program quite simply at the topmost statement in the running file.
PyFunc ideally would do something like this but actually implementing this fell to the wayside when trying to correctly convert the bindinggs to an appropriate IR in the correct order.
Likely having a semantic analysis pass for collecting bindings would be a viable solution, but in the end the easy and effective option is to require a "main" function declaration to be the program entry point.

\subsection{Functions}
Functions themselves are rather complicated.
The IR has only lambdas which can take only a single argument, but the frontend allows for an arbitrary number of parameters.
To help deal with this conversion, a helper-function was devised.
This function constructs a let rec binding (let rec to allow recursive function definitions) from the given parameters.
Inside the let rec a series of lambdas are created for each parameter in reverse order (so as to have applications, partial or complete, apply the parameters in the same order they appear in the definition).

\begin{lstlisting}[language=Caml, keepspaces=true, caption=PyFunc frontend's function binding conversion code.]
    let rec func_to_lambdas p_list (body : IR.tree) =
      if List.is_empty p_list then
        body
      else
        let lambda_body = IR.ExprFunc (List.hd p_list, body) in
        func_to_lambdas (List.tl p_list) lambda_body
    in

    let convert_func func_binder func_param_list func_body scope =
      let p_list = List.rev func_param_list in
      let lambdas = func_to_lambdas p_list (convert func_body) in
        IR.ExprLetRec (func_binder, lambdas, scope)
    in
\end{lstlisting}

% \begin{figure}
%     \begin{subfigure}
%         \begin{forest}
%                 [Binder: a [Func a : x, y, z [+ [* [x][y]] [z]]]]
%         \end{forest}
%     \end{subfigure}
% \label{fig:ir-func-ast-tree}
% \end{figure}

Assignment statements are relatively simple conceptually, yet somewhat more difficult in implementation.
Of the two kinds of assignment, immutable assignment is more simple to convert as the immutable binding is just a let expression in the IR.
Even allowing recursive expressions as bindings is not particularly difficult, as the IR structure makes let and let rec expressions structurally identical.

Having a mutable assignment however is a problem of a more significant complexity.
The typechecker and unification algorithm would have to have a case introduced to handle mutable bindings.
Furthermore, the unifcation algorithm would need to know to never attempt to generalise the mutable binding.

Whilst this is a possible additon to the implementation, it does fall on the side of being a "could have" feature, and did not make it into the final verision of the project.

% \section{Intermediate Representation}

% The IR is almost identical to the IR specified in listing \ref{ir-ast}.

\section{Hindley-Milner Typechecker} \label{sec:hindley-milner-typechecker}

The type checker is the most technically difficult aspect of this project.
It required several data structures and the implementation of significant pieces of the HM theory already outlined to be encoded into OCaml.

\subsection{Monotypes, Polytypes, and Type Variables}

A simple description of a type variable could be something like: "A place holder that represents an unknown type".
This is true, but from an implementation standpoint it creates a minor issue.
Fundamentally, types can be monotypes, polytypes or type variables as previously outlined, but more often than not they exist in the same spaces interchangeably.
The distinction between polytypes, monotypes, and type variables warrant being explicitly stated:

\begin{enumerate}
    \item Monotypes refer to concrete type that do not contain any type variables.
    They represent specific types such as, integers, booleans, functions, etc.

    \item Polytypes, or type-schemes, refer to types that may contain one or more type variables.
    They represent a range of types.
    Polytypes can also be \emph{instantiated} with a monotype to produce a monotype version of the polytype.

    \item Type variables are placeholders for an unresolved type.
    They could be any type, monotype or polytype.
    Type variables can be specialised via unification to produce specific types during type inference.
\end{enumerate}

% With this in mind we can begin with the definition of the monotypes and polytypes in PyFunc's IR.

% \begin{lstlisting}[language=Caml, caption="Pyfunc internal definitions for types.]
%   type resolution_state = Unresolved | Resolved of t
%   and typevar = string * (resolution_state UnionFind.elem)
%   and monoType = 
%     | TypeInt
%     | TypeFloat
%     | TypeBool
%     | TypeString
%     | TypeFunc of (t * t) (* A recursive definition refering to t. *)
%     | TypePair of (t * t)
%     | TypeUnit
%     | TypeVar of typevar
%     | TypeList of t
%   and polyType = quantifier list * monoType
%   and t = monoType
% \end{lstlisting}

Now, from an implementation stand point, the distinction between a polytype and a monotype is quantification.
Polytypes have quantifiers that distinguish the scope of polymorphism within a type expression.
Internally handling a monotype that has a singlular concrete type and a polytype that has a quantifier and an a possibly indeterminate type is an added layer of complication.

A conceptually easy way to handle this in the implementation is to treat monotypes as polytype with no quantifiers at all.

For example,

\[\sigma := (quantifier, \tau) \text{ and } \tau := (\;\;, \tau)\]

% Regardless, before type variables can be implemented, polytypes and the concept of quantifiers needs to be described.
So, before we can define the types themselves, some implementation of a quantifier must be described.
\newpage
This very simple Quantifier module does just that.

\begin{lstlisting}[language=Caml, caption=The PyFunc type checker's quantifier module.]
  module Quantifier = struct
    type t = string
    let make x = x
    let of_typeVar (tv, _) = tv (*Type variables as quantifiers.*)
    let compare = String.compare
  end
\end{lstlisting}

The quantifiers are simply strings which can be compared, which allows for very easy modification in the type checker.

The next step in the implementation of the Hindley-Milner types, is itself describing the types that type checker will depend on.

\begin{lstlisting}[language=Caml]
  type resolution_state = Unresolved | Resolved of t
  and typevar = string * (resolution_state UnionFind.elem)
  and monoType = 
    | TypeInt
    | TypeBool
    | TypeString
    | TypeFunc of (t * t) (* A recursive definition refering to monoType. *)
    | TypePair of (t * t)
    | TypeUnit
    | TypeVar of typevar
    | TypeList of t
  and polyType = quantifier list * monoType
  and t = monoType (* The default type is a monotype. *)
\end{lstlisting}

This includes some definintions that need a little clarifying:
\begin{itemize}
    \item The type \texttt{resolution\_state} is a simple way of tracking if type variables are resolved or not.
    If it is unresolved then it carries no variable, whereas if it is resolved, it carries the monotype it has been resolved to.
    \item The type \texttt{typevar} is an easy way to represent type variables for unification.
    The inclusion of \texttt{UnionFind.elem} enables us to use the Union Find algorithm to resolve type variables across the constraint set it relates to.
    \item Polytypes are types containing variables bound by zero or more $\forall$ quantifiers. 
    The \texttt{Polytype} type shows this directly by being a pair of a list of quanitifers and the base monotype.
\end{itemize}

Now with these type defiitions, the additional infrastructure of type variables needs to be added.
There needs to be to function to generate a new type variable that does not already exist in the context.
This type variable should also be initialised in an "unresolved" state.

\begin{lstlisting}[language=Caml, caption=The simple implementation of type variables. The fresh\_TV function generates a new\, unresolved type variable.]
    let id_count = ref 0
    let reset () = id_count := 0
    
    let fresh_TV ?(scope_prefix="_") () =
      let id = !id_count in
      let () = incr id_count in
      let resolution_state = UnionFind.make Unresolved in
      (scope_prefix ^ (string_of_int id), resolution_state)
end
\end{lstlisting}

This excerpt shows how new type variables can be defined in such a way that they do not ever overlap.

A type variable is identified by the string quantifier, which as seen in the code is generate with a unique id that counts up every time a fresh type variable is created with the \texttt{fresh\_TV} function.

\subsection{Type Environments and Why We Need Them}

Now that we have described the basic implementation of type variables, we need to describe a type environment ($\Gamma$) so in the type checker, we can bind new type annotations internally.
A simple way to do this is using both a set and a map together.
With a set to contain type variables and a map from variable names to their associated polytypes.

\begin{lstlisting}[language=Caml]
    type t = ContextSet.t * (Type.polyType ContextMap.t)
    let empty = (ContextSet.empty, ContextMap.empty)
    let fresh_TVs = fst
\end{lstlisting}

Now, the other major jobs that this environment needs to handle are binding type variables and finding bound type variables.

Let's start with binding type variables:

\begin{lstlisting}[language=Caml]
    let bind_polytype k (quantifier, typ) (env_fresh_TVs, env) : t =
      let type_fresh_TVs = ContextSet.diff (fresh_monoTvs typ) (ContextSet.of_list quantifier) in
      (ContextSet.union env_fresh_TVs type_fresh_TVs,
       ContextMap.add k (quantifier, typ) env)
\end{lstlisting}

This function \texttt{bind\_polytype} binds a polytype to a variable in the type environment.
It takes three arguments:
\begin{itemize}
    \item $k$: The variable name.
    \item $(quantifier, typ)$: The polytype to bind.
    \item $(env\_fresh\_TVs, env)$: The existing type environment.
\end{itemize}

This function then calculates the set of fresh type variables introduced by the type typ and updates the environment accordingly by adding the variable $k$ mapped to the polytype $(quantifier, typ)$.
If a monotype needs to be bound, it should be bound just as a polytype with an empty list of quantifiers.

Beyond this, defining a method to find if a variable exists within the context map is almost trivial.

\begin{lstlisting}[language=Caml]
    let find k (_, env) = 
      match ContextMap.find_opt k env with (*find_opt returns optional type.*)
        | Some typ -> typ 
        | None -> raise (Errors.type_error ("Unbound variable " ^ k))
\end{lstlisting}

With just these few features, a fully functional type environment has been defined.

\subsection{Generalisation and Instatitation}

% DESCRIBE THEIR ROLES
Generalisation and instantiation are the key operations that manipulate how polytypes are quantified.
Instantiation substitutes type variables for all quantifiers in a polytype.
Whereas, generalisation finds the most general type a type variable could be.

As described in the design chapter, instantiation has been merged with the variable inference rule, and generalisation has been merged with the let inference rule.

So, to start of with, here is the implementation for instantiation:
\begin{lstlisting}[language=Caml, caption=PyFunc's instantiation function.]
    let instantiate poly_type =
        let (quant, monotype) = poly_type in 
        let substitution_map = List.map (fun q -> (q, TypeVar.fresh_TV ())) quant in
        let rec inner = function
        | TypeVar type_var -> (
            match List.assoc_opt (TypeVar.var type_var) substitution_map with
                | Some instance -> TypeVar instance
                | None -> TypeVar type_var
            )
        | TypeFunc (type_a, type_b) -> TypeFunc (inner type_a, inner type_b)
        | TypePair (type_a, type_b) -> TypePair (inner type_a, inner type_b)
        | t -> t
        in
        inner monotype
\end{lstlisting}

Here's a breakdown of how the function works:

\begin{enumerate}
    \item \textbf{Pattern Matching:} The function begins by pattern matching on the poly\_type to extract its components, namely the quantifier list (quant) and the monomorphic type (monotype).
    
    \item \textbf{Generating Substitution Map:} Next, the function generates a substitution map using List.map.
    For each type variable in the quantifier list, it associates a fresh type variable using TypeVar.fresh\_TV(), creating a mapping from each quantified type variable to a fresh type variable.

    \item \textbf{Recursive Inner Function:} The function defines a recursive inner function named "inner" that traverses the monomorphic type recursively and replaces each occurrence of quantified type variables with their corresponding fresh type variables from the substitution map.

    If the encountered type is a type variable (TypeVar), it checks if there's an entry for the type variable in the substitution map.
    If so, it replaces the type variable with its corresponding fresh type variable; otherwise, it leaves the type variable unchanged.
    
    If the encountered type is a function type (TypeFunc) or a pair type (TypePair), it recursively applies the inner function to each component of the compound type.

    For any other type, it leaves the type unchanged.

    \item \textbf{Applying Inner:} 
    Finally, the function applies the inner function to the monotype, instantiating the polytype by replacing each quantified type variable with its corresponding fresh type variable.
\end{enumerate}

By comparison, the generalisation function is much more concise.

\begin{lstlisting}[language=Caml, caption=PyFunc's generalisation function implemenation.]
    let generalise env (monoType : Type.monoType) : Type.polyType =
        (List.map Quantifier.make 
        (ContextSet.elements 
            (ContextSet.diff (fresh_TVs monoType) (Env.fresh_TVs env))
        )
        ), monoType
\end{lstlisting}

The function is used to generalise a monotype into a polytype within a given type environment (env). Here's an explanation of how the function works:

\begin{enumerate}
    \item \textbf{Extracting Type Variables:}

    The function first gets the set of type variables present in the monotype using the fresh\_TVs function.
    These are type variables that are unbound in any context and are ready to be generalised.

    \item \textbf{Filtering Type Variables:}

    Next the set difference (\texttt{ContextSet.diff}) between the previously gathered type variables and those in the type environment is calculated.
    This is to filter out any type variables that are present in the environment, ensuring that the generalised type variables are in fact free.

    \item \textbf{Createing New Quantifiers:}
    
    The function then maps over the filtered type variables, using \texttt{List.map}, generating universal quantifiers for the type variables.

    \item \textbf{Constructing the Polytype:}
    Finally, once the qunatifiers have been generated, the polytype can be constructed from the original monotype and the generated quantifer list.
\end{enumerate}

Overall, this is one of the most important steps in the type checker, as without it, polymorphism would not be possible in the type system.

\subsection{Unification}

The unification algorithm is the part of the type checker that does the heavy lifting for the project.
It finds a common substitution for variables in two or more expressions, such that the expressions can be made equivalent. 
% \begin{figure}
\begin{lstlisting}[language=Caml, caption=PyFunc's unification algorithm.]
let rec unify type_a type_b = 
match type_a, type_b with
    (* Matching types succeed by not throwing an error.*)
    | type_a, type_b when type_a = type_b -> () 
    (* Matching where types are pairs / funcs. *)
    | TypeFunc (a1, a2), TypeFunc (b1, b2) | TypePair (a1, a2), TypePair (b1, b2) -> 
        unify a1 b1; unify a2 b2
    (* Check type occurs, then unionfind to see if it is resolved or unresolved. *)
    | TypeVar type_var, t | t, TypeVar type_var -> 
        in_type_check type_var t;
        let res_state = TypeVar.resolution_state type_var in (
            match UnionFind.get res_state with
            | Unresolved -> UnionFind.set res_state (Resolved t)
            | Resolved type_b -> unify type_b t
        )
    (* List Operation *)
    | TypeList a, TypeList b -> 
        unify a b
    | _, _ -> 
        let error = Format.asprintf "[Unification error]: Cannot unify %a with %a" 
            Type.pp type_a 
            Type.pp type_b 
        in 
    raise (Errors.type_error error)
\end{lstlisting}
% \end{figure}

The provided unification algorithm recursively matches and unifies two types \texttt{type\_a} and \texttt{type\_b}, handling various cases such as matching identical types, pairs, functions, type variables, and lists.
When encountering type variables, it checks for occurrences and uses the UnionFind data structure to manage unresolved and resolved states.
If types cannot be unified, it raises a type error with detailed information about the types involved.
Overall, the algorithm efficiently handles type unification in PyFunc, ensuring type consistency and correctness during type inference.

\subsection{The Typerchecker}
% \subsection{The Not So Simple Typechecking}

The rest of the typechecker outside of the inferrence rules is relatively simple.

Anytime the type checker needs to be invoked, the function \texttt{\_typecheck} can be called.
The function allows the type checking of any type via pattern-matching the passed IR-AST node and passing the children to specific type checking functions.:

\begin{lstlisting}[language=Caml, caption=The body of PyFunc's main type check function\, which defers to other more specific type checking functions via patter-matching.]
function
    | ExprVar value -> instantiate (Env.find value env)
    | ExprFunc (binder, body) -> typecheck_func env binder body
    | ExprApplic (expr_func, expr_arg) -> typecheck_applic env expr_func expr_arg
    | ExprOpUnary (op, expr) -> typecheck_op_unary env op expr
    | ExprOpBinary (op, expr_a, expr_b) -> typecheck_opbinary env op expr_a expr_b
    | ExprConst const -> typecheck_const const
    | ExprIf (expr_cond, expr_if, expr_else) -> typecheck_if env expr_cond expr_if expr_else
    | ExprLet (binder, expr_a, expr_b) -> typecheck_let env binder expr_a expr_b
    | ExprLetRec (binder, expr_a, expr_b) -> typecheck_letrec env binder expr_a expr_b
    | ExprLetPair (a, b, expr_a, expr_b) -> typecheck_let_pair env a b expr_a expr_b
    | ExprPair (expr_a, expr_b) -> typecheck_pair env expr_a expr_b
    | ExprFirst expr -> typecheck_first env expr
    | ExprSecond expr -> typecheck_second env expr
    | ExprList list -> typecheck_list (env : Env.t) list
\end{lstlisting}

% If a construct, an if-expression for example, requires some part of it to be a certain type
Of the numerous specific type checking functions, here are a few excerpts of the more interesting cases:
\begin{itemize}
    \item \textbf{If expressions:}
If expressions require two things, the condition to evaluate to a boolean type, and the type of both branches to match.

To do this, the rule unifies the \texttt{cond\_type} with a \texttt{TypeBool} type literal.
If unification does not produce an error, \texttt{cond\_type} is of type boolean.

To check that both branches produce the same type, they are unified together.
The return type could be either the \texttt{if\_type} or the \texttt{else\_type}, but in the excerpt the \texttt{if\_type} is used.
\begin{lstlisting}[language=Caml, caption=PyFunc Typechecker's typecheck\_if function. If no error is produced then the provided if-expression code is well-typed.]
    (* Typechecking ifs *)
    let typecheck_if env expr_cond expr_if expr_else = 
      let cond_type = _typecheck env expr_cond in
      let if_type = _typecheck env expr_if in 
      let else_type = _typecheck env expr_else in
        unify cond_type TypeBool;
        unify if_type else_type;
        if_type
    in
\end{lstlisting}



    \item \textbf{Let Pair:}
    This exceprt is interesting because the type environemnt is updated twice with new bindings.
    In practice the procedure is very simple.
    The variables $a$ and $b$ are instantiated as fresh type variables, then they are wrapped in a pair type and unified with \texttt{a\_type}.
    This is to ensure that \texttt{a\_type} is of an acceptable type, I.e, a pair type construct.
    After determining if the type is well-typed, the fresh type variables for $a$ and $b$ are entered into the type context, before \texttt{b\_type} is returned.
    \begin{lstlisting}[language=Caml, caption=PyFunc Typechecker's typecheck\_let\_pair function. If no error is produced then the provided let-pair code is well-typed.]
    let typecheck_let_pair env a b expr_a expr_b =
      let a_type = _typecheck env expr_a in
      let var_a = new_var () in
      let var_b = new_var () in
        unify (TypePair (var_a, var_b)) a_type;
        let env = Env.bind a var_a env in
        let env = Env.bind b var_b env in
        let b_type = _typecheck env expr_b in
          b_type
    in
    \end{lstlisting}



    \item \textbf{Application:}
    
    Application is one of the less regular rules, though simple all the same.
    First, a fresh type variable is created, to serve as the output type of the function.
    Then the actual function itself and the provided argument are type checked, making \texttt{func\_type} and \texttt{arg\_type}.
    Finally, the types of the arguments and the fresh type are wrapped in a function type construct and unified with the \texttt{func\_type} produced by the type checker.
    If the unification occurs properly then the application is well-typed, and the fresh type variable is returned.
    \begin{lstlisting}[language=Caml, caption=PyFunc Typechecker's typecheck\_applic function. If no error is produced then the provided application code is well-typed.]
        (* Typechecking Applications *)
        let typecheck_applic env expr_func expr_arg =
          let fresh_var = new_var () in
          let func_type = _typecheck env expr_func in
          let arg_type = _typecheck env expr_arg in
          unify func_type (TypeFunc (arg_type, fresh_var));
          fresh_var
        in
    \end{lstlisting}
\end{itemize}

\section{Interpreter}

The interpreter is the final stage of the PyFunc language pipeline and is responsible for the actual execution of the code.
Because the language has a reduced form intermediate representation, the interpreter is in a way the simplest part of the project.
This is also helped by being a tree-walk interpreter\footnote{In this context, a tree-walk interpreter means a depth-first recursive traversal of the AST where the code is executed as the interpreter traverses.}, but a few unusal cases made this somewhat awkward to implement than expected in some areas.

\subsection{Interpreted Values}

The interpreter needed some type to output as a computed value, which could represent the various possible program outputs and termination states.

So a simple value type was designed like so:

\begin{lstlisting}[language=Caml, caption="PyFunc's Interpreter value type definition.]
type value = 
  | Vint of int 
  | Vfloat of float
  | Vbool of bool 
  | Vstring of string 
  | Vunit of unit 
  | Vvar of string 
  | Vpair of (value * value)
  | Vlist of value list
\end{lstlisting}

Having an interpreted value type was necessary but also got in the way in one key case: Partial application.

Partial application is a technique used in functional programming languages where a function with multiple parameters can be applied to some, but not all, of its arguments.
The result of this partial application is a new function that takes only the remaining parameters not already given.

This raised an issue in the interpreter.
Due to the function not having the sufficent parameters to totally evaluate, the interpreter would need to stop evaluating the expression and return it as a function until it had all the necessary arguments.

However, this was very difficult with the structure of the interpreter being a recursive depth-first search of the IR-AST.
Initally as a patch, the option for a value to be a VTree was added.
A VTree was literally a segment of the AST (that would cause an error if evaluated) wrapped into a type.

But for the case of partial application it worked very well.

For example, \[ f(a, b, c) -> \text{let } f = \lambda a. \lambda b. \lambda c. a + b + c \text{ in ...}\]

With partial application what happens is: 
\[f (1) = VTree (\lambda b. \lambda c. 1 + b + c)\]

When another application occurs on VTree then the VTree value is unwrapped and reconsidered part of the AST.
As it still is not fully evaluated, it is again wrapped in the value type as a VTree.
When all the requisite applications are done it returns a value type other than a VTree.

This idea whilst having some merit is unweildy and generally quite clunky.
Everytime the function was paritally applied the AST of the function would need substitutions to be applied to it, modifying the AST.
However, a simple remedy to this clunky solution is something that ought to have been obvious, \emph{closures}.

A closure value type could wrap the partially applied function and its local environment until used again, making a far easier to implement version of the raw VTree idea.

Using a closure instead of the VTree, allowed for variables to still be in the local environment.
Because of that, not substitutions in the AST were required, greatly simplifying the implementation.

There was only one caveat to using closures to handle partial application, and that was the fact that recursive functions (implemented via let-rec) could not be initally defined with a closure as they have to include themselves in their local environment\footnote{Recursive let expressions effectively forward declare themselves so they can reference themselves in their own definition.}.
This is impossible for a closure to do as it introduces a circular dependancy to the definition of the closure and the environment it includes.

So as a simple fix, a simple VFn value type was added that is only used in this one instance.
The VFn type simple holds the body of a recursive function for the initial definition, to mitigate the depedancy issue.

\begin{lstlisting}[language=Caml, label=lst:vclos-example, caption=The aditional type definitions of a VClos and VFn.]
    type t = (string * value) list (* Env type *)
    and value = 
    | ...
    | VFn of (tree)
    | VClos of (binder * tree * env.t)
    | ...
\end{lstlisting}

\subsection{Environment}

In a similar fashion to the typechecker, the environments in the interpreter are passed down the evaluation tree during interpretation.

However, maintaining a collection of runtime bindings is not the same as keeping track of disjoint sets, so a new interpreter environemnt had to be defined.

All that is required of the interpreter's environment is that it hold string to value bindings, like a let expression.
In listing \ref{lst:vclos-example} the type of the environment was already stated as:

\[\text{type } t = (\text{string} * \text{value}) \;list\]

Which is a simple list of string to value bindings.
The simple list of string-value pairs proved more than capable of working as the local environment for the interpreter at runtime.

Such a simple implementation of the environement itself, allow for the accessor function to also be simple.

For example, the \texttt{Env.get} function is defined:

\begin{lstlisting}[language=Caml, caption=PyFunc's Interpreter Environment get function.]
    let get (env : t) key =
    let result = 
      try List.assoc key env with 
      Not_found -> raise (Errors.lookup_error ("Undefined variable '" ^ key ^ "'!")) in
    result
\end{lstlisting}

% When an evaulation function returns an interpreted value

% Contrary to the evironments in the type checker which were only passed down the ast as an argument, the interpreter uses a reference type to handle the env.

% A important reason for using a reference type for the env was because of the complications regarding scope of values in the interpreter.

% With a passed env the previous sections use of VTree would be impossible.
% For example, (EXAMPLE) Basically whenever a set of tree nodes get cast a VTree, the env no longer gets passed down the recursive calls, which means that the added bindings relating to the parameters are completely lost in every partial application, resulting unresolvable errors for bindings being in and out of scope.

% Using a reference env allows us to completely circumnavigate this issue entirely and set the parameters into scope and remove them only when the function has been fully applied and interpreted.

\subsection{General Rules}
% Beyond these unusal edge cases which required special attention, 
The rest of the interpreter was largely straightforward evaluations of syntax.

Here are excepts indicating the implementations of the following rules:

\begin{itemize}
    \item \textbf{Unary and Binary Operations:}
    
    \begin{lstlisting}[language=Caml, caption=Excerpt of the PyFunc Interpreter rules for binary and unary operations.]
    and eval_op_binary env op expr_a expr_b =
        let left = eval env expr_a in
        let right = eval env expr_b in
        match left, op, right with
        | (Vint a), Add,       (Vint b) ->  Vint (a + b)
        | (Vint a), Subtract,  (Vint b) ->  Vint (a - b)
        | ...

    and eval_op_unary env op expr = 
        let expr_value = eval env expr in
        match op, expr_value with
        | Positive, (Vint i) -> Vint (~+i)
        | Negative, (Vint i) -> Vint (~-i)
        | Not, (Vbool b) -> Vbool (not b)
        | ...
    \end{lstlisting}
    The rules for operations are relatively simple by virtue of the reduced complexity of the IR allowing for pattern matching to handle all of the major differentation of operations.

    \item \textbf{Recursive Let Expressions:}
    \begin{lstlisting}[language=Caml, caption=The Pyfunc Interpreter let rec rule.]
    and eval_letrec env binder let_expr in_expr =
        (* Set variable into the Env before evaluation. *)
        let fn = VFn (let_expr) in
        let env' = (Env.set env binder fn) in
        (* Compute value with recursive referencing. *)
        eval env' in_expr
    \end{lstlisting}

    In the example above, the in\_expr refers to the continuation of the let rec expression.    

    \item \textbf{Application:}
    \begin{lstlisting}[language=Caml, caption=The PyFunc interpreter's application rule.]
    and eval_applic env func arg =
      (* Attmept computation. *)
      let fn = eval env func in
      (* Unwrap value. *)
      match fn with 
      | VFn body -> eval env body
      | VClos (binder, body, c_env) -> 
        let arg_value = eval env arg in
      (* Bind arg *)
        let env' = Env.set c_env binder arg_value in
        eval env' body 
      | v -> v
    \end{lstlisting}

    \item \textbf{Variables:}
    
    \begin{lstlisting}[language=Caml, caption=The PyFunc interpreter's rule for variables.]
    and eval_var env var =
    (* Attempt to get var from env *)
    try let result = Env.get env var in
    match result with
      | VFn body -> print_endline ("VFn var: " ^ var); eval env body
      | VClos (binder, body, env') -> eval env' body
      | x -> x

    with
      | Errors.Lookup_Error msg -> raise (Errors.Lookup_Error msg)
      | exn -> raise exn
\end{lstlisting}

\end{itemize}



% \section{Guidance}
% You can't talk about everything. Cover the high level first, then cover important, relevant or impressive details.



% % \section{General points}

% These points apply to the whole dissertation, not just this chapter.



% % \subsection{Figures}
% \emph{Always} refer to figures included, like Figure \ref{fig:relu}, in the body of the text. Include full, explanatory captions and make sure the figures look good on the page.
% You may include multiple figures in one float, as in Figure \ref{fig:synthetic}, using \texttt{subcaption}, which is enabled in the template.



% % Figures are important. Use them well.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/relu.pdf}    

%     \caption{In figure captions, explain what the reader is looking at: ``A schematic of the rectifying linear unit, where $a$ is the output amplitude,
%     $d$ is a configurable dead-zone, and $Z_j$ is the input signal'', as well as why the reader is looking at this: 
%     ``It is notable that there is no activation \emph{at all} below 0, which explains our initial results.'' 
%     \textbf{Use vector image formats (.pdf) where possible}. Size figures appropriately, and do not make them over-large or too small to read.
%     }

%     % use the notation fig:name to cross reference a figure
%     \label{fig:relu} 
% \end{figure}


% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{images/synthetic.png}
%         \caption{Synthetic image, black on white.}
%         \label{fig:syn1}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%       %(or a blank line to force the subfigure onto a new line)
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{images/synthetic_2.png}
%         \caption{Synthetic image, white on black.}
%         \label{fig:syn2}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%     %(or a blank line to force the subfigure onto a new line)    
%     \caption{Synthetic test images for edge detection algorithms. \subref{fig:syn1} shows various gray levels that require an adaptive algorithm. \subref{fig:syn2}
%     shows more challenging edge detection tests that have crossing lines. Fusing these into full segments typically requires algorithms like the Hough transform.
%     This is an example of using subfigures, with \texttt{subref}s in the caption.
%     }\label{fig:synthetic}
% \end{figure}

% \clearpage

% % \subsection{Equations}

% Equations should be typeset correctly and precisely. Make sure you get parenthesis sizing correct, and punctuate equations correctly 
% (the comma is important and goes \textit{inside} the equation block). Explain any symbols used clearly if not defined earlier. 

% For example, we might define:
% \begin{equation}
%     \hat{f}(\xi) = \frac{1}{2}\left[ \int_{-\infty}^{\infty} f(x) e^{2\pi i x \xi} \right],
% \end{equation}    
% where $\hat{f}(\xi)$ is the Fourier transform of the time domain signal $f(x)$.

% % \subsection{Algorithms}
% Algorithms can be set using \texttt{algorithm2e}, as in Algorithm \ref{alg:metropolis}.

% % NOTE: line ends are denoted by \; in algorithm2e
% \begin{center}
% \begin{minipage}{0.8\linewidth}%
% \begin{algorithm}[H]
%     \DontPrintSemicolon
%     \KwData{$f_X(x)$, a probability density function returing the density at $x$.\; $\sigma$ a standard deviation specifying the spread of the proposal distribution.\;
%     $x_0$, an initial starting condition.}
%     \KwResult{$s=[x_1, x_2, \dots, x_n]$, $n$ samples approximately drawn from a distribution with PDF $f_X(x)$.}
%     \Begin{
%         $s \longleftarrow []$\;
%         $p \longleftarrow f_X(x)$\;
%         $i \longleftarrow 0$\;
%         \While{$i < n$}
%         {
%             $x^\prime \longleftarrow \mathcal{N}(x, \sigma^2)$\;
%             $p^\prime \longleftarrow f_X(x^\prime)$\;
%             $a \longleftarrow \frac{p^\prime}{p}$\;
%             $r \longleftarrow U(0,1)$\;
%             \If{$r<a$}
%             {
%                 $x \longleftarrow x^\prime$\;
%                 $p \longleftarrow f_X(x)$\;
%                 $i \longleftarrow i+1$\;
%                 append $x$ to $s$\;
%             }
%         }
%     }
    
% \caption{The Metropolis-Hastings MCMC algorithm for drawing samples from arbitrary probability distributions, 
% specialised for normal proposal distributions $q(x^\prime|x) = \mathcal{N}(x, \sigma^2)$. The symmetry of the normal distribution means the acceptance rule takes the simplified form.}\label{alg:metropolis}
% \end{algorithm}
% \end{minipage}
% \end{center}

% % \subsection{Tables}

% If you need to include tables, like Table \ref{tab:operators}, use a tool like https://www.tablesgenerator.com/ to generate the table as it is
% extremely tedious otherwise. 

% \begin{table}[]
%     \caption{The standard table of operators in Python, along with their functional equivalents from the \texttt{operator} package. Note that table
%     captions go above the table, not below. Do not add additional rules/lines to tables. }\label{tab:operators}
%     %\tt 
%     \rowcolors{2}{}{gray!3}
%     \begin{tabular}{@{}lll@{}}
%     %\toprule
%     \textbf{Operation}    & \textbf{Syntax}                & \textbf{Function}                            \\ %\midrule % optional rule for header
%     Addition              & \texttt{a + b}                          & \texttt{add(a, b)}                                    \\
%     Concatenation         & \texttt{seq1 + seq2}                    & \texttt{concat(seq1, seq2)}                           \\
%     Containment Test      & \texttt{obj in seq}                     & \texttt{contains(seq, obj)}                           \\
%     Division              & \texttt{a / b}                          & \texttt{div(a, b) }  \\
%     Division              & \texttt{a / b}                          & \texttt{truediv(a, b) } \\
%     Division              & \texttt{a // b}                         & \texttt{floordiv(a, b)}                               \\
%     Bitwise And           & \texttt{a \& b}                         & \texttt{and\_(a, b)}                                  \\
%     Bitwise Exclusive Or  & \texttt{a \textasciicircum b}           & \texttt{xor(a, b)}                                    \\
%     Bitwise Inversion     & \texttt{$\sim$a}                        & \texttt{invert(a)}                                    \\
%     Bitwise Or            & \texttt{a | b}                          & \texttt{or\_(a, b)}                                   \\
%     Exponentiation        & \texttt{a ** b}                         & \texttt{pow(a, b)}                                    \\
%     Identity              & \texttt{a is b}                         & \texttt{is\_(a, b)}                                   \\
%     Identity              & \texttt{a is not b}                     & \texttt{is\_not(a, b)}                                \\
%     Indexed Assignment    & \texttt{obj{[}k{]} = v}                 & \texttt{setitem(obj, k, v)}                           \\
%     Indexed Deletion      & \texttt{del obj{[}k{]}}                 & \texttt{delitem(obj, k)}                              \\
%     Indexing              & \texttt{obj{[}k{]}}                     & \texttt{getitem(obj, k)}                              \\
%     Left Shift            & \texttt{a \textless{}\textless b}       & \texttt{lshift(a, b)}                                 \\
%     Modulo                & \texttt{a \% b}                         & \texttt{mod(a, b)}                                    \\
%     Multiplication        & \texttt{a * b}                          & \texttt{mul(a, b)}                                    \\
%     Negation (Arithmetic) & \texttt{- a}                            & \texttt{neg(a)}                                       \\
%     Negation (Logical)    & \texttt{not a}                          & \texttt{not\_(a)}                                     \\
%     Positive              & \texttt{+ a}                            & \texttt{pos(a)}                                       \\
%     Right Shift           & \texttt{a \textgreater{}\textgreater b} & \texttt{rshift(a, b)}                                 \\
%     Sequence Repetition   & \texttt{seq * i}                        & \texttt{repeat(seq, i)}                               \\
%     Slice Assignment      & \texttt{seq{[}i:j{]} = values}          & \texttt{setitem(seq, slice(i, j), values)}            \\
%     Slice Deletion        & \texttt{del seq{[}i:j{]}}               & \texttt{delitem(seq, slice(i, j))}                    \\
%     Slicing               & \texttt{seq{[}i:j{]}}                   & \texttt{getitem(seq, slice(i, j))}                    \\
%     String Formatting     & \texttt{s \% obj}                       & \texttt{mod(s, obj)}                                  \\
%     Subtraction           & \texttt{a - b}                          & \texttt{sub(a, b)}                                    \\
%     Truth Test            & \texttt{obj}                            & \texttt{truth(obj)}                                   \\
%     Ordering              & \texttt{a \textless b}                  & \texttt{lt(a, b)}                                     \\
%     Ordering              & \texttt{a \textless{}= b}               & \texttt{le(a, b)}                                     \\
%     % \bottomrule
%     \end{tabular}
%     \end{table}
% % \subsection{Code}

% Avoid putting large blocks of code in the report (more than a page in one block, for example). Use syntax highlighting if possible, as in Listing \ref{lst:callahan}.

% \begin{lstlisting}[language=python, float, caption={The algorithm for packing the $3\times 3$ outer-totalistic binary CA successor rule into a 
%     $16\times 16\times 16\times 16$ 4 bit lookup table, running an equivalent, notionally 16-state $2\times 2$ CA.}, label=lst:callahan]
%     def create_callahan_table(rule="b3s23"):
%         """Generate the lookup table for the cells."""        
%         s_table = np.zeros((16, 16, 16, 16), dtype=np.uint8)
%         birth, survive = parse_rule(rule)

%         # generate all 16 bit strings
%         for iv in range(65536):
%             bv = [(iv >> z) & 1 for z in range(16)]
%             a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p = bv

%             # compute next state of the inner 2x2
%             nw = apply_rule(f, a, b, c, e, g, i, j, k)
%             ne = apply_rule(g, b, c, d, f, h, j, k, l)
%             sw = apply_rule(j, e, f, g, i, k, m, n, o)
%             se = apply_rule(k, f, g, h, j, l, n, o, p)

%             # compute the index of this 4x4
%             nw_code = a | (b << 1) | (e << 2) | (f << 3)
%             ne_code = c | (d << 1) | (g << 2) | (h << 3)
%             sw_code = i | (j << 1) | (m << 2) | (n << 3)
%             se_code = k | (l << 1) | (o << 2) | (p << 3)

%             # compute the state for the 2x2
%             next_code = nw | (ne << 1) | (sw << 2) | (se << 3)

%             # get the 4x4 index, and write into the table
%             s_table[nw_code, ne_code, sw_code, se_code] = next_code

%         return s_table

% \end{lstlisting}

%==================================================================================================================================
\chapter{Evaluation} 
% How good is your solution? How well did you solve the general problem, and what evidence do you have to support that?

In this section we evaluate the implementation of the PyFunc language.
% To do this several steps were required.

Initailly, direct code comparisions for relatively non-trivial examples are used to indicate the general similarity between the structure of PyFunc code and Python code.
Metrics such as Levenshtein distance do to some degree quantify the differences, though the actual usefulness of such a metric is not conclusive.

To compare the output of the PyFunc runtime and the Python runtime a parallel comparison was required.
But as there does not exist a full implementation test-suite for the Python language specification a substitute means for determining correct function was necessary.
The next best thing to a language implementation test-suite is the actual internal test-suite of the current Python release version (CPython v3.12.2).
The test suite was used as a series of unit-tests for the various features of PyFunc.

It should be noted that the CPython test suite is an interal test-suite and its standard is not held as the optimal language implementation test-suite for Python.
Furthermore, the test-suite also tests significant amounts functionaility that PyFunc does not support in any way, ranging from built-in functions to the behaviour of overloaded operators.

(I.e., $-\text{True}$ is valid Python, but is not valid PyFunc as a negative unary operator is for numeric types only.)

\section{Direct Comparison Code Comparison}

% \section{Guidance}
% \begin{itemize}
%     \item
%         Ask specific questions that address the general problem.
%     \item
%         Answer them with precise evidence (graphs, numbers, statistical
%         analysis, qualitative analysis).
%     \item
%         Be fair and be scientific.
%     \item
%         The key thing is to show that you know how to evaluate your work, not
%         that your work is the most amazing product ever.
% \end{itemize}
\section{Testing Against The CPython Test-Suite}



% \section{Evidence}
% Make sure you present your evidence well. Use appropriate visualisations, reporting techniques and statistical analysis, as appropriate.

% If you visualise, follow the basic rules, as illustrated in Figure \ref{fig:boxplot}:
% \begin{itemize}
% \item Label everything correctly (axis, title, units).
% \item Caption thoroughly.
% \item Reference in text.
% \item \textbf{Include appropriate display of uncertainty (e.g. error bars, Box plot)}
% \item Minimize clutter.
% \end{itemize}

% See the file \texttt{guide\_to\_visualising.pdf} for further information and guidance.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/boxplot_finger_distance.pdf}    

%     \caption{Average number of fingers detected by the touch sensor at different heights above the surface, averaged over all gestures. Dashed lines indicate
%     the true number of fingers present. The Box plots include bootstrapped uncertainty notches for the median. It is clear that the device is biased toward 
%     undercounting fingers, particularly at higher $z$ distances.
%     }

%     % use the notation fig:name to cross reference a figure
%     \label{fig:boxplot} 
% \end{figure}


% %==================================================================================================================================


\chapter{Conclusions}    
Summarise the whole project for a lazy reader who didn't read the rest (e.g. a prize-awarding committee).
% \section{Guidance}
\begin{itemize}
    \item
        Summarise briefly and fairly.

        The aim of this project was to create a Python-like, functional programming language with a Hindley-Milner type system.
    \item
        You should be addressing the general problem you introduced in the
        Introduction. 
        
        The implementation outlined is a fully functional programming language that can now perform non-trivial computations.

        Furthermore, the project indicates that Pythonic syntax is rather amenable to this style of programming, as minimal changes were needed to convert the syntax of the imperative, statement-based Python to the expression-based functional language of PyFunc.

        The implementation performed well on an applicable subset of the CPython test suite, producing favourable results that indicate a level of functionality in the PyFunc implementation.

    \item
        Include summary of concrete results (``the new compiler ran 2x
        faster'')
    \item
        Indicate what future work could be done, but remember: \textbf{you
        won't get credit for things you haven't done}.
\end{itemize}

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}

\chapter{Appendices}
\label{chp:appendix}

\section{User Guide}
\subsection{Install}
\subsection{Shell Commands}
\subsection{Example Programs}
\section{Full Parser Grammar}
\subsection{Lexer Specification}

\begin{lstlisting}[language=C, label=lst:lexer-spec, caption=PyFunc's lexer specification for OCamllex., keepspaces=true]
rule token = parse
    (* Source Cleaners *)
    | [' ' '\t' '\r' '\n']* { token lexbuf }
    | '#'[^'\n']*           { token lexbuf }  (* Single line comments. *)
    | "def"     {DEFINE}
    | "True"    {TRUE}
    | "False"   {FALSE}
    | "if"      {IF}
    | "elif"    {ELIF}
    | "else"    {ELSE}
    | "None"    {UNITVAL}
    | '('       {LPAREN}
    | ')'       {RPAREN}
    | '{'       {LBRACE}
    | '}'       {RBRACE}
    | '['       {LBRACK}
    | ']'       {RBRACK}
    | '='       {EQ}
    | "and"     {AND}
    | "or"      {OR}
    | "not"     {NOT}
    | '<'       {LT}
    | '>'       {GT}
    | ">="      {GEQ}
    | "<="      {LEQ}
    | "=="      {EQQ}
    | "!="      {NEQ}
    | '+'       {PLUS}
    | '-'       {MINUS}
    | '*'       {STAR}
    | '/'       {DIVIDE}
    | "::"      {CONS}
    | '%'       {MOD}
    | ','       {COMMA}
    | ':'       {COLON}
    | eof       {EOF}
    | ['A'-'Z' 'a'-'z' '_']['A'-'Z' 'a'-'z' '_' '0'-'9' '\'']*  {ID (lexeme lexbuf)}
    | '"'       {read_string (Buffer.create 17) lexbuf}
    | ['0'-'9']+ as num_string {INTVAL (int_of_string num_string)}
    | _         { raise (lexical_error ("Illegal character: " ^ lexeme lexbuf))}
    and read_string buffer = parse
        | '"'           {STRINGVAL (Buffer.contents buffer)}
        | '\\' '/'      {Buffer.add_char buffer '/'; read_string buffer lexbuf}
        | '\\' '\\'     {Buffer.add_char buffer '\\'; read_string buffer lexbuf}
        | '\\' 'b'      {Buffer.add_char buffer '\b'; read_string buffer lexbuf}
        | '\\' 'f'      {Buffer.add_char buffer '\012'; read_string buffer lexbuf}
        | '\\' 'n'      {Buffer.add_char buffer '\n'; read_string buffer lexbuf}
        | '\\' 'r'      {Buffer.add_char buffer '\r'; read_string buffer lexbuf}
        | '\\' 't'      {Buffer.add_char buffer '\t'; read_string buffer lexbuf}
        | [^ '"' '\\']+ {Buffer.add_string buffer (Lexing.lexeme lexbuf);read_string buffer lexbuf}
        | _             {raise (Lexical_error ("Illegal string character: " ^ Lexing.lexeme lexbuf))}
        | eof           {raise (Lexical_error ("String is not terminated"))}
\end{lstlisting}

% Typical inclusions in the appendices are:

% \begin{itemize}
% \item
%   Copies of ethics approvals (required if obtained)
% \item
%   Copies of questionnaires etc. used to gather data from subjects.
% \item
%   Extensive tables or figures that are too bulky to fit in the main body of
%   the report, particularly ones that are repetitive and summarised in the body.

% \item Outline of the source code (e.g. directory structure), or other architecture documentation like class diagrams.

% \item User manuals, and any guides to starting/running the software.

% \end{itemize}

\textbf{Don't include your source code in the appendices}. It will be
submitted separately.

\end{appendices}

%==================================================================================================================================
%   BIBLIOGRAPHY   

% The bibliography style is abbrvnat
% The bibliography always appears last, after the appendices.

\bibliographystyle{abbrvnat}

\bibliography{l4proj}



\end{document}
